{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Orchestrate Conversation with Dynamic Prompt Injection\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Wjgq0iUveByn"
      },
      "id": "Wjgq0iUveByn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective:\n",
        "This assignment focuses on a crucial advanced technique in LLM applications: **dynamic prompt injection**. You'll build a conversational system that adapts its prompts based on user input, retrieved context, or internal logic, demonstrating how to maintain coherence and provide relevant responses by injecting information into the LLM's working memory or instructions *mid-conversation*."
      ],
      "metadata": {
        "id": "3DLSbtRBeByq"
      },
      "id": "3DLSbtRBeByq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "2_hHIRyreByr"
      },
      "id": "2_hHIRyreByr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions:\n",
        "1.  **LLM Access**: You'll need access to an LLM API. **OpenAI's models (e.g., GPT-4o, GPT-4, GPT-3.5-turbo)** are recommended for their strong performance. Configure your API key securely (e.g., via environment variables).\n",
        "2.  **Environment Setup**: Install the necessary Python libraries: `pip install openai python-dotenv`.\n",
        "3.  **Scenario**: You will build a simple **\"Product Inquiry Assistant\"**. This assistant will:\n",
        "    * Receive user questions about products.\n",
        "    * **Dynamically inject** product details (from a simulated database/dictionary) into the prompt before querying the LLM.\n",
        "    * Handle follow-up questions, always considering the current product context.\n",
        "    * Allow the user to switch products, triggering a new dynamic injection.\n",
        "4.  **Jupyter Notebook**: All your code, outputs, observations, and analysis must be documented in this Jupyter Notebook.\n",
        "5.  **Analysis**: Explain how dynamic prompt injection works, its benefits, and challenges."
      ],
      "metadata": {
        "id": "RrcZ-U-ZeByr"
      },
      "id": "RrcZ-U-ZeByr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "HW8JXFJyeByt"
      },
      "id": "HW8JXFJyeByt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and LLM Configuration\n",
        "Begin by setting up your environment and configuring your LLM."
      ],
      "metadata": {
        "id": "Cj8O5EKJeByt"
      },
      "id": "Cj8O5EKJeByt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1: Install Libraries and Configure LLM\n",
        "Install `openai` and `python-dotenv`. Set up your OpenAI API key from environment variables."
      ],
      "metadata": {
        "id": "nCdV_slZeByu"
      },
      "id": "nCdV_slZeByu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries (if not already installed)\n",
        "# !pip install openai python-dotenv --quiet\n",
        "\n",
        "import openai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# --- IMPORTANT: Create a .env file in the same directory as this notebook with the line: ---\n",
        "# OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY_HERE\"\n",
        "\n",
        "# Configure OpenAI API key\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openai.api_key:\n",
        "    print(\"WARNING: OPENAI_API_KEY not found in environment variables. Please set it in .env file.\")\n",
        "else:\n",
        "    print(\"OpenAI API key loaded!\")\n",
        "\n",
        "# Define the LLM model to use\n",
        "LLM_MODEL = \"gpt-3.5-turbo\" # You can use \"gpt-4o\", \"gpt-4\", etc. if you have access\n",
        "\n",
        "print(f\"Using LLM Model: {LLM_MODEL}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "z3CahljreByv"
      },
      "id": "z3CahljreByv",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Simulate a Product Database\n",
        "Create a Python dictionary to represent a simple product database. This will be the source of information for dynamic injection."
      ],
      "metadata": {
        "id": "6svIAUfgeByx"
      },
      "id": "6svIAUfgeByx"
    },
    {
      "cell_type": "code",
      "source": [
        "product_database = {\n",
        "    \"laptop\": {\n",
        "        \"name\": \"ProBook X1\",\n",
        "        \"specs\": \"16-inch Retina display, M3 Pro chip, 16GB RAM, 512GB SSD, 12-hour battery life.\",\n",
        "        \"features\": \"Lightweight aluminum unibody, advanced cooling system, backlit keyboard, Thunderbolt 4 ports.\",\n",
        "        \"price\": \"$1999\",\n",
        "        \"warranty\": \"1-year limited warranty.\"\n",
        "    },\n",
        "    \"smartphone\": {\n",
        "        \"name\": \"NexPhone Ultra\",\n",
        "        \"specs\": \"6.7-inch OLED display, A10 Bionic chip, 8GB RAM, 256GB storage, triple-lens camera system.\",\n",
        "        \"features\": \"Water and dust resistant, face unlock, fast charging, 5G connectivity.\",\n",
        "        \"price\": \"$999\",\n",
        "        \"warranty\": \"1-year manufacturer warranty.\"\n",
        "    },\n",
        "    \"headphones\": {\n",
        "        \"name\": \"SoundWave Pro\",\n",
        "        \"specs\": \"Over-ear design, Active Noise Cancellation (ANC), Bluetooth 5.2, 30-hour battery life.\",\n",
        "        \"features\": \"Comfortable earcups, foldable design, ambient sound mode, custom EQ settings via app.\",\n",
        "        \"price\": \"$299\",\n",
        "        \"warranty\": \"6-month limited warranty.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Product database simulated with:\", list(product_database.keys()))"
      ],
      "outputs": [],
      "metadata": {
        "id": "T9MeDMg3eByy"
      },
      "id": "T9MeDMg3eByy",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "EsKfRatweByz"
      },
      "id": "EsKfRatweByz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Build the Conversational Agent with Dynamic Prompt Injection\n",
        "You'll create a function that manages the conversation, identifies the current product context, and dynamically injects relevant details into the LLM's prompt."
      ],
      "metadata": {
        "id": "tb7cHgyJeBy3"
      },
      "id": "tb7cHgyJeBy3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1: Implement `get_product_context` Function\n",
        "This function will identify if a product mention exists in the user's query and retrieve its details from the `product_database`."
      ],
      "metadata": {
        "id": "W51txXQpeBy4"
      },
      "id": "W51txXQpeBy4"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_product_context(query: str, current_product: str = None) -> tuple:\n",
        "    \"\"\"\n",
        "    Identifies a product in the query and returns its details.\n",
        "    Prioritizes explicit mentions, otherwise uses the current product context.\n",
        "    Returns (product_name, product_details_string).\n",
        "    \"\"\"\n",
        "    detected_product = None\n",
        "    for product_key in product_database.keys():\n",
        "        if product_key in query.lower():\n",
        "            detected_product = product_key\n",
        "            break\n",
        "\n",
        "    if detected_product:\n",
        "        product_name = product_database[detected_product]['name']\n",
        "        details = product_database[detected_product]\n",
        "        product_details_string = f\"Product Name: {details['name']}\\nSpecs: {details['specs']}\\nFeatures: {details['features']}\\nPrice: {details['price']}\\nWarranty: {details['warranty']}\"\n",
        "        return detected_product, product_details_string\n",
        "    elif current_product and current_product in product_database:\n",
        "        # If no new product detected, use the current conversation product\n",
        "        details = product_database[current_product]\n",
        "        product_details_string = f\"Product Name: {details['name']}\\nSpecs: {details['specs']}\\nFeatures: {details['features']}\\nPrice: {details['price']}\\nWarranty: {details['warranty']}\"\n",
        "        return current_product, product_details_string\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "print(\"get_product_context function defined!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "q_eSG46teBy4"
      },
      "id": "q_eSG46teBy4",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2: Implement `get_llm_response` Function\n",
        "This function will interact with the OpenAI API. It will dynamically build the prompt based on the user's query and the injected product context."
      ],
      "metadata": {
        "id": "GpJURocbeBy6"
      },
      "id": "GpJURocbeBy6"
    },
    {
      "cell_type": "code",
      "source": [
        "async def get_llm_response(conversation_history: list, product_context_string: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Generates a response from the LLM, dynamically injecting product context.\n",
        "    \"\"\"\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful product inquiry assistant. Your goal is to answer questions about products based on the provided product details. If a question is outside the scope of the provided product details or unrelated to products, kindly state that you can only assist with product inquiries. Always be polite and concise.\"\n",
        "    }\n",
        "\n",
        "    # Dynamically inject product context at the beginning of the conversation history\n",
        "    # This acts like a 'system' message but can be updated dynamically.\n",
        "    dynamic_context_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": f\"CURRENT PRODUCT DETAILS:\\n{product_context_string}\\n\\nAnswer user questions strictly based on these details. If a question is about a different product, ask the user to specify which product they are interested in.\"\n",
        "        if product_context_string else\n",
        "        \"No specific product context provided. Please ask the user to specify a product (e.g., laptop, smartphone, headphones) before answering product-specific questions.\"\n",
        "    }\n",
        "\n",
        "    # Combine dynamic context with the rest of the conversation history\n",
        "    messages = [system_message, dynamic_context_message] + conversation_history\n",
        "\n",
        "    try:\n",
        "        response = await openai.chat.completions.create(\n",
        "            model=LLM_MODEL,\n",
        "            messages=messages,\n",
        "            temperature=0.7,\n",
        "            max_tokens=150\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except openai.APIError as e:\n",
        "        print(f\"OpenAI API error: {e}\")\n",
        "        return \"I'm sorry, I'm having trouble connecting to the AI right now. Please try again later.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return \"An unexpected error occurred. Please try again.\"\n",
        "\n",
        "print(\"get_llm_response function defined!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "Rla61U_8eBy6"
      },
      "id": "Rla61U_8eBy6",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.3: Implement the Main Conversational Loop\n",
        "Create an asynchronous function that handles the entire conversational flow, managing the `current_product` state and calling the other functions."
      ],
      "metadata": {
        "id": "7Uzld47ReBy8"
      },
      "id": "7Uzld47ReBy8"
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_product_assistant():\n",
        "    conversation_history = []\n",
        "    current_product_key = None\n",
        "    product_details_string = None\n",
        "\n",
        "    print(\"Welcome to the Product Inquiry Assistant! You can ask about: laptop, smartphone, headphones.\")\n",
        "    print(\"Type 'exit' to end the conversation.\")\n",
        "    print(\"--------------------------------------------------------------------------------------\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nYour question: \")\n",
        "        if user_query.lower() == 'exit':\n",
        "            print(\"Thank you for using the Product Inquiry Assistant. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # 1. Detect if a new product is mentioned or use existing context\n",
        "        detected_product, new_product_details_string = get_product_context(user_query, current_product_key)\n",
        "\n",
        "        if detected_product and detected_product != current_product_key:\n",
        "            print(f\"Assistant: Switching context to {product_database[detected_product]['name']}.\")\n",
        "            current_product_key = detected_product\n",
        "            product_details_string = new_product_details_string\n",
        "            # Reset conversation history when product context changes significantly\n",
        "            conversation_history = []\n",
        "        elif detected_product is None and current_product_key is None:\n",
        "            print(\"Assistant: Please specify which product you are interested in (e.g., laptop, smartphone, headphones).\")\n",
        "            continue # Skip LLM call if no product is specified initially\n",
        "\n",
        "        # Add user query to conversation history\n",
        "        conversation_history.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "        # 2. Get LLM response with dynamically injected product context\n",
        "        llm_response = await get_llm_response(conversation_history, product_details_string)\n",
        "\n",
        "        # Add LLM response to conversation history\n",
        "        conversation_history.append({\"role\": \"assistant\", \"content\": llm_response})\n",
        "\n",
        "        print(f\"Assistant: {llm_response}\")\n",
        "\n",
        "print(\"run_product_assistant function defined!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "_fV2XrmfeBy9"
      },
      "id": "_fV2XrmfeBy9",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "wRC65iSDeBy_"
      },
      "id": "wRC65iSDeBy_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Test the Conversational Agent\n",
        "Run your `run_product_assistant` function and interact with it. Observe how the context shifts and how the LLM responds based on the injected data."
      ],
      "metadata": {
        "id": "j36t0MwQeBzA"
      },
      "id": "j36t0MwQeBzA"
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "# Run the assistant\n",
        "await run_product_assistant()"
      ],
      "outputs": [],
      "metadata": {
        "id": "aKnLSu1feBzA"
      },
      "id": "aKnLSu1feBzA",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Qlek2YPreBzB"
      },
      "id": "Qlek2YPreBzB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Analysis and Reflection\n",
        "Based on your interaction with the bot, answer the following questions."
      ],
      "metadata": {
        "id": "ZwKZD4EeeBzB"
      },
      "id": "ZwKZD4EeeBzB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4.1: Dynamic Prompt Injection Mechanism\n",
        "* **How it Works**: Describe in detail how dynamic prompt injection is implemented in your `get_llm_response` function. How does the `product_context_string` become part of the LLM's understanding?\n",
        "* **Location of Injection**: Why was the `dynamic_context_message` placed at the beginning of the `messages` list, right after the initial `system_message`? What would happen if it was placed at the end, or if the `conversation_history` was not reset when switching products?\n",
        "* **Product Switching**: Explain how your system detects a change in the user's desired product and how it adapts the context accordingly."
      ],
      "metadata": {
        "id": "ct6X9u5WeBzK"
      },
      "id": "ct6X9u5WeBzK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4.2: Benefits and Use Cases\n",
        "* **Advantages**: What are the key benefits of using dynamic prompt injection in conversational AI, as demonstrated by this assignment? How does it improve the LLM's relevance and accuracy?\n",
        "* **Alternative Methods**: Briefly discuss an alternative to dynamic prompt injection for managing context (e.g., fine-tuning, retrieval-augmented generation (RAG)). When might dynamic prompt injection be preferred over these alternatives?\n",
        "* **Real-world Applications**: Beyond product inquiries, what are other real-world scenarios where dynamic prompt injection would be highly beneficial (e.g., personalized tutors, technical support, content generation based on user preferences)?"
      ],
      "metadata": {
        "id": "tGooJw3heBzL"
      },
      "id": "tGooJw3heBzL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4.3: Challenges and Limitations\n",
        "* **Prompt Size**: What potential issues could arise if the injected dynamic context becomes very large (e.g., hundreds or thousands of tokens)? How might this impact performance or cost?\n",
        "* **Contextual Overlap/Conflict**: What challenges might arise if the dynamic context contradicts information already present in the conversation history or if the LLM struggles to prioritize the injected context?\n",
        "* **Scalability**: How would you scale this approach if you had millions of products or highly complex product details? What mechanisms would you need beyond a simple dictionary?\n"
      ],
      "metadata": {
        "id": "TplKv7EleBzM"
      },
      "id": "TplKv7EleBzM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ZU8HfKf1eBzN"
      },
      "id": "ZU8HfKf1eBzN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission:\n",
        "* Ensure all code cells have been executed and their outputs are visible.\n",
        "* All analysis and reflections are clearly written in markdown cells.\n",
        "* Make sure your `.env` file (or equivalent API key setup) is mentioned but **NOT** included in the submitted notebook for security reasons.\n",
        "* Save your Jupyter Notebook as `[YourName]_Dynamic_Prompt_Injection_Assignment.ipynb`."
      ],
      "metadata": {
        "id": "-7BnPuBQeBzN"
      },
      "id": "-7BnPuBQeBzN"
    }
  ]
}