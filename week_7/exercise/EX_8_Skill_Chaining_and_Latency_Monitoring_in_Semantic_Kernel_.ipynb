{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Skill Chaining and Latency Monitoring in Semantic Kernel\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "352Mxzmzh3RZ"
      },
      "id": "352Mxzmzh3RZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective:\n",
        "This assignment focuses on understanding and implementing **skill chaining** within the **Semantic Kernel** framework. You will design and connect multiple AI functions (skills) to create a complex workflow. Crucially, you will also **monitor the latency** of these chained operations to analyze the performance implications of sequential AI calls."
      ],
      "metadata": {
        "id": "ourvM8lFh3Rb"
      },
      "id": "ourvM8lFh3Rb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "6EV4gsGRh3Rc"
      },
      "id": "6EV4gsGRh3Rc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions:\n",
        "1.  **LLM Access**: You'll need access to an LLM service supported by Semantic Kernel (e.g., OpenAI, Azure OpenAI). Configure your API key(s) securely.\n",
        "2.  **Semantic Kernel Setup**: Install the `semantic-kernel` library (`pip install semantic-kernel`).\n",
        "3.  **Scenario: Content Creation Pipeline**: Simulate a content creation pipeline where text is processed through several stages:\n",
        "    * **Skill 1: Summarization**: Takes a long article and produces a concise summary.\n",
        "    * **Skill 2: Tone Adjustment**: Takes the summary and rewrites it to a specific tone (e.g., formal to informal, neutral to enthusiastic).\n",
        "    * **Skill 3: Keyword Extraction**: Extracts relevant keywords from the adjusted summary.\n",
        "    * **Skill 4 (Optional/Advanced): Title Generation**: Generates catchy titles based on the adjusted summary and keywords.\n",
        "4.  **Skill Chaining**: Implement this pipeline by defining each step as a distinct Semantic Kernel skill (Semantic Functions). You must demonstrate how the output of one skill serves as the input for the next, forming a chain.\n",
        "5.  **Latency Monitoring**: Integrate a mechanism to measure the execution time of each individual skill and the total time taken for the entire chained operation. Use Python's `time` module or similar.\n",
        "6.  **Jupyter Notebook**: All your code, outputs, observations, and analysis must be documented in this Jupyter Notebook.\n",
        "7.  **Analysis**: Explain your skill design, chaining implementation, and the observed latency measurements. Discuss the factors influencing latency and potential optimization strategies."
      ],
      "metadata": {
        "id": "7zzWsiSlh3Rc"
      },
      "id": "7zzWsiSlh3Rc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "wzRJkTJ8h3Rd"
      },
      "id": "wzRJkTJ8h3Rd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Semantic Kernel Configuration\n",
        "Begin by setting up your environment and configuring Semantic Kernel with your LLM."
      ],
      "metadata": {
        "id": "sdO5u599h3Rd"
      },
      "id": "sdO5u599h3Rd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1: Install Libraries and Configure Semantic Kernel\n",
        "Install `semantic-kernel` and `python-dotenv`. Set up your LLM API key. We'll use OpenAI for this example."
      ],
      "metadata": {
        "id": "jAdykt69h3Rd"
      },
      "id": "jAdykt69h3Rd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries (if not already installed)\n",
        "# !pip install semantic-kernel python-dotenv --quiet\n",
        "\n",
        "import semantic_kernel as sk\n",
        "import os\n",
        "import time\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# --- IMPORTANT: Create a .env file in the same directory as this notebook with the line: ---\n",
        "# OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY_HERE\"\n",
        "# OPENAI_CHAT_MODEL=\"gpt-3.5-turbo\" # or \"gpt-4o\", \"gpt-4\"\n",
        "\n",
        "# Initialize the kernel\n",
        "kernel = sk.Kernel()\n",
        "\n",
        "# Configure LLM (OpenAI example)\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "model_id = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-3.5-turbo\")\n",
        "\n",
        "if not api_key:\n",
        "    print(\"WARNING: OPENAI_API_KEY not found in environment variables. Please set it in .env file.\")\n",
        "else:\n",
        "    kernel.add_service(\n",
        "        sk.OpenAIChatCompletion(\n",
        "            service_id=\"chat_completion\",\n",
        "            ai_model_id=model_id,\n",
        "            api_key=api_key\n",
        "        ),\n",
        "    )\n",
        "    print(f\"Semantic Kernel initialized with OpenAI model: {model_id}\")\n",
        "\n",
        "# Example long article for processing\n",
        "long_article = (\n",
        "    \"The recent surge in AI advancements has led to significant discussions about its societal impact. \"\n",
        "    \"From automated customer service to complex scientific research, artificial intelligence is reshaping industries \"\n",
        "    \"at an unprecedented pace. However, alongside the excitement, concerns about job displacement, \"\n",
        "    \"ethical implications, and data privacy have also gained prominence. Governments and organizations \"\n",
        "    \"worldwide are grappling with the challenge of regulating AI development to ensure responsible innovation. \"\n",
        "    \"Bias in algorithms, explainability of AI decisions, and the potential for misuse are critical areas \"\n",
        "    \"requiring careful consideration. Public discourse is crucial to fostering a balanced understanding \"\n",
        "    \"of AI's potential benefits and risks. Many experts believe that collaboration between technologists, \"\n",
        "    \"policymakers, and ethicists is vital for navigating this transformative era successfully. \"\n",
        "    \"Investment in AI education and retraining programs is also seen as essential to prepare the workforce \"\n",
        "    \"for the future. Ultimately, the trajectory of AI's integration into society will depend on a collective \"\n",
        "    \"commitment to ethical guidelines and human-centric design.\"\n",
        ")\n",
        "\n",
        "print(\"Sample article defined.\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "1GLHrGjrh3Rd"
      },
      "id": "1GLHrGjrh3Rd",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "gjmXmLmmh3Rf"
      },
      "id": "gjmXmLmmh3Rf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Define Semantic Kernel Skills\n",
        "Create separate Semantic Functions for each step of the content creation pipeline."
      ],
      "metadata": {
        "id": "0VI8kkw-h3Rf"
      },
      "id": "0VI8kkw-h3Rf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1: Summarization Skill\n",
        "Create a skill that takes an input text and summarizes it."
      ],
      "metadata": {
        "id": "R8JCxyXth3Rf"
      },
      "id": "R8JCxyXth3Rf"
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_skill_prompt = \"\"\"\n",
        "{{$input}}\n",
        "\n",
        "Given the text above, provide a concise summary of 2-3 sentences. Focus on the main points.\n",
        "\"\"\"\n",
        "\n",
        "summarize_function = kernel.create_function_from_prompt(\n",
        "    function_name=\"Summarize\",\n",
        "    plugin_name=\"ContentProcessing\",\n",
        "    prompt=summarize_skill_prompt,\n",
        "    prompt_template_config=sk.PromptTemplateConfig(\n",
        "        input_variables=[sk.InputVariable(name=\"input\", description=\"The text to summarize.\")],\n",
        "        execution_settings=[sk.OpenAIChatCompletion.DEFAULT_MAX_TOKENS]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Summarization skill created.\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "UC8CnHevh3Rg"
      },
      "id": "UC8CnHevh3Rg",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2: Tone Adjustment Skill\n",
        "Create a skill that takes an input text (the summary) and adjusts its tone."
      ],
      "metadata": {
        "id": "v-djjx7xh3Rg"
      },
      "id": "v-djjx7xh3Rg"
    },
    {
      "cell_type": "code",
      "source": [
        "tone_adjust_skill_prompt = \"\"\"\n",
        "Rewrite the following text in a more {{$tone}} tone:\n",
        "\n",
        "{{$input}}\n",
        "\"\"\"\n",
        "\n",
        "tone_adjust_function = kernel.create_function_from_prompt(\n",
        "    function_name=\"AdjustTone\",\n",
        "    plugin_name=\"ContentProcessing\",\n",
        "    prompt=tone_adjust_skill_prompt,\n",
        "    prompt_template_config=sk.PromptTemplateConfig(\n",
        "        input_variables=[\n",
        "            sk.InputVariable(name=\"input\", description=\"The text to adjust tone.\"),\n",
        "            sk.InputVariable(name=\"tone\", description=\"The desired tone (e.g., 'enthusiastic', 'formal', 'casual').\")\n",
        "        ],\n",
        "        execution_settings=[sk.OpenAIChatCompletion.DEFAULT_MAX_TOKENS]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Tone Adjustment skill created.\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "pMopG9k_h3Rg"
      },
      "id": "pMopG9k_h3Rg",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.3: Keyword Extraction Skill\n",
        "Create a skill that takes an input text (the adjusted summary) and extracts relevant keywords."
      ],
      "metadata": {
        "id": "Pojs1eRyh3Rg"
      },
      "id": "Pojs1eRyh3Rg"
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_extract_skill_prompt = \"\"\"\n",
        "Extract 3-5 key terms or phrases from the following text, separated by commas:\n",
        "\n",
        "{{$input}}\n",
        "\"\"\"\n",
        "\n",
        "keyword_extract_function = kernel.create_function_from_prompt(\n",
        "    function_name=\"ExtractKeywords\",\n",
        "    plugin_name=\"ContentProcessing\",\n",
        "    prompt=keyword_extract_skill_prompt,\n",
        "    prompt_template_config=sk.PromptTemplateConfig(\n",
        "        input_variables=[sk.InputVariable(name=\"input\", description=\"The text to extract keywords from.\")],\n",
        "        execution_settings=[sk.OpenAIChatCompletion.DEFAULT_MAX_TOKENS]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Keyword Extraction skill created.\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "6oMazrcWh3Rh"
      },
      "id": "6oMazrcWh3Rh",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.4 (Optional/Advanced): Title Generation Skill\n",
        "Create a skill that generates catchy titles based on the summary and keywords."
      ],
      "metadata": {
        "id": "Ie20DvsAh3Rh"
      },
      "id": "Ie20DvsAh3Rh"
    },
    {
      "cell_type": "code",
      "source": [
        "title_generate_skill_prompt = \"\"\"\n",
        "Based on the following summary and keywords, suggest 3 catchy and relevant titles, each on a new line.\n",
        "\n",
        "Summary: {{$summary}}\n",
        "Keywords: {{$keywords}}\n",
        "\"\"\"\n",
        "\n",
        "title_generate_function = kernel.create_function_from_prompt(\n",
        "    function_name=\"GenerateTitles\",\n",
        "    plugin_name=\"ContentProcessing\",\n",
        "    prompt=title_generate_skill_prompt,\n",
        "    prompt_template_config=sk.PromptTemplateConfig(\n",
        "        input_variables=[\n",
        "            sk.InputVariable(name=\"summary\", description=\"The content summary.\"),\n",
        "            sk.InputVariable(name=\"keywords\", description=\"Extracted keywords.\")\n",
        "        ],\n",
        "        execution_settings=[sk.OpenAIChatCompletion.DEFAULT_MAX_TOKENS]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Title Generation skill created (optional).\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "OrBT5oUfh3Rh"
      },
      "id": "OrBT5oUfh3Rh",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "DF4__fF2h3Ri"
      },
      "id": "DF4__fF2h3Ri"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Skill Chaining and Latency Monitoring\n",
        "Implement the sequential execution of your skills and measure the time taken for each step and the overall pipeline."
      ],
      "metadata": {
        "id": "jmhYXGABh3Ri"
      },
      "id": "jmhYXGABh3Ri"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.1: Execute Chained Skills with Latency Measurement\n",
        "Write asynchronous Python code to execute the skills in sequence, passing the output of one as input to the next. Measure and print the latency for each step and the total latency."
      ],
      "metadata": {
        "id": "glLr5fYch3Ri"
      },
      "id": "glLr5fYch3Ri"
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_content_pipeline(article_text: str, desired_tone: str):\n",
        "    print(f\"\\n--- Starting Content Pipeline for Tone: '{desired_tone}' ---\")\n",
        "    total_pipeline_start_time = time.time()\n",
        "    latency_measurements = {}\n",
        "\n",
        "    # Step 1: Summarization\n",
        "    step_start_time = time.time()\n",
        "    summary_result = await kernel.invoke(summarize_function, input=article_text)\n",
        "    summary_output = str(summary_result)\n",
        "    latency_measurements[\"Summarization\"] = time.time() - step_start_time\n",
        "    print(f\"Summary ({latency_measurements['Summarization']:.2f}s): {summary_output}\")\n",
        "\n",
        "    # Step 2: Tone Adjustment\n",
        "    step_start_time = time.time()\n",
        "    tone_adjusted_result = await kernel.invoke(tone_adjust_function, input=summary_output, tone=desired_tone)\n",
        "    tone_adjusted_output = str(tone_adjusted_result)\n",
        "    latency_measurements[\"Tone Adjustment\"] = time.time() - step_start_time\n",
        "    print(f\"Tone Adjusted ({latency_measurements['Tone Adjustment']:.2f}s): {tone_adjusted_output}\")\n",
        "\n",
        "    # Step 3: Keyword Extraction\n",
        "    step_start_time = time.time()\n",
        "    keywords_result = await kernel.invoke(keyword_extract_function, input=tone_adjusted_output)\n",
        "    keywords_output = str(keywords_result)\n",
        "    latency_measurements[\"Keyword Extraction\"] = time.time() - step_start_time\n",
        "    print(f\"Keywords ({latency_measurements['Keyword Extraction']:.2f}s): {keywords_output}\")\n",
        "\n",
        "    # Step 4 (Optional): Title Generation\n",
        "    titles_output = \"N/A\"\n",
        "    if title_generate_function: # Check if the optional skill was created\n",
        "        step_start_time = time.time()\n",
        "        titles_result = await kernel.invoke(title_generate_function, summary=tone_adjusted_output, keywords=keywords_output)\n",
        "        titles_output = str(titles_result)\n",
        "        latency_measurements[\"Title Generation\"] = time.time() - step_start_time\n",
        "        print(f\"Titles ({latency_measurements['Title Generation']:.2f}s):\\n{titles_output}\")\n",
        "\n",
        "    total_pipeline_end_time = time.time()\n",
        "    total_latency = total_pipeline_end_time - total_pipeline_start_time\n",
        "    latency_measurements[\"Total Pipeline\"] = total_latency\n",
        "\n",
        "    print(\"\\n--- Pipeline Summary ---\")\n",
        "    print(\"Individual Skill Latencies:\")\n",
        "    for skill, latency in latency_measurements.items():\n",
        "        if skill != \"Total Pipeline\":\n",
        "            print(f\"- {skill}: {latency:.2f} seconds\")\n",
        "    print(f\"Total Pipeline Latency: {total_latency:.2f} seconds\")\n",
        "    print(\"------------------------\")\n",
        "\n",
        "    return {\n",
        "        \"summary\": summary_output,\n",
        "        \"adjusted_content\": tone_adjusted_output,\n",
        "        \"keywords\": keywords_output,\n",
        "        \"titles\": titles_output,\n",
        "        \"latency\": latency_measurements\n",
        "    }\n",
        "\n",
        "print(\"run_content_pipeline function defined.\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "2ZDeWfhNh3Ri"
      },
      "id": "2ZDeWfhNh3Ri",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.2: Run the Pipeline\n",
        "Execute the pipeline with your sample article and observe the outputs and latencies."
      ],
      "metadata": {
        "id": "xpV7DsDVh3Ri"
      },
      "id": "xpV7DsDVh3Ri"
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "# Run the pipeline with an 'enthusiastic' tone\n",
        "results_enthusiastic = await run_content_pipeline(long_article, \"enthusiastic\")\n",
        "\n",
        "# You can run it again with a different tone to see variations\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "results_formal = await run_content_pipeline(long_article, \"formal\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "YnwbzrU2h3Rj"
      },
      "id": "YnwbzrU2h3Rj",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "66qp6Ijvh3Rj"
      },
      "id": "66qp6Ijvh3Rj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Analysis and Reflection\n",
        "Based on your implementation and observations, answer the following questions."
      ],
      "metadata": {
        "id": "TJzEqh29h3Rj"
      },
      "id": "TJzEqh29h3Rj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4.1: Skill Chaining Effectiveness\n",
        "* **Design**: How effectively did you design your skills to enable chaining? Describe how the output of one skill became the input for the next.\n",
        "* **Flexibility**: How does this chaining approach contribute to the modularity and reusability of your AI functions?\n",
        "* **Prompt Engineering**: Discuss any challenges faced in prompt engineering to ensure smooth transitions and consistent output quality across chained skills."
      ],
      "metadata": {
        "id": "vrzODTLzh3Rj"
      },
      "id": "vrzODTLzh3Rj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4.2: Latency Analysis\n",
        "* **Observations**: Report the individual skill latencies and the total pipeline latency you observed. Are there significant differences between skill latencies? What might explain these differences?\n",
        "* **Factors Influencing Latency**: Beyond the number of skills, what other factors do you think contributed to the overall latency (e.g., LLM model choice, complexity of prompt, token count, API network latency, LLM provider's load)?\n",
        "* **Impact of Chaining**: How does chaining multiple LLM calls sequentially impact the overall user experience, especially compared to a single, more complex LLM call (if it were possible)?\n",
        "* **Optimization Strategies**: Propose at least two strategies to reduce latency in a real-world skill-chained application (e.g., parallelization, caching, smaller models, prompt optimization, batched processing)."
      ],
      "metadata": {
        "id": "w4BySz7ch3Rk"
      },
      "id": "w4BySz7ch3Rk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4.3: Practical Applications and Robustness\n",
        "* **Real-world Use Cases**: Provide examples of real-world scenarios where skill chaining with latency monitoring would be crucial (e.g., intelligent chatbots, automated report generation, data processing workflows).\n",
        "* **Error Handling**: How would you enhance this pipeline to handle errors gracefully (e.g., if one skill fails due to an LLM error or returns unexpected output)?\n",
        "* **Monitoring beyond Latency**: What other metrics, besides latency, would be important to monitor for a production-ready skill-chained system (e.g., token usage, cost, output quality, error rates)?"
      ],
      "metadata": {
        "id": "2hOKDc-Hh3Rk"
      },
      "id": "2hOKDc-Hh3Rk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "jRn6qt45h3Rk"
      },
      "id": "jRn6qt45h3Rk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission:\n",
        "* Ensure all code cells have been executed and their outputs are visible.\n",
        "* All analysis and reflections are clearly written in markdown cells.\n",
        "* Make sure your `.env` file (or equivalent API key setup) is mentioned but **NOT** included in the submitted notebook for security reasons.\n",
        "* Save your Jupyter Notebook as `[YourName]_SemanticKernel_Chaining_Latency_Assignment.ipynb`."
      ],
      "metadata": {
        "id": "1EjFwxbTh3Rk"
      },
      "id": "1EjFwxbTh3Rk"
    }
  ]
}