{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Prompt Latency and Performance Testing with Google Generative AI API\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LmHP8DBSMOaM"
      },
      "id": "LmHP8DBSMOaM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective:\n",
        "This assignment aims to provide hands-on experience in measuring and analyzing the **latency and performance characteristics** of Large Language Model (LLM) API calls. You will specifically use the Google Generative AI API (e.g., Gemini Pro) to understand how factors like prompt complexity, length, and concurrency impact response times and overall throughput. This knowledge is crucial for designing efficient and responsive AI applications."
      ],
      "metadata": {
        "id": "mUhQ2L6EMOaO"
      },
      "id": "mUhQ2L6EMOaO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "UT_gNGmrMOaP"
      },
      "id": "UT_gNGmrMOaP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions:\n",
        "1.  **Google Cloud Project & API Key**: Ensure you have a Google Cloud Project with the Google Generative AI API enabled and an API Key generated. You can get started at [Google AI Studio](https://aistudio.google.com/).\n",
        "2.  **Environment Setup**: Install the necessary Python library: `pip install google-generativeai matplotlib seaborn pandas`.\n",
        "3.  **Jupyter Notebook**: All your code, measured data, plots, observations, and analysis must be documented in this Jupyter Notebook.\n",
        "4.  **Data Collection**: For each test, collect sufficient data points (e.g., 20-50 samples) to ensure statistical significance. Store results in lists or Pandas DataFrames for easy analysis.\n",
        "5.  **Analysis and Visualization**: Use `matplotlib` and `seaborn` to visualize your latency data (e.g., histograms, box plots, line plots).\n",
        "6.  **Critical Thinking**: Beyond just reporting numbers, analyze *why* certain results occur and discuss their implications for real-world applications.\n",
        "\n",
        "**Important**: Be mindful of your API usage and potential costs, especially when running many requests or concurrent tests. Free tiers typically have generous limits, but it's good practice to monitor your usage."
      ],
      "metadata": {
        "id": "5R-UjP5vMOaP"
      },
      "id": "5R-UjP5vMOaP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "w3IP0l-LMOaQ"
      },
      "id": "w3IP0l-LMOaQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Basic Latency Measurement\n",
        "In this section, you'll configure your API access and measure the baseline latency for a simple prompt."
      ],
      "metadata": {
        "id": "dtGDecnhMOaQ"
      },
      "id": "dtGDecnhMOaQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1: API Configuration\n",
        "Configure the `google-generativeai` library with your API key. Select a model (e.g., `gemini-pro`)."
      ],
      "metadata": {
        "id": "zPCjhnaIMOaQ"
      },
      "id": "zPCjhnaIMOaQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- YOUR API KEY HERE ---\n",
        "# It's recommended to load your API key from an environment variable\n",
        "# For example: GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "# For this assignment, you can temporarily paste it directly, but be careful not to share your notebook with the key.\n",
        "GOOGLE_API_KEY = \"YOUR_API_KEY_HERE\"\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Choose a model (e.g., 'gemini-pro' for text generation)\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "print(\"API configured and model loaded!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ileg2JzqMOaR"
      },
      "id": "Ileg2JzqMOaR",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Basic Prompt Latency Test\n",
        "Measure the time taken for a simple, short prompt. Repeat the call multiple times (e.g., 20-50 times) to get an average and understand variability."
      ],
      "metadata": {
        "id": "IszPs9OkMOaS"
      },
      "id": "IszPs9OkMOaS"
    },
    {
      "cell_type": "code",
      "source": [
        "num_runs = 30 # Number of times to run the test\n",
        "simple_prompt = \"Tell me a very short, one-sentence story.\"\n",
        "latencies_simple = []\n",
        "responses_simple = []\n",
        "\n",
        "print(f\"Running {num_runs} tests for simple prompt...\")\n",
        "for i in range(num_runs):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        response = model.generate_content(simple_prompt)\n",
        "        end_time = time.time()\n",
        "        latency = end_time - start_time\n",
        "        latencies_simple.append(latency)\n",
        "        responses_simple.append(response.text.strip())\n",
        "        print(f\"Run {i+1}: Latency = {latency:.4f} seconds\")\n",
        "    except Exception as e:\n",
        "        print(f\"Run {i+1}: Error - {e}\")\n",
        "        latencies_simple.append(np.nan) # Mark as NaN for error\n",
        "\n",
        "df_simple = pd.DataFrame({\"latency\": latencies_simple})\n",
        "\n",
        "print(\"\\n--- Results for Simple Prompt ---\")\n",
        "print(df_simple.describe())\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df_simple['latency'].dropna(), kde=True)\n",
        "plt.title('Distribution of Latency for Simple Prompt')\n",
        "plt.xlabel('Latency (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optionally print a few responses to check quality\n",
        "print(\"\\n--- Sample Responses ---\")\n",
        "for i, resp in enumerate(responses_simple[:5]): # Print first 5 responses\n",
        "    print(f\"Response {i+1}: {resp}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "0804QetsMOaS"
      },
      "id": "0804QetsMOaS",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis for Task 1.2:\n",
        "* What is the average, median, and standard deviation of latency for the simple prompt?\n",
        "* Observe the histogram: Is the latency distribution normal, skewed, or does it have outliers?\n",
        "* Does the model consistently provide a one-sentence story as requested?"
      ],
      "metadata": {
        "id": "zn5M_t3LMOaT"
      },
      "id": "zn5M_t3LMOaT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "4IOydzozMOaT"
      },
      "id": "4IOydzozMOaT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Impact of Prompt Complexity and Length\n",
        "Investigate how changes in prompt characteristics affect latency."
      ],
      "metadata": {
        "id": "wABXdSw5MOaT"
      },
      "id": "wABXdSw5MOaT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1: Varying Prompt Length\n",
        "Create prompts of varying lengths (short, medium, long) and compare their latencies. Aim for approximate token counts (e.g., <20, 50-100, >200 input tokens)."
      ],
      "metadata": {
        "id": "_-9arLmDMOaT"
      },
      "id": "_-9arLmDMOaT"
    },
    {
      "cell_type": "code",
      "source": [
        "num_runs_length = 20 # Number of runs for each length category\n",
        "\n",
        "prompts_by_length = {\n",
        "    \"short\": \"Define AI.\",\n",
        "    \"medium\": \"Explain the concept of machine learning in simple terms, providing a real-world example.\",\n",
        "    \"long\": \"Elaborate on the historical development of artificial intelligence, starting from its early conceptualization in the mid-20th century, discussing key milestones, influential figures, and significant breakthroughs that have shaped the field into what it is today. Also, touch upon the ethical implications of AI development and its potential future impact on society. Aim for about 200-300 words.\"\n",
        "}\n",
        "\n",
        "results_length = []\n",
        "\n",
        "for length_type, prompt_text in prompts_by_length.items():\n",
        "    print(f\"\\nRunning tests for {length_type} prompt ({len(prompt_text)} chars)...\")\n",
        "    for i in range(num_runs_length):\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            response = model.generate_content(prompt_text)\n",
        "            end_time = time.time()\n",
        "            latency = end_time - start_time\n",
        "            results_length.append({\n",
        "                \"prompt_type\": length_type,\n",
        "                \"latency\": latency,\n",
        "                \"input_length_chars\": len(prompt_text),\n",
        "                \"output_length_chars\": len(response.text.strip())\n",
        "            })\n",
        "            # print(f\"  Run {i+1}: Latency = {latency:.4f}s\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Run {i+1}: Error - {e}\")\n",
        "            results_length.append({\n",
        "                \"prompt_type\": length_type,\n",
        "                \"latency\": np.nan,\n",
        "                \"input_length_chars\": len(prompt_text),\n",
        "                \"output_length_chars\": np.nan\n",
        "            })\n",
        "\n",
        "df_length = pd.DataFrame(results_length)\n",
        "print(\"\\n--- Results for Prompt Length Variation ---\")\n",
        "print(df_length.groupby('prompt_type')['latency'].describe())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='prompt_type', y='latency', data=df_length.dropna())\n",
        "plt.title('Latency by Prompt Length')\n",
        "plt.xlabel('Prompt Type')\n",
        "plt.ylabel('Latency (seconds)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='input_length_chars', y='latency', hue='prompt_type', data=df_length.dropna())\n",
        "plt.title('Latency vs. Input Prompt Length (Characters)')\n",
        "plt.xlabel('Input Prompt Length (Characters)')\n",
        "plt.ylabel('Latency (seconds)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "y84iz3yhMOaU"
      },
      "id": "y84iz3yhMOaU",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis for Task 2.1:\n",
        "* How does prompt length correlate with latency? Is it linear?\n",
        "* Does the output length also affect latency? (You might need to add a column for output token length and analyze this).\n",
        "* What are the implications for designing prompts for latency-sensitive applications?"
      ],
      "metadata": {
        "id": "0ID-4AgzMOaU"
      },
      "id": "0ID-4AgzMOaU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2: Varying Prompt Complexity\n",
        "Design prompts that require different levels of reasoning or creativity (e.g., simple factual lookup, creative writing, complex problem-solving). Keep them roughly similar in length but vary complexity."
      ],
      "metadata": {
        "id": "B5_X_ZCjMOaV"
      },
      "id": "B5_X_ZCjMOaV"
    },
    {
      "cell_type": "code",
      "source": [
        "num_runs_complexity = 20\n",
        "\n",
        "prompts_by_complexity = {\n",
        "    \"factual\": \"What is the capital of France?\",\n",
        "    \"creative\": \"Write a 1-sentence poetic description of a sunset.\",\n",
        "    \"reasoning\": \"If all A are B, and some B are C, can we conclude that some A are C? Explain your reasoning in one short paragraph.\"\n",
        "}\n",
        "\n",
        "results_complexity = []\n",
        "\n",
        "for comp_type, prompt_text in prompts_by_complexity.items():\n",
        "    print(f\"\\nRunning tests for {comp_type} prompt...\")\n",
        "    for i in range(num_runs_complexity):\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            response = model.generate_content(prompt_text)\n",
        "            end_time = time.time()\n",
        "            latency = end_time - start_time\n",
        "            results_complexity.append({\n",
        "                \"prompt_type\": comp_type,\n",
        "                \"latency\": latency,\n",
        "                \"input_length_chars\": len(prompt_text),\n",
        "                \"output_length_chars\": len(response.text.strip())\n",
        "            })\n",
        "            # print(f\"  Run {i+1}: Latency = {latency:.4f}s\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Run {i+1}: Error - {e}\")\n",
        "            results_complexity.append({\n",
        "                \"prompt_type\": comp_type,\n",
        "                \"latency\": np.nan,\n",
        "                \"input_length_chars\": len(prompt_text),\n",
        "                \"output_length_chars\": np.nan\n",
        "            })\n",
        "\n",
        "df_complexity = pd.DataFrame(results_complexity)\n",
        "print(\"\\n--- Results for Prompt Complexity Variation ---\")\n",
        "print(df_complexity.groupby('prompt_type')['latency'].describe())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='prompt_type', y='latency', data=df_complexity.dropna())\n",
        "plt.title('Latency by Prompt Complexity')\n",
        "plt.xlabel('Prompt Type')\n",
        "plt.ylabel('Latency (seconds)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "OinoOPcHMOaV"
      },
      "id": "OinoOPcHMOaV",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis for Task 2.2:\n",
        "* Does prompt complexity (even with similar length) impact latency? Why or why not?\n",
        "* Were there any differences in the consistency or quality of responses across complexity levels?\n",
        "* How might this inform prompt engineering for different task types?"
      ],
      "metadata": {
        "id": "V0DGl5fQMOaV"
      },
      "id": "V0DGl5fQMOaV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "b3-7hXj5MOaV"
      },
      "id": "b3-7hXj5MOaV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Concurrency and Rate Limits\n",
        "Explore how sending multiple requests, either sequentially or concurrently, affects overall performance and observe rate limiting."
      ],
      "metadata": {
        "id": "JVKuQAXWMOaW"
      },
      "id": "JVKuQAXWMOaW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.1: Sequential vs. Concurrent Requests\n",
        "Compare the total time taken to process a batch of requests (e.g., 10-20 requests) when sent sequentially versus concurrently. For concurrency, you can use `asyncio` or `threading.ThreadPoolExecutor`."
      ],
      "metadata": {
        "id": "TYz4eywRMOaW"
      },
      "id": "TYz4eywRMOaW"
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "num_batch_requests = 10 # Number of requests in a batch\n",
        "prompt_for_batch = \"Generate a random fact.\"\n",
        "\n",
        "# --- Sequential Requests ---\n",
        "print(\"\\n--- Running Sequential Requests ---\")\n",
        "sequential_latencies = []\n",
        "start_total_seq = time.time()\n",
        "for i in range(num_batch_requests):\n",
        "    start_call = time.time()\n",
        "    try:\n",
        "        response = model.generate_content(prompt_for_batch)\n",
        "        end_call = time.time()\n",
        "        sequential_latencies.append(end_call - start_call)\n",
        "    except Exception as e:\n",
        "        print(f\"Sequential request {i+1} error: {e}\")\n",
        "        sequential_latencies.append(np.nan)\n",
        "\n",
        "end_total_seq = time.time()\n",
        "total_time_sequential = end_total_seq - start_total_seq\n",
        "print(f\"Total time for {num_batch_requests} sequential requests: {total_time_sequential:.4f} seconds\")\n",
        "\n",
        "# --- Concurrent Requests (using ThreadPoolExecutor for simplicity) ---\n",
        "print(\"\\n--- Running Concurrent Requests (ThreadPoolExecutor) ---\")\n",
        "concurrent_latencies = []\n",
        "\n",
        "def make_api_call(prompt):\n",
        "    start_call = time.time()\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        end_call = time.time()\n",
        "        return end_call - start_call\n",
        "    except Exception as e:\n",
        "        print(f\"Concurrent request error: {e}\")\n",
        "        return np.nan\n",
        "\n",
        "start_total_conc = time.time()\n",
        "with ThreadPoolExecutor(max_workers=5) as executor: # You can adjust max_workers\n",
        "    futures = [executor.submit(make_api_call, prompt_for_batch) for _ in range(num_batch_requests)]\n",
        "    for future in futures:\n",
        "        concurrent_latencies.append(future.result())\n",
        "\n",
        "end_total_conc = time.time()\n",
        "total_time_concurrent = end_total_conc - start_total_conc\n",
        "print(f\"Total time for {num_batch_requests} concurrent requests: {total_time_concurrent:.4f} seconds\")\n",
        "\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Speedup (Sequential / Concurrent): {total_time_sequential / total_time_concurrent:.2f}x\")\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "JzHNKj1mMOaW"
      },
      "id": "JzHNKj1mMOaW",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis for Task 3.1:\n",
        "* How much faster are concurrent requests compared to sequential ones for the given batch size?\n",
        "* What is the theoretical maximum speedup you might expect, and why might you not achieve it?\n",
        "* Discuss the trade-offs of using concurrency (e.g., complexity, resource usage, rate limits)."
      ],
      "metadata": {
        "id": "Sz60OrxOMOaW"
      },
      "id": "Sz60OrxOMOaW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.2: Observing Rate Limiting (Optional/Careful)\n",
        "Intentionally (or semi-intentionally) hit the API's rate limits by sending a very large number of requests in a short period. Observe the error messages and how the API handles it.\n",
        "\n",
        "**Caution**: This might temporarily block your API key or incur costs if you exceed free tier limits. Proceed with care and be ready to stop the execution if necessary."
      ],
      "metadata": {
        "id": "hUkaeKFRMOaW"
      },
      "id": "hUkaeKFRMOaW"
    },
    {
      "cell_type": "code",
      "source": [
        "num_aggressive_requests = 100 # Try a large number, e.g., 100-200. Adjust based on your rate limit.\n",
        "prompt_aggressive = \"Generate a single random word.\"\n",
        "aggressive_results = []\n",
        "\n",
        "print(f\"\\n--- Attempting to hit Rate Limit with {num_aggressive_requests} requests ---\")\n",
        "for i in range(num_aggressive_requests):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        response = model.generate_content(prompt_aggressive)\n",
        "        end_time = time.time()\n",
        "        latency = end_time - start_time\n",
        "        aggressive_results.append({\"status\": \"success\", \"latency\": latency})\n",
        "    except Exception as e:\n",
        "        end_time = time.time()\n",
        "        latency = end_time - start_time\n",
        "        aggressive_results.append({\"status\": \"error\", \"latency\": latency, \"error_message\": str(e)})\n",
        "        print(f\"Request {i+1} failed: {e}\")\n",
        "        # Optional: Add a small delay if you hit rate limits frequently\n",
        "        # time.sleep(0.5)\n",
        "\n",
        "df_aggressive = pd.DataFrame(aggressive_results)\n",
        "print(\"\\n--- Aggressive Test Results ---\")\n",
        "print(df_aggressive['status'].value_counts())\n",
        "print(df_aggressive.loc[df_aggressive['status'] == 'error', 'error_message'].value_counts())\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "pkyvh2B0MOaX"
      },
      "id": "pkyvh2B0MOaX",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis for Task 3.2:\n",
        "* Did you encounter rate limiting? What specific error messages did you receive (e.g., `ResourceExhausted` or `TooManyRequests`)?\n",
        "* How quickly did the rate limit activate? (If it did)\n",
        "* What strategies can you implement in your application to handle rate limits gracefully (e.g., backoff, retry mechanisms, token buckets)?"
      ],
      "metadata": {
        "id": "51J__uG4MOaX"
      },
      "id": "51J__uG4MOaX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "z68S9iInMOaX"
      },
      "id": "z68S9iInMOaX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Conclusion and Reflection\n",
        "In a markdown cell, provide a comprehensive summary of your findings and reflections based on this assignment."
      ],
      "metadata": {
        "id": "ddaIvrHEMOaX"
      },
      "id": "ddaIvrHEMOaX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Key Latency Factors**: Summarize the primary factors that influence LLM API latency based on your experiments.\n",
        "* **Performance Bottlenecks**: Identify potential performance bottlenecks in integrating LLMs into applications.\n",
        "* **Strategies for Optimization**: What are your top recommendations for optimizing LLM API usage for better latency and throughput?\n",
        "* **Implications for AI Applications**: How does understanding latency affect the design of real-time vs. batch processing AI applications?\n",
        "* **Ethical Considerations**: Are there any ethical implications related to performance (e.g., equitable access, environmental impact of computation)?"
      ],
      "metadata": {
        "id": "T9BMVBC0MOaX"
      },
      "id": "T9BMVBC0MOaX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "640DYynSMOaY"
      },
      "id": "640DYynSMOaY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission:\n",
        "* Ensure all code cells have been executed and their outputs (including prints and plots) are visible.\n",
        "* All analysis and reflections are clearly written in markdown cells.\n",
        "* Save your Jupyter Notebook as `[YourName]_LLM_Latency_Assignment.ipynb`."
      ],
      "metadata": {
        "id": "Z7OWppkPMOaY"
      },
      "id": "Z7OWppkPMOaY"
    }
  ]
}