{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Build a Prompt-Driven Q&A Chatbot\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DbwFG9NhNQdG"
      },
      "id": "DbwFG9NhNQdG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective:\n",
        "This assignment challenges you to **develop a simple, prompt-driven Q&A chatbot** using a Large Language Model (LLM). You'll learn to design an effective core prompt, manage conversational history, handle various user inputs, and evaluate your chatbot's performance. The goal is to build a basic conversational agent that can answer questions based on its underlying LLM knowledge, guided by your carefully crafted prompts."
      ],
      "metadata": {
        "id": "CUPZXlT2NQdH"
      },
      "id": "CUPZXlT2NQdH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "7Bn5vug9NQdI"
      },
      "id": "7Bn5vug9NQdI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions:\n",
        "1.  **LLM Access**: You'll need access to an LLM API (e.g., Google's Gemini, OpenAI's GPT-4, Anthropic's Claude). For this assignment, we'll assume you're using **Google's Gemini Pro model** via the `google-generativeai` library.\n",
        "2.  **Environment Setup**: Install the necessary Python library: `pip install google-generativeai`.\n",
        "3.  **Jupyter Notebook**: All your code, chatbot interactions, observations, and analysis must be documented in this Jupyter Notebook.\n",
        "4.  **API Key**: Securely handle your API key. It's best practice to load it from an environment variable.\n",
        "5.  **Iterative Development**: Build your chatbot incrementally, testing each component as you go.\n",
        "6.  **Analysis**: Critically evaluate your chatbot's responses and identify areas for improvement."
      ],
      "metadata": {
        "id": "InK4dRxCNQdI"
      },
      "id": "InK4dRxCNQdI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "5xdm0Qb7NQdI"
      },
      "id": "5xdm0Qb7NQdI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Initial Setup and Core Prompt Design\n",
        "Begin by setting up your API access and crafting the foundational prompt for your Q&A chatbot."
      ],
      "metadata": {
        "id": "DaA6cwGPNQdJ"
      },
      "id": "DaA6cwGPNQdJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1: API Configuration\n",
        "Configure the `google-generativeai` library with your API key and initialize the Gemini Pro model."
      ],
      "metadata": {
        "id": "_ZPjsVAiNQdJ"
      },
      "id": "_ZPjsVAiNQdJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- YOUR API KEY HERE ---\n",
        "# It's highly recommended to load your API key from an environment variable for security.\n",
        "# For example: GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "# For this assignment, you can temporarily paste it directly, but be careful not to share your notebook with the key.\n",
        "GOOGLE_API_KEY = \"YOUR_API_KEY_HERE\" # Replace with your actual API key\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialize the Gemini Pro model\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "print(\"API configured and Gemini Pro model loaded!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "hfWYVzDSNQdJ"
      },
      "id": "hfWYVzDSNQdJ",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Crafting the Core Prompt\n",
        "Design the **system prompt** that defines your chatbot's persona, rules, and purpose. This prompt will be sent with every user query to guide the LLM's responses."
      ],
      "metadata": {
        "id": "re_mqb9bNQdK"
      },
      "id": "re_mqb9bNQdK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Goal**: Create a helpful, concise Q&A assistant.\n",
        "* **Core Prompt Requirements**:\n",
        "    * Clearly state its role (e.g., \"You are a helpful Q&A assistant.\").\n",
        "    * Instruct it to answer questions directly and concisely.\n",
        "    * Tell it to avoid conversational fillers like \"Hello! How can I help you?\" or \"Is there anything else?\".\n",
        "    * Specify how it should handle questions it cannot answer (e.g., \"If you don't know the answer, state 'I don't have enough information to answer that.'\").\n",
        "\n",
        "* **Your Core Prompt**:\n",
        "    ```\n",
        "    [Your Core Prompt String Here]\n",
        "    ```"
      ],
      "metadata": {
        "id": "hoC9OJicNQdK"
      },
      "id": "hoC9OJicNQdK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your core prompt (system instruction)\n",
        "core_prompt = \"\"\"\n",
        "You are a helpful and concise Q&A assistant.\n",
        "Answer questions directly and briefly.\n",
        "Do not include conversational fillers or greetings.\n",
        "If you don't have enough information to answer a question, respond with: \"I don't have enough information to answer that.\"\n",
        "\"\"\"\n",
        "\n",
        "print(\"Core Prompt Defined:\")\n",
        "print(core_prompt)"
      ],
      "outputs": [],
      "metadata": {
        "id": "pdAOKwArNQdL"
      },
      "id": "pdAOKwArNQdL",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.3: Testing the Core Prompt\n",
        "Send a few test questions to the LLM with *only* your core prompt and the user's question. Observe how it behaves."
      ],
      "metadata": {
        "id": "CU7CA8jsNQdL"
      },
      "id": "CU7CA8jsNQdL"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(prompt_text):\n",
        "    try:\n",
        "        # For single-turn interactions, you can just use generate_content\n",
        "        # For multi-turn, you'd use chat.send_message\n",
        "        response = model.generate_content(prompt_text)\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "print(\"--- Testing Core Prompt ---\")\n",
        "\n",
        "test_question_1 = \"What is the capital of France?\"\n",
        "full_prompt_1 = core_prompt + \"\\nUser: \" + test_question_1\n",
        "print(f\"\\nUser: {test_question_1}\")\n",
        "print(f\"Bot: {get_response(full_prompt_1)}\")\n",
        "\n",
        "test_question_2 = \"Explain quantum entanglement in simple terms.\"\n",
        "full_prompt_2 = core_prompt + \"\\nUser: \" + test_question_2\n",
        "print(f\"\\nUser: {test_question_2}\")\n",
        "print(f\"Bot: {get_response(full_prompt_2)}\")\n",
        "\n",
        "test_question_3 = \"What is the current population of Mars?\"\n",
        "full_prompt_3 = core_prompt + \"\\nUser: \" + test_question_3\n",
        "print(f\"\\nUser: {test_question_3}\")\n",
        "print(f\"Bot: {get_response(full_prompt_3)}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "jEIkfNMDNQdL"
      },
      "id": "jEIkfNMDNQdL",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis for Task 1.3:\n",
        "* Did the LLM adhere to the persona and rules defined in your core prompt?\n",
        "* How well did it handle factual questions vs. questions it couldn't answer?\n",
        "* Identify any unexpected behaviors or areas where the core prompt might need refinement."
      ],
      "metadata": {
        "id": "ao9KvnhqNQdL"
      },
      "id": "ao9KvnhqNQdL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "AVQsC6D_NQdL"
      },
      "id": "AVQsC6D_NQdL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Building the Chat Loop and History Management\n",
        "Now, you'll create a basic chat loop and implement a mechanism to pass conversational history to the LLM."
      ],
      "metadata": {
        "id": "H9UC3TdBNQdM"
      },
      "id": "H9UC3TdBNQdM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1: Implementing Conversational History\n",
        "Modify your `get_response` function to use `model.start_chat()` and `chat.send_message()` to maintain conversation history. This allows the LLM to understand context from previous turns."
      ],
      "metadata": {
        "id": "YISpeG-ENQdM"
      },
      "id": "YISpeG-ENQdM"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_chatbot(system_prompt):\n",
        "    # Initialize a new chat session with the system prompt\n",
        "    chat = model.start_chat(history=[{'role': 'user', 'parts': [system_prompt]}, {'role': 'model', 'parts': ['OK.']}])\n",
        "    return chat\n",
        "\n",
        "def send_message_to_chatbot(chat_session, user_message):\n",
        "    try:\n",
        "        response = chat_session.send_message(user_message)\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Initialize the chatbot with your core prompt\n",
        "my_chatbot = create_chatbot(core_prompt)\n",
        "\n",
        "print(\"--- Testing Chatbot with History ---\")\n",
        "\n",
        "# Test questions to check context\n",
        "print(\"\\nUser: What is the largest ocean on Earth?\")\n",
        "response_1 = send_message_to_chatbot(my_chatbot, \"What is the largest ocean on Earth?\")\n",
        "print(f\"Bot: {response_1}\")\n",
        "\n",
        "print(\"\\nUser: And how deep is its deepest point?\")\n",
        "response_2 = send_message_to_chatbot(my_chatbot, \"And how deep is its deepest point?\")\n",
        "print(f\"Bot: {response_2}\")\n",
        "\n",
        "print(\"\\nUser: What is the highest mountain on land?\")\n",
        "response_3 = send_message_to_chatbot(my_chatbot, \"What is the highest mountain on land?\")\n",
        "print(f\"Bot: {response_3}\")\n",
        "\n",
        "print(\"\\nUser: Is it taller than the deepest point you mentioned earlier?\")\n",
        "response_4 = send_message_to_chatbot(my_chatbot, \"Is it taller than the deepest point you mentioned earlier?\")\n",
        "print(f\"Bot: {response_4}\")\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "--manlBNNQdM"
      },
      "id": "--manlBNNQdM",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis for Task 2.1:\n",
        "* Did the chatbot successfully use context from previous turns (e.g., understanding \"its deepest point\" without explicitly naming the ocean again)?\n",
        "* Are there any instances where context was lost or misinterpreted?\n",
        "* Discuss the advantages and potential disadvantages of passing full conversation history to the LLM."
      ],
      "metadata": {
        "id": "dhcXbr1WNQdM"
      },
      "id": "dhcXbr1WNQdM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2: Building the Interactive Chat Loop\n",
        "Create a loop that allows a user to continuously interact with your chatbot until they type an exit command (e.g., `quit`, `exit`, `bye`)."
      ],
      "metadata": {
        "id": "h7VIFrRONQdM"
      },
      "id": "h7VIFrRONQdM"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_chatbot_conversation(initial_prompt):\n",
        "    print(\"\\n--- Starting Chatbot Conversation ---\")\n",
        "    print(\"Type 'quit' or 'exit' to end the conversation.\")\n",
        "\n",
        "    # Re-initialize chatbot for a fresh conversation\n",
        "    chat = create_chatbot(initial_prompt)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nUser: \")\n",
        "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "            print(\"Bot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        bot_response = send_message_to_chatbot(chat, user_input)\n",
        "        print(f\"Bot: {bot_response}\")\n",
        "\n",
        "# Run the chatbot\n",
        "# Call this function to start the interactive session:\n",
        "# run_chatbot_conversation(core_prompt)\n",
        "\n",
        "# To run it, uncomment the line below and execute the cell.\n",
        "# run_chatbot_conversation(core_prompt)\n",
        "\n",
        "print(\"The interactive chat loop function is defined. Uncomment the last line to run it.\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "5zGz7b48NQdN"
      },
      "id": "5zGz7b48NQdN",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.3: Manual Chatbot Testing and Evaluation\n",
        "Run your interactive chatbot. Engage in a conversation, asking various types of questions. Document at least **5-7 turns of interaction** (User and Bot messages) and provide a qualitative evaluation of your chatbot's performance."
      ],
      "metadata": {
        "id": "OnKyC7yUNQdN"
      },
      "id": "OnKyC7yUNQdN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paste Your Interaction Log Here:**\n",
        "```\n",
        "User: [Your first question]\n",
        "Bot: [Bot's response]\n",
        "\n",
        "User: [Your second question]\n",
        "Bot: [Bot's response]\n",
        "\n",
        "...\n",
        "```"
      ],
      "metadata": {
        "id": "XO8sTdTRNQdN"
      },
      "id": "XO8sTdTRNQdN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis for Task 2.3:\n",
        "* How effective was your chatbot at answering diverse questions?\n",
        "* Did it maintain its persona and follow the instructions in the core prompt throughout the conversation?\n",
        "* Identify any instances of:\n",
        "    * **Hallucination**: Providing incorrect but confidently stated information.\n",
        "    * **Irrelevant responses**: Not addressing the user's query.\n",
        "    * **Failure to follow instructions**: (e.g., still using greetings, not stating \"I don't have enough information\").\n",
        "    * **Context loss**: Forgetting previous turns.\n",
        "* Based on this manual testing, what are the top 2-3 areas for improvement?"
      ],
      "metadata": {
        "id": "_n445XDKNQdN"
      },
      "id": "_n445XDKNQdN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "tO1evB3_NQdN"
      },
      "id": "tO1evB3_NQdN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Advanced Prompting and Refinements (Optional/Bonus)\n",
        "This section is for those who want to explore more advanced techniques to improve their chatbot."
      ],
      "metadata": {
        "id": "6J3U9eVoNQdO"
      },
      "id": "6J3U9eVoNQdO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.1 (Bonus): Adding Specific Knowledge or Constraints\n",
        "Modify your core prompt or add an initial context to your `chat.start_chat()` history to provide the chatbot with specific, limited knowledge or additional constraints (e.g., make it a Q&A bot about *only* a specific historical period, or tell it to only answer questions from a provided text). Test its ability to adhere to these new boundaries.\n",
        "\n",
        "**Example Additions**: \"Only answer questions about the Roman Empire.\"\n",
        "\"Here is a text about renewable energy: [Your Text]. Answer questions ONLY from this text. If the answer is not in the text, say 'I can only answer from the provided text.'\""
      ],
      "metadata": {
        "id": "UxrcrNxbNQdO"
      },
      "id": "UxrcrNxbNQdO"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Implement your modified chatbot setup here ---\n",
        "# new_core_prompt = \"...\"\n",
        "# new_chatbot = create_chatbot(new_core_prompt)\n",
        "# run_chatbot_conversation(new_core_prompt) # Remember to uncomment to run\n",
        "\n",
        "# Provide interaction logs and analysis similar to Task 2.3."
      ],
      "outputs": [],
      "metadata": {
        "id": "L-VmGauFNQdO"
      },
      "id": "L-VmGauFNQdO",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "4a2KG7mjNQdO"
      },
      "id": "4a2KG7mjNQdO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Conclusion and Reflection\n",
        "In a markdown cell, provide a comprehensive summary of your findings and reflections based on this assignment."
      ],
      "metadata": {
        "id": "JvEBS2JoNQdO"
      },
      "id": "JvEBS2JoNQdO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Effectiveness of Prompt Engineering**: How crucial is prompt engineering in guiding an LLM's behavior for a Q&A chatbot?\n",
        "* **Challenges in Chatbot Development**: What were the main challenges you faced in building and testing your prompt-driven chatbot?\n",
        "* **Importance of History**: How significant is maintaining conversation history for a Q&A chatbot, and what are its limitations?\n",
        "* **Future Improvements**: If you had more time, what specific features or improvements would you add to your chatbot?\n",
        "* **Ethical Considerations**: What ethical concerns (e.g., misinformation, bias, privacy) might arise from deploying a simple Q&A chatbot like this?"
      ],
      "metadata": {
        "id": "eAbCgqjGNQdO"
      },
      "id": "eAbCgqjGNQdO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "rNmEzoKJNQdP"
      },
      "id": "rNmEzoKJNQdP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission:\n",
        "* Ensure all code cells have been executed and their outputs (including prints and any interaction logs) are visible.\n",
        "* All analysis and reflections are clearly written in markdown cells.\n",
        "* Save your Jupyter Notebook as `[YourName]_Q&A_Chatbot_Assignment.ipynb`."
      ],
      "metadata": {
        "id": "53tI6KO8NQdP"
      },
      "id": "53tI6KO8NQdP"
    }
  ]
}