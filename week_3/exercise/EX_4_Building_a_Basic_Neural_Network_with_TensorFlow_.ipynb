{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-mzBhRczC5z"
      },
      "source": [
        "#  Python Assignment: Building a Basic Neural Network with TensorFlow\n",
        "\n",
        "This assignment will guide you through the fundamental steps of constructing, training, and evaluating a basic feed-forward neural network using TensorFlow's Keras API. You'll work with a common classification dataset, learn to preprocess data, define network architecture, compile, train, and assess your model's performance using various metrics and visualizations. This is a foundational exercise for anyone venturing into deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tLykdcHzC50"
      },
      "source": [
        "## Part 1: Data Preparation (30 points)\n",
        "\n",
        "We'll use the Fashion MNIST dataset, which is a good substitute for MNIST for simple classification tasks and helps avoid over-reliance on the original digit dataset. It consists of 28x28 grayscale images of fashion items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuQmDoUMzC51"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore') # Suppress warnings for cleaner output\n",
        "np.random.seed(42) # for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1.1 Load the Fashion MNIST Dataset\n",
        "#    Use `tf.keras.datasets.fashion_mnist.load_data()` to load the training and test sets.\n",
        "\n",
        "print(\"\\n--- Loading Fashion MNIST Dataset ---\")\n",
        "# TODO: Load Fashion MNIST data\n",
        "# (X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "print(f\"Raw Training Data Shape: {X_train_raw.shape}, Labels Shape: {y_train_raw.shape}\")\n",
        "print(f\"Raw Test Data Shape: {X_test_raw.shape}, Labels Shape: {y_test_raw.shape}\")\n",
        "\n",
        "# Define class names for better visualization (optional but good practice)\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# 1.2 Data Preprocessing\n",
        "#    a. **Normalization:** Scale the pixel values of both training and test images to be between 0 and 1.\n",
        "#    b. **Reshaping:** Flatten the 28x28 images into 1D arrays (vectors) of 784 pixels. This is necessary for a `Dense` layer.\n",
        "#    c. **One-Hot Encoding Labels:** Convert integer labels (0-9) into one-hot encoded vectors. For example, '2' becomes `[0,0,1,0,0,0,0,0,0,0]`.\n",
        "\n",
        "print(\"\\n--- Preprocessing Data ---\")\n",
        "\n",
        "# a. Normalization\n",
        "# TODO: Normalize pixel values\n",
        "# X_train_norm = X_train_raw / 255.0\n",
        "# X_test_norm = X_test_raw / 255.0\n",
        "print(f\"Normalized Pixel Max (Train): {X_train_norm.max()}, Min: {X_train_norm.min()}\")\n",
        "\n",
        "# b. Reshaping (Flattening)\n",
        "# TODO: Flatten images\n",
        "# X_train_flat = X_train_norm.reshape(X_train_norm.shape[0], -1) # -1 infers dimension\n",
        "# X_test_flat = X_test_norm.reshape(X_test_norm.shape[0], -1)\n",
        "print(f\"Flattened Training Data Shape: {X_train_flat.shape}\")\n",
        "print(f\"Flattened Test Data Shape: {X_test_flat.shape}\")\n",
        "\n",
        "# c. One-Hot Encoding Labels\n",
        "# TODO: One-hot encode labels\n",
        "# encoder = OneHotEncoder(sparse_output=False)\n",
        "# y_train_ohe = encoder.fit_transform(y_train_raw.reshape(-1, 1))\n",
        "# y_test_ohe = encoder.transform(y_test_raw.reshape(-1, 1))\n",
        "print(f\"One-Hot Encoded Training Labels Shape: {y_train_ohe.shape}\")\n",
        "print(f\"One-Hot Encoded Test Labels Shape: {y_test_ohe.shape}\")\n",
        "\n",
        "# 1.3 Display Sample Images (Optional but recommended)\n",
        "#    Plot a few sample images from the training set with their corresponding labels to verify preprocessing.\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_train_raw[i], cmap=plt.cm.binary)\n",
        "    # Get original integer label, then use class_names\n",
        "    plt.xlabel(class_names[y_train_raw[i]])\n",
        "plt.suptitle(\"Sample Fashion MNIST Images\")\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajNSx-d5zC52"
      },
      "source": [
        "## Part 2: Building the Neural Network Model (25 points)\n",
        "\n",
        "You will now define the architecture of a simple feed-forward neural network using Keras's Sequential API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUOQtHawzC53"
      },
      "outputs": [],
      "source": [
        "# 2.1 Define the Model Architecture\n",
        "#    Create a `tf.keras.Sequential` model with the following layers:\n",
        "#    - An `InputLayer` or specify `input_shape` in the first `Dense` layer.\n",
        "#    - One `Dense` hidden layer with at least 128 neurons and `relu` activation.\n",
        "#    - An output `Dense` layer with 10 neurons (for 10 classes) and `softmax` activation.\n",
        "\n",
        "print(\"\\n--- Building Neural Network Model ---\")\n",
        "\n",
        "# TODO: Build the Sequential model\n",
        "# model = keras.Sequential([\n",
        "#     keras.layers.Dense(128, activation='relu', input_shape=(784,)), # Input layer + 1st hidden layer\n",
        "#     keras.layers.Dense(10, activation='softmax') # Output layer\n",
        "# ])\n",
        "\n",
        "# 2.2 Compile the Model\n",
        "#    Configure the model for training with:\n",
        "#    - An `optimizer` (e.g., `'adam'` or `tf.keras.optimizers.Adam()`).\n",
        "#    - A `loss` function appropriate for multi-class classification with one-hot encoded labels (`'categorical_crossentropy'`).\n",
        "#    - `metrics` to monitor during training (e.g., `['accuracy']`).\n",
        "\n",
        "# TODO: Compile the model\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# 2.3 Display Model Summary\n",
        "#    Print the model summary to see the layers, output shapes, and number of parameters.\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mChAc29zC53"
      },
      "source": [
        "## Part 3: Training the Neural Network (20 points)\n",
        "\n",
        "Train your compiled model on the prepared training data and monitor its performance on a validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Db8IBCWzC53"
      },
      "outputs": [],
      "source": [
        "# 3.1 Train the Model\n",
        "#    Use `model.fit()` to train the model.\n",
        "#    - `epochs`: Choose a reasonable number (e.g., 10-20).\n",
        "#    - `batch_size`: Choose a common batch size (e.g., 32, 64, 128).\n",
        "#    - `validation_split`: Use a portion of the training data for validation (e.g., 0.2).\n",
        "#    Store the returned `history` object for plotting.\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "\n",
        "print(f\"\\n--- Training Model for {epochs} epochs with batch size {batch_size} ---\")\n",
        "# TODO: Train the model\n",
        "# history = model.fit(X_train_flat, y_train_ohe,\n",
        "#                     epochs=epochs,\n",
        "#                     batch_size=batch_size,\n",
        "#                     validation_split=0.2, # Use 20% of training data for validation\n",
        "#                     verbose=1) # Set verbose to 1 for progress bar\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ6aSQuozC53"
      },
      "source": [
        "## Part 4: Model Evaluation (25 points)\n",
        "\n",
        "Assess your trained model's performance on unseen test data using various metrics and visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1vaR6SizC54"
      },
      "outputs": [],
      "source": [
        "# 4.1 Evaluate on Test Data\n",
        "#    Use `model.evaluate()` to get the final loss and accuracy on the `X_test_flat` and `y_test_ohe`.\n",
        "\n",
        "print(\"\\n--- Evaluating Model on Test Data ---\")\n",
        "# TODO: Evaluate the model\n",
        "# test_loss, test_accuracy = model.evaluate(X_test_flat, y_test_ohe, verbose=2)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# 4.2 Plot Training History\n",
        "#    Plot the training and validation loss over epochs.\n",
        "#    Plot the training and validation accuracy over epochs.\n",
        "#    These plots help diagnose overfitting or underfitting.\n",
        "\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "accuracy_values = history_dict['accuracy']\n",
        "val_accuracy_values = history_dict['val_accuracy']\n",
        "\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# TODO: Plot training and validation loss\n",
        "# plt.plot(epochs_range, loss_values, 'bo', label='Training loss')\n",
        "# plt.plot(epochs_range, val_loss_values, 'b', label='Validation loss')\n",
        "# plt.title('Training and Validation Loss')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# TODO: Plot training and validation accuracy\n",
        "# plt.plot(epochs_range, accuracy_values, 'bo', label='Training accuracy')\n",
        "# plt.plot(epochs_range, val_accuracy_values, 'b', label='Validation accuracy')\n",
        "# plt.title('Training and Validation Accuracy')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4.3 Make Predictions and Visualize Results (Tougher Aspect)\n",
        "#    Make predictions on the `X_test_flat`.\n",
        "#    Convert probabilities to class labels.\n",
        "#    Display a few sample test images along with their predicted and actual labels.\n",
        "#    Highlight if a prediction is incorrect.\n",
        "\n",
        "print(\"\\n--- Making Predictions and Visualizing Samples ---\")\n",
        "# TODO: Make predictions\n",
        "# predictions_proba = model.predict(X_test_flat)\n",
        "# predicted_labels = np.argmax(predictions_proba, axis=1)\n",
        "# true_labels = np.argmax(y_test_ohe, axis=1) # Convert one-hot back to integer labels\n",
        "\n",
        "num_samples_to_show = 10\n",
        "plt.figure(figsize=(15, 6))\n",
        "for i in range(num_samples_to_show):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_test_raw[i], cmap=plt.cm.binary)\n",
        "\n",
        "    true_label = class_names[true_labels[i]]\n",
        "    pred_label = class_names[predicted_labels[i]]\n",
        "\n",
        "    color = 'green' if predicted_labels[i] == true_labels[i] else 'red'\n",
        "    plt.xlabel(f\"True: {true_label}\\nPred: {pred_label}\", color=color)\n",
        "plt.suptitle(\"Sample Test Predictions (Green=Correct, Red=Incorrect)\")\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# 4.4 Generate Classification Report and Confusion Matrix (Optional but highly recommended)\n",
        "#    Provide a more detailed breakdown of performance per class.\n",
        "\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "# TODO: Print classification report\n",
        "# print(classification_report(true_labels, predicted_labels, target_names=class_names))\n",
        "\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "# TODO: Plot confusion matrix\n",
        "# cm = confusion_matrix(true_labels, predicted_labels)\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "# plt.xlabel('Predicted Label')\n",
        "# plt.ylabel('True Label')\n",
        "# plt.title('Confusion Matrix')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm4I3PAbzC54"
      },
      "source": [
        "## Part 5: Reflection and Further Exploration (5 points)\n",
        "\n",
        "Answer the following questions based on your experience in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hpeQmSrzC54"
      },
      "source": [
        "### Your Answers to Reflection Questions:\n",
        "\n",
        "1.  **How did the training and validation loss/accuracy change over epochs? What might this indicate about your model (e.g., underfitting, overfitting, good fit)?**\n",
        "\n",
        "    _(Your answer here)_\n",
        "\n",
        "2.  **What is the purpose of the `softmax` activation function in the output layer for this multi-class classification problem?**\n",
        "\n",
        "    _(Your answer here)_\n",
        "\n",
        "3.  **If you observed signs of overfitting in your model, what are two common techniques you could apply to mitigate it in a neural network?**\n",
        "\n",
        "    * **Technique 1:** _(Name and brief explanation)_\n",
        "    * **Technique 2:** _(Name and brief explanation)_\n",
        "\n",
        "4.  **For image data like Fashion MNIST, why might a simple feed-forward neural network (like the one you built) be less effective than a Convolutional Neural Network (CNN)?**\n",
        "\n",
        "    _(Your answer here)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fjqd6PAzC54"
      },
      "source": [
        "## Deliverables:\n",
        "\n",
        "1.  This completed Jupyter Notebook (`tensorflow_basic_nn_assignment.ipynb`) with all code cells executed and reflection questions answered.\n",
        "2.  Ensure all plots are clearly visible and well-labeled within the notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}