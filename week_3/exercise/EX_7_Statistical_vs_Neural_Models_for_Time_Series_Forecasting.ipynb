{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hs3glfo3jhZ"
      },
      "source": [
        "# Python Assignment: Statistical vs. Neural Models for Time Series Forecasting\n",
        "\n",
        "This assignment challenges you to compare the performance of a traditional statistical time series model (SARIMA) against a deep learning model (LSTM) on the same dataset. You will go through data preparation, model building, training, and evaluation for both approaches, finally providing a comparative analysis of their strengths and weaknesses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9Ee1w6V3jhb"
      },
      "source": [
        "## Part 1: Data Acquisition and Preparation (30 points)\n",
        "\n",
        "We will use the 'Daily Minimum Temperatures in Melbourne, Australia, 1981-1990' dataset. This is a classic univariate time series dataset with clear seasonality and trend, making it suitable for both types of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dxeDyCR3jhb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore') # Suppress warnings for cleaner output\n",
        "np.random.seed(42) # for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1.1 Load and Inspect Data\n",
        "#    The dataset can be found at: https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-minimum-temperatures.csv\n",
        "#    Load it directly, parse dates, and set 'Date' as the index.\n",
        "#    Make sure the 'Temp' column is numeric.\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-minimum-temperatures.csv'\n",
        "\n",
        "print(\"\\n--- Loading Daily Minimum Temperatures Dataset ---\")\n",
        "# TODO: Load the dataset\n",
        "# df = pd.read_csv(url, header=0, index_col=0, parse_dates=True, squeeze=True)\n",
        "# df.index.name = 'Date'\n",
        "# df.name = 'Temperature'\n",
        "# df = df.astype(float)\n",
        "\n",
        "print(\"Data Head:\\n\", df.head())\n",
        "print(\"\\nData Info:\")\n",
        "df.info()\n",
        "\n",
        "# 1.2 Visualize the Time Series\n",
        "#    Plot the entire time series to observe trends, seasonality, and any anomalies.\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "# TODO: Plot the time series\n",
        "# df.plot()\n",
        "# plt.title('Daily Minimum Temperatures (1981-1990)')\n",
        "# plt.xlabel('Date')\n",
        "# plt.ylabel('Temperature (Celsius)')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# 1.3 Split Data into Training and Testing Sets\n",
        "#    Split the data into training and testing sets, maintaining temporal order.\n",
        "#    Use the first ~8 years (e.g., up to '1988-12-31') for training and the remaining for testing.\n",
        "\n",
        "train_end_date = '1988-12-31'\n",
        "train_data = df.loc[:train_end_date].copy()\n",
        "test_data = df.loc[train_end_date:].copy() # Includes 1989-01-01 onwards\n",
        "\n",
        "print(f\"\\nTraining data points: {len(train_data)}\")\n",
        "print(f\"Test data points: {len(test_data)}\")\n",
        "\n",
        "# 1.4 Data Preparation for Neural Network (Scaling and Sequencing)\n",
        "#    - **Scaling:** Apply `MinMaxScaler` to the *training data only* and transform both training and testing data.\n",
        "#    - **Sequencing:** Create input-output sequences for the LSTM.\n",
        "#      Use `n_steps_in` past observations to predict `n_steps_out` future observations.\n",
        "\n",
        "print(\"\\n--- Preparing Data for Neural Network ---\")\n",
        "# TODO: Scale data\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# scaled_train_data = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
        "# scaled_test_data = scaler.transform(test_data.values.reshape(-1, 1))\n",
        "\n",
        "def create_sequences(data, n_steps_in, n_steps_out):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
        "        # TODO: Define input sequence (X_seq)\n",
        "        # X_seq = data[i:(i + n_steps_in), 0]\n",
        "        # TODO: Define output sequence (y_seq)\n",
        "        # y_seq = data[(i + n_steps_in):(i + n_steps_in + n_steps_out), 0]\n",
        "\n",
        "        # X.append(X_seq)\n",
        "        # y.append(y_seq)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "n_steps_in = 30 # Use 30 past days to predict\n",
        "n_steps_out = 1 # Predict next 1 day\n",
        "\n",
        "# Create sequences for training and testing\n",
        "X_train_seq, y_train_seq = create_sequences(scaled_train_data, n_steps_in, n_steps_out)\n",
        "X_test_seq, y_test_seq = create_sequences(scaled_test_data, n_steps_in, n_steps_out)\n",
        "\n",
        "# Reshape X for LSTM input: (samples, timesteps, features)\n",
        "X_train_seq = X_train_seq.reshape(X_train_seq.shape[0], X_train_seq.shape[1], 1)\n",
        "X_test_seq = X_test_seq.reshape(X_test_seq.shape[0], X_test_seq.shape[1], 1)\n",
        "\n",
        "print(f\"NN Train set: {X_train_seq.shape}, {y_train_seq.shape}\")\n",
        "print(f\"NN Test set: {X_test_seq.shape}, {y_test_seq.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGeROKBR3jhd"
      },
      "source": [
        "## Part 2: Statistical Model: SARIMA (30 points)\n",
        "\n",
        "You will build, train, and forecast using a Seasonal Autoregressive Integrated Moving Average (SARIMA) model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg-afSOT3jhe"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Training SARIMA Model ---\")\n",
        "\n",
        "# 2.1 Stationarity Test (Optional but good practice)\n",
        "#    Perform an Augmented Dickey-Fuller test on the training data to check for stationarity.\n",
        "#    (You might need to difference the data if it's not stationary, though SARIMA handles differencing internally with 'd' and 'D').\n",
        "\n",
        "# result = adfuller(train_data)\n",
        "# print('ADF Statistic: %f' % result[0])\n",
        "# print('p-value: %f' % result[1])\n",
        "# print('Critical Values:')\n",
        "# for key, value in result[4].items():\n",
        "#     print('\\t%s: %.3f' % (key, value))\n",
        "\n",
        "# 2.2 Determine SARIMA Orders (p,d,q)(P,D,Q,s)\n",
        "#    Based on the dataset, a seasonal period 's' of 365 (daily data with annual seasonality) is appropriate.\n",
        "#    For simplicity, we'll provide typical orders, but in a real scenario, you'd use ACF/PACF plots or `auto_arima`.\n",
        "#    For this dataset, a common starting point is (1,1,1)(1,1,1,365).\n",
        "\n",
        "order = (1, 1, 1)      # (p, d, q)\n",
        "seasonal_order = (1, 1, 1, 365) # (P, D, Q, s)\n",
        "\n",
        "# 2.3 Initialize and Fit SARIMA Model\n",
        "#    Fit the SARIMA model on the `train_data`.\n",
        "\n",
        "# TODO: Initialize and fit SARIMA model\n",
        "# sarima_model = SARIMAX(train_data, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
        "# sarima_results = sarima_model.fit(disp=False)\n",
        "# print(sarima_results.summary())\n",
        "\n",
        "# 2.4 Make Forecasts\n",
        "#    Generate predictions for the entire `test_data` period.\n",
        "\n",
        "start_index = len(train_data)\n",
        "end_index = len(df) - 1 # Forecast up to the end of the original DataFrame\n",
        "\n",
        "# TODO: Make predictions\n",
        "# sarima_predictions = sarima_results.predict(start=start_index, end=end_index)\n",
        "\n",
        "# Align predictions with test_data index\n",
        "# sarima_predictions.index = test_data.index\n",
        "print(\"SARIMA Predictions Head:\\n\", sarima_predictions.head())\n",
        "\n",
        "# 2.5 Evaluate SARIMA Performance\n",
        "#    Calculate RMSE and MAE for the SARIMA model on the test set.\n",
        "\n",
        "rmse_sarima = np.sqrt(mean_squared_error(test_data, sarima_predictions))\n",
        "mae_sarima = mean_absolute_error(test_data, sarima_predictions)\n",
        "\n",
        "print(f\"\\nSARIMA RMSE: {rmse_sarima:.4f}\")\n",
        "print(f\"SARIMA MAE: {mae_sarima:.4f}\")\n",
        "\n",
        "# 2.6 Plot SARIMA Forecast\n",
        "#    Plot the actual test data alongside the SARIMA predictions.\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "# TODO: Plot SARIMA forecast\n",
        "# plt.plot(train_data.index, train_data, label='Training Data', color='gray')\n",
        "# plt.plot(test_data.index, test_data, label='Actual Test Data', color='blue')\n",
        "# plt.plot(sarima_predictions.index, sarima_predictions, label='SARIMA Predictions', color='red', linestyle='--')\n",
        "# plt.title('SARIMA Model: Actual vs. Predicted Temperatures')\n",
        "# plt.xlabel('Date')\n",
        "# plt.ylabel('Temperature (Celsius)')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFo1-pMU3jhf"
      },
      "source": [
        "## Part 3: Neural Model: LSTM (30 points)\n",
        "\n",
        "You will build, train, and forecast using an LSTM neural network on the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O5bXhGz3jhf"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Training LSTM Model ---\")\n",
        "\n",
        "# 3.1 Define the LSTM Architecture\n",
        "#    Create a `tf.keras.Sequential` model using `X_train_seq.shape[1]` for `input_shape`.\n",
        "#    Include at least one `LSTM` layer, potentially `Dropout`, and a final `Dense` layer.\n",
        "\n",
        "# TODO: Build the Sequential LSTM model\n",
        "# model_lstm = keras.Sequential([\n",
        "#     layers.LSTM(50, activation='relu', input_shape=(X_train_seq.shape[1], 1)),\n",
        "#     layers.Dropout(0.2),\n",
        "#     layers.Dense(n_steps_out)\n",
        "# ])\n",
        "\n",
        "# 3.2 Compile the Model\n",
        "#    Use `'adam'` optimizer and `'mse'` loss, monitoring `['mae']`.\n",
        "\n",
        "# TODO: Compile the model\n",
        "# model_lstm.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# 3.3 Display Model Summary\n",
        "model_lstm.summary()\n",
        "\n",
        "# 3.4 Train the Model\n",
        "#    Train the LSTM model on `X_train_seq` and `y_train_seq`.\n",
        "#    Use a `validation_split` and consider `EarlyStopping`.\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "# early_stopping_lstm = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "print(f\"\\n--- Training LSTM for {epochs} epochs ---\")\n",
        "# TODO: Train the model\n",
        "# history_lstm = model_lstm.fit(X_train_seq, y_train_seq,\n",
        "#                               epochs=epochs,\n",
        "#                               batch_size=batch_size,\n",
        "#                               validation_split=0.1,\n",
        "#                               # callbacks=[early_stopping_lstm], # Uncomment if using early stopping\n",
        "#                               verbose=1)\n",
        "\n",
        "# 3.5 Make Predictions and Inverse Transform\n",
        "#    Generate predictions on `X_test_seq` and inverse transform them using the `scaler`.\n",
        "#    Also, inverse transform `y_test_seq` for comparison.\n",
        "\n",
        "print(\"\\n--- Making LSTM Predictions and Inverse Transforming ---\")\n",
        "# TODO: Make predictions\n",
        "# y_pred_lstm_scaled = model_lstm.predict(X_test_seq)\n",
        "\n",
        "# TODO: Inverse transform\n",
        "# y_pred_lstm = scaler.inverse_transform(y_pred_lstm_scaled)\n",
        "# y_test_actual_lstm = scaler.inverse_transform(y_test_seq)\n",
        "\n",
        "# Create an index for LSTM predictions, aligning with the actual test data start\n",
        "# Note: The LSTM test set starts `n_steps_in` days *after* the raw test_data starts\n",
        "# (due to sequence creation). Adjusting index for plotting clarity.\n",
        "lstm_test_index_start = test_data.index[n_steps_in]\n",
        "lstm_test_index_end = test_data.index[n_steps_in + len(y_pred_lstm) - 1]\n",
        "lstm_prediction_dates = pd.date_range(start=lstm_test_index_start, end=lstm_test_index_end)\n",
        "\n",
        "print(\"LSTM Predictions Head (original scale):\\n\", pd.Series(y_pred_lstm.flatten()[:5], index=lstm_prediction_dates[:5]))\n",
        "\n",
        "# 3.6 Evaluate LSTM Performance\n",
        "#    Calculate RMSE and MAE for the LSTM model on the original scale.\n",
        "\n",
        "rmse_lstm = np.sqrt(mean_squared_error(y_test_actual_lstm, y_pred_lstm))\n",
        "mae_lstm = mean_absolute_error(y_test_actual_lstm, y_pred_lstm)\n",
        "\n",
        "print(f\"\\nLSTM RMSE: {rmse_lstm:.4f}\")\n",
        "print(f\"LSTM MAE: {mae_lstm:.4f}\")\n",
        "\n",
        "# 3.7 Plot LSTM Forecast\n",
        "#    Plot the actual test data (aligned with LSTM predictions) alongside the LSTM predictions.\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "# TODO: Plot LSTM forecast\n",
        "# plt.plot(train_data.index, train_data, label='Training Data', color='gray')\n",
        "# plt.plot(test_data.index, test_data, label='Actual Test Data', color='blue')\n",
        "# plt.plot(lstm_prediction_dates, y_pred_lstm, label='LSTM Predictions', color='green', linestyle='--')\n",
        "# plt.title('LSTM Model: Actual vs. Predicted Temperatures')\n",
        "# plt.xlabel('Date')\n",
        "# plt.ylabel('Temperature (Celsius)')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKxImrF03jhg"
      },
      "source": [
        "## Part 4: Comparative Analysis (10 points)\n",
        "\n",
        "Synthesize your findings and compare the two models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7XuUDuz3jhg"
      },
      "source": [
        "### Your Analysis:\n",
        "\n",
        "1.  **Tabulate and compare the RMSE and MAE results for both SARIMA and LSTM models.**\n",
        "\n",
        "    | Model | RMSE | MAE |\n",
        "    |-------|------|-----|\n",
        "    | SARIMA| _(Your RMSE)_ | _(Your MAE)_ |\n",
        "    | LSTM  | _(Your RMSE)_ | _(Your MAE)_ |\n",
        "\n",
        "2.  **Based on the quantitative metrics and visual plots, which model performed better for this dataset? Briefly explain why you think this might be the case.**\n",
        "\n",
        "    _(Your explanation here)_\n",
        "\n",
        "3.  **Discuss the strengths and weaknesses of SARIMA compared to LSTM for time series forecasting, based on your experience in this assignment.**\n",
        "\n",
        "    * **SARIMA Strengths:** _(Your points here)_\n",
        "    * **SARIMA Weaknesses:** _(Your points here)_\n",
        "    * **LSTM Strengths:** _(Your points here)_\n",
        "    * **LSTM Weaknesses:** _(Your points here)_\n",
        "\n",
        "4.  **In what real-world scenarios might you prefer a SARIMA model over an LSTM, even if the LSTM yields slightly better metrics? And vice versa?**\n",
        "\n",
        "    * **Prefer SARIMA when:** _(Your scenarios here)_\n",
        "    * **Prefer LSTM when:** _(Your scenarios here)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlqj2EzU3jhh"
      },
      "source": [
        "## Part 5: Reflection and Advanced Considerations (5 points)\n",
        "\n",
        "Think about the broader implications and next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjsb23_x3jhh"
      },
      "source": [
        "### Your Answers to Reflection Questions:\n",
        "\n",
        "1.  **Hyperparameter Tuning:** Both SARIMA and LSTMs have numerous hyperparameters. Briefly describe how you would approach tuning hyperparameters for each model type to potentially optimize their performance.\n",
        "\n",
        "    * **SARIMA Tuning:** _(Your approach here)_\n",
        "    * **LSTM Tuning:** _(Your approach here)_\n",
        "\n",
        "2.  **Feature Engineering:** This assignment used only the target variable. How could you incorporate additional features (e.g., day of week, month, holidays, external weather forecasts) into both SARIMA and LSTM models to potentially improve their predictive power?\n",
        "\n",
        "    * **SARIMA with Exogenous Regressors:** _(Your explanation here)_\n",
        "    * **LSTM with Multivariate Inputs:** _(Your explanation here)_\n",
        "\n",
        "3.  **Computational Resources:** Briefly comment on the relative computational resources (time, memory) required to train and forecast with a SARIMA model versus a deep LSTM model for this dataset size.\n",
        "\n",
        "    _(Your answer here)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHt-s5Z93jhh"
      },
      "source": [
        "## Deliverables:\n",
        "\n",
        "1.  This completed Jupyter Notebook (`statistical_vs_neural_assignment.ipynb`) with all code cells executed and reflection questions answered.\n",
        "2.  Ensure all plots are clearly visible and well-labeled within the notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}