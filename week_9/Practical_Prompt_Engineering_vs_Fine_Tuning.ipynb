{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#üß™ Practical: Prompt Engineering vs Fine-Tuning"
      ],
      "metadata": {
        "id": "pHJaRXZ_HB6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Objective:\n",
        "# Understand the difference between solving a task via prompt engineering vs. model fine-tuning using small examples.\n",
        "# This practical uses Gemini 1.5 Flash + LangChain to simulate both approaches on a text transformation task.\n",
        "\n",
        "# ‚úÖ Tools Used:\n",
        "# - LangChain\n",
        "# - Gemini 1.5 Flash API (Free)\n",
        "# - Google Colab (runtime)\n"
      ],
      "metadata": {
        "id": "hGE5AXT5HGjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#‚úÖ Use Case: Text Rewriting ‚Äì Convert formal text into casual tone"
      ],
      "metadata": {
        "id": "8HD78IRkHqdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# üì¶ Setup Instructions\n",
        "!pip install -q langchain langchain-google-genai google-generativeai\n",
        "\n"
      ],
      "metadata": {
        "id": "9npsTJLWGG76"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üîê Step 1: Configure Gemini API (Free Tier)"
      ],
      "metadata": {
        "id": "HQaRHVfTHyEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyCDyiafjDZo4pJf36HDz4QQtCgpCe2DD3E\"  # Replace with your own free key\n",
        "\n",
        "# Create LLM client\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.7,\n",
        "    convert_system_message_to_human=True\n",
        ")"
      ],
      "metadata": {
        "id": "nJRCZO4jHMpr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üß† Step 2: Define the Task\n",
        "Formal Input:"
      ],
      "metadata": {
        "id": "ME_s0HPUIAvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"We would like to inform you that your attendance in the meeting is mandatory for all team members.\"\n"
      ],
      "metadata": {
        "id": "-LPM4RofIALP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üß™ Step 3A: Prompt Engineering Version"
      ],
      "metadata": {
        "id": "EoMrLgjRIFj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Rewrite this sentence in a casual tone, like a friend texting you:\n",
        "\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "response_prompt = llm.invoke(prompt)\n",
        "print(\"üîß Prompt-engineered Output:\\n\", response_prompt.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y8WUf5qIDvY",
        "outputId": "c46191e6-f952-45b1-e3ad-664bc28e0249"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Prompt-engineered Output:\n",
            " Hey! Just FYI, everyone HAS to be at the meeting.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üß™ Step 3B: Simulate Fine-Tuned Behavior via Custom Prompt Template\n",
        "\n",
        "We'll simulate fine-tuning by forcing the LLM to act as if it has been trained for this specific style."
      ],
      "metadata": {
        "id": "JihUvDOTIKOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_style = \"\"\"You are a tone-conversion assistant trained to convert any formal message into a casual tone.\n",
        "Do not explain or change the meaning. Just output the casual version.\n",
        "\n",
        "Formal: {input}\n",
        "Casual:\"\"\"\n",
        "\n",
        "ft_prompt = finetune_style.format(input=text)\n",
        "response_finetuned = llm.invoke(ft_prompt)\n",
        "\n",
        "print(\"üß¨ Fine-tuned Simulation Output:\\n\", response_finetuned.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7O9wWFaIIdH",
        "outputId": "8775a49f-2f54-4984-bbe0-4b189adfee46"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß¨ Fine-tuned Simulation Output:\n",
            " Everyone needs to be at the meeting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üß† Step 4: Compare Results"
      ],
      "metadata": {
        "id": "zqDlLUt2ITDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nOriginal:\\n\", text)\n",
        "print(\"\\nüîß Prompt-Engineered:\\n\", response_prompt.content)\n",
        "print(\"\\nüß¨ Fine-tuned Simulation:\\n\", response_finetuned.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rctw4rAKIRYh",
        "outputId": "82de8ad9-579f-4779-fe00-8f59fdc59258"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original:\n",
            " We would like to inform you that your attendance in the meeting is mandatory for all team members.\n",
            "\n",
            "üîß Prompt-Engineered:\n",
            " Hey! Just FYI, everyone HAS to be at the meeting.  \n",
            "\n",
            "üß¨ Fine-tuned Simulation:\n",
            " Everyone needs to be at the meeting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üß† Step 5: Reflection Questions (Print & Discuss)"
      ],
      "metadata": {
        "id": "6eaTdgP9IV-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reflection = \"\"\"\n",
        "üß† Reflection Prompts:\n",
        "\n",
        "1. Which version (prompted or simulated fine-tuned) feels more natural?\n",
        "2. Which one is easier to reuse at scale in production?\n",
        "3. Would you fine-tune for this task if prompt output was inconsistent?\n",
        "4. When would fine-tuning give *significantly* better results than prompting?\n",
        "\n",
        "üí¨ Discuss with your team or note down your answers.\n",
        "\"\"\"\n",
        "print(reflection)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI7vq7qoIUg-",
        "outputId": "8aa37624-9770-4112-8991-b763458a412a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Reflection Prompts:\n",
            "\n",
            "1. Which version (prompted or simulated fine-tuned) feels more natural?\n",
            "2. Which one is easier to reuse at scale in production?\n",
            "3. Would you fine-tune for this task if prompt output was inconsistent?\n",
            "4. When would fine-tuning give *significantly* better results than prompting?\n",
            "\n",
            "üí¨ Discuss with your team or note down your answers.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#‚úÖ Summary\n",
        "\n",
        "| Method                  | Advantage                   | Limitation                             |\n",
        "| ----------------------- | --------------------------- | -------------------------------------- |\n",
        "| Prompt Engineering      | Cheap, flexible, quick      | Inconsistent across long inputs        |\n",
        "| Fine-Tuning (Simulated) | More reliable on edge cases | Costly, requires data + training infra |\n"
      ],
      "metadata": {
        "id": "ej2r0TzQIY_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n0hXqS8bIXUH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}