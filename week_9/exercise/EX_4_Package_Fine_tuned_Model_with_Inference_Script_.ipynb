{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Package Fine-tuned Model with Inference Script\n",
        "\n",
        "## Objective\n",
        "This assignment focuses on taking a fine-tuned machine learning model (specifically a BERT-like model for text classification from a previous assignment, or a pre-trained one) and packaging it for efficient and reproducible inference. You will create a dedicated inference script, handle dependencies, and demonstrate how to load the model and make predictions."
      ],
      "metadata": {
        "id": "SlhnD3AjAt_t"
      },
      "id": "SlhnD3AjAt_t"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Model & Environment Setup (25 Marks)\n",
        "\n",
        "1.  **Model Acquisition:**\n",
        "    * **Option A (Recommended):** Use the BERT model you fine-tuned in a previous assignment for text classification. Ensure you have the saved model weights and tokenizer.\n",
        "    * **Option B:** If you haven't completed the BERT fine-tuning assignment, select a small, pre-trained Hugging Face model for a text classification task (e.g., `distilbert-base-uncased-finetuned-sst2` for sentiment analysis, or a similar model from the Hugging Face Hub).\n",
        "    * Clearly state which model you are using, its source, and the classification task it performs.\n",
        "\n",
        "2.  **Dataset for Inference Testing:**\n",
        "    * Obtain a small, representative test dataset (separate from any training/validation data) that the model has **not** seen before. This should have at least **20 samples**.\n",
        "    * Store this data in a simple format, e.g., a CSV file or a list of strings, along with their true labels (if available for verification).\n",
        "\n",
        "3.  **Environment Setup:**\n",
        "    * Create a new Python virtual environment for this assignment.\n",
        "    * Install only the necessary libraries for inference (e.g., `transformers`, `torch` (or `tensorflow`), `scikit-learn`, `pandas`, `numpy`). Avoid development dependencies like `accelerate` or `datasets` if they are not strictly needed for inference.\n",
        "    * Provide a `requirements.txt` file specifically for your inference environment."
      ],
      "metadata": {
        "id": "pR3Xg9RuAt_u"
      },
      "id": "pR3Xg9RuAt_u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtLE4E03At_v"
      },
      "outputs": [],
      "source": [
        "# Your code for model acquisition (loading/saving), dataset preparation, and requirements.txt.\n",
        "# Describe your chosen model and inference dataset."
      ],
      "id": "KtLE4E03At_v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Inference Script Development (50 Marks)\n",
        "\n",
        "1.  **Project Structure:**\n",
        "    * Create a clean project directory. It should contain:\n",
        "        * Your saved model (weights and tokenizer files).\n",
        "        * Your `requirements.txt` file.\n",
        "        * Your inference data for testing.\n",
        "        * A new Python file named `inference_script.py`.\n",
        "\n",
        "2.  **`inference_script.py` - Model Loading:**\n",
        "    * In `inference_script.py`, implement a function (e.g., `load_model(model_path)`) that:\n",
        "        * Loads the tokenizer using `AutoTokenizer.from_pretrained()`.\n",
        "        * Loads the model weights using `AutoModelForSequenceClassification.from_pretrained()`.\n",
        "        * Ensures the model is loaded onto the correct device (CPU or GPU).\n",
        "        * Returns both the tokenizer and the model.\n",
        "\n",
        "3.  **`inference_script.py` - Prediction Function:**\n",
        "    * Implement a function (e.g., `predict_sentiment(texts, model, tokenizer, device, batch_size=16)`) that:\n",
        "        * Takes a list of raw text strings as input.\n",
        "        * Tokenizes the texts using the loaded tokenizer (ensure `truncation`, `padding`, `return_tensors`).\n",
        "        * Moves tokenized inputs to the correct device.\n",
        "        * Performs inference using the loaded model.\n",
        "        * Converts the model's logits into predicted class labels (e.g., using `argmax`).\n",
        "        * Handles batching for efficiency if processing multiple texts.\n",
        "        * Returns a list of predicted labels (and optionally probabilities).\n",
        "\n",
        "4.  **`inference_script.py` - Main Execution Block:**\n",
        "    * Implement a main execution block (`if __name__ == \"__main__\":`) in `inference_script.py` that:\n",
        "        * Defines the `model_path` (where your model is saved).\n",
        "        * Loads the model and tokenizer using your `load_model` function.\n",
        "        * Loads your test dataset (e.g., a list of strings).\n",
        "        * Calls your `predict_sentiment` function to get predictions for all test texts.\n",
        "        * Prints the original text, the predicted label, and the true label (if available) for each sample.\n",
        "\n",
        "5.  **Run and Verify:**\n",
        "    * Run `inference_script.py` from your terminal within the activated virtual environment.\n",
        "    * Show the command used to run the script and its full output, demonstrating successful predictions for your test dataset.\n",
        "    * Discuss the performance of your inference script (e.g., speed, accuracy compared to true labels if known)."
      ],
      "metadata": {
        "id": "7tc4_pDHAt_w"
      },
      "id": "7tc4_pDHAt_w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9yiY2kWAt_x"
      },
      "outputs": [],
      "source": [
        "# Your full `inference_script.py` code here.\n",
        "# Command line output of running the script.\n",
        "# Discussion of inference performance."
      ],
      "id": "I9yiY2kWAt_x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Packaging and Reproducibility (25 Marks)\n",
        "\n",
        "1.  **Project Structure Review:**\n",
        "    * Create a simple text file (e.g., `README.md`) that outlines:\n",
        "        * The purpose of this project.\n",
        "        * Instructions on how to set up the environment (`python -m venv .venv`, `pip install -r requirements.txt`).\n",
        "        * Instructions on how to run the inference script (`python inference_script.py`).\n",
        "        * Any assumptions (e.g., GPU availability).\n",
        "\n",
        "2.  **Dependency Management:**\n",
        "    * Ensure your `requirements.txt` is concise and includes *only* the dependencies required for inference. Explain why this is important for packaging and deployment.\n",
        "\n",
        "3.  **Discussion on Model Packaging:**\n",
        "    * Discuss the importance of packaging models with their inference scripts and dependencies for production deployments.\n",
        "    * How does this approach (e.g., a dedicated `inference_script.py` and `requirements.txt`) contribute to reproducibility and ease of deployment compared to directly using a Jupyter notebook for inference?\n",
        "    * What are the next steps you would take to further package this for deployment (e.g., Dockerization, using model serving frameworks like FastAPI/Flask, cloud-specific solutions)?"
      ],
      "metadata": {
        "id": "HRxrhSoNAt_x"
      },
      "id": "HRxrhSoNAt_x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Submit this Jupyter Notebook (.ipynb file) with all cells executed and outputs visible.\n",
        "* Include all necessary project files in a single compressed archive (e.g., `.zip` file) or provide a link to a Git repository:\n",
        "    * `inference_script.py`\n",
        "    * `requirements.txt`\n",
        "    * Your saved model files (tokenizer config, model weights, etc.)\n",
        "    * Your inference test data (e.g., `test_data.csv`)\n",
        "    * `README.md`\n",
        "* Ensure your code is well-commented and easy to understand.\n",
        "* All commands and outputs should be clearly presented as requested.\n",
        "* Make sure your script runs without errors in a clean environment following your instructions."
      ],
      "metadata": {
        "id": "9QWI2TSUAt_x"
      },
      "id": "9QWI2TSUAt_x"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}