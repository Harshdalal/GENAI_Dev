{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Upload Model to Hugging Face Hub and Test via API\n",
        "\n",
        "## Objective\n",
        "This assignment focuses on democratizing your fine-tuned machine learning model by uploading it to the Hugging Face Hub. You will learn to prepare your model for upload, push it to a new repository, and then interact with it via the Hugging Face Inference API, making your model publicly accessible and testable."
      ],
      "metadata": {
        "id": "Ob7sjBAhC5tO"
      },
      "id": "Ob7sjBAhC5tO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Model & Environment Setup (25 Marks)\n",
        "\n",
        "1.  **Model Acquisition:**\n",
        "    * **Option A (Recommended):** Use the fine-tuned BERT model (or LoRA adapter for a small GPT model) from your previous assignments. Ensure you have the saved model and tokenizer files locally.\n",
        "    * **Option B:** If you haven't completed those assignments, fine-tune a small text classification model (e.g., `distilbert-base-uncased` on `imdb` or `ag_news` datasets) or a small generative model (`distilgpt2` on a simple text generation task) for this assignment. Alternatively, you can use a pre-trained model like `distilbert-base-uncased-finetuned-sst2` if fine-tuning is not feasible, but you'll need to re-save it to simulate a local fine-tuned model.\n",
        "    * Clearly state which model you are using, its source, and the task it performs.\n",
        "\n",
        "2.  **Hugging Face Account & Token:**\n",
        "    * Create a Hugging Face account if you don't have one ([huggingface.co/join](https://huggingface.co/join)).\n",
        "    * Generate a new **write** access token from your Hugging Face settings (Settings -> Access Tokens -> New token). Keep this token secure.\n",
        "    * Use `huggingface-cli login` in your terminal or `from huggingface_hub import login; login()` in your notebook to log in with your token.\n",
        "\n",
        "3.  **Environment Setup:**\n",
        "    * Create a new Python virtual environment.\n",
        "    * Install necessary libraries: `transformers`, `torch` (or `tensorflow`), `huggingface_hub`, `requests`.\n",
        "    * Provide a `requirements.txt` file."
      ],
      "metadata": {
        "id": "_xWtUBqWC5tQ"
      },
      "id": "_xWtUBqWC5tQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MOcD0PNC5tR"
      },
      "outputs": [],
      "source": [
        "# Your code for model acquisition (loading/saving if applicable), requirements.txt.\n",
        "# Description of your chosen model and task.\n",
        "# Hugging Face login (can be done directly here or via CLI, but show confirmation)."
      ],
      "id": "7MOcD0PNC5tR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Uploading the Model to Hugging Face Hub (40 Marks)\n",
        "\n",
        "1.  **Prepare Model for Upload:**\n",
        "    * If your model is already saved, ensure all necessary files (e.g., `pytorch_model.bin` or `tf_model.h5`, `config.json`, `tokenizer.json`, `vocab.txt`, `tokenizer_config.json`, `special_tokens_map.json`) are in a single directory.\n",
        "    * For LoRA adapters, save them using `peft_model.save_pretrained(\"my_lora_adapters\")`.\n",
        "\n",
        "2.  **Create a New Repository on Hugging Face Hub:**\n",
        "    * Define a unique `repo_id` for your model (e.g., `your-username/my-fine-tuned-classifier`).\n",
        "    * Use `create_repo` from `huggingface_hub` to create a new public (or private if preferred) repository on the Hub.\n",
        "    * Print the URL of the newly created repository.\n",
        "\n",
        "3.  **Upload Model Files:**\n",
        "    * Use the `push_to_hub` method directly on your loaded `AutoModel` and `AutoTokenizer` instances.\n",
        "        * Example: `model.push_to_hub(repo_id)` and `tokenizer.push_to_hub(repo_id)`.\n",
        "    * Alternatively, use `upload_folder` from `huggingface_hub` to push all files from a local directory to the repository.\n",
        "    * Confirm the successful upload.\n",
        "\n",
        "4.  **Add a Model Card (Optional but Recommended - 5 Marks):**\n",
        "    * After uploading, go to your model's repository on the Hugging Face Hub.\n",
        "    * Edit the `README.md` file (model card) to include:\n",
        "        * A brief description of the model and its task.\n",
        "        * Information about the dataset it was fine-tuned on.\n",
        "        * How to use the model for inference (e.g., code snippet).\n",
        "        * Any known limitations or biases.\n",
        "    * Provide a screenshot of your model card on the Hugging Face Hub.\n",
        "\n",
        "5.  **Verify Upload:**\n",
        "    * Navigate to your model's repository on the Hugging Face Hub in your web browser.\n",
        "    * Verify that all files have been uploaded correctly.\n",
        "    * Use the built-in \"Inference API\" widget on the model page to send a sample input and observe the output. Take a screenshot of a successful inference using this widget."
      ],
      "metadata": {
        "id": "Q3WbjYmBC5tR"
      },
      "id": "Q3WbjYmBC5tR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNFw72jtC5tS"
      },
      "outputs": [],
      "source": [
        "# Your code for preparing model files and creating the repository.\n",
        "# Code for uploading the model and tokenizer to the Hub.\n",
        "# Screenshots of your model card and successful Inference API widget usage."
      ],
      "id": "dNFw72jtC5tS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Testing from API (35 Marks)\n",
        "\n",
        "1.  **Accessing the Inference API Endpoint:**\n",
        "    * Find the Inference API endpoint URL for your model on its Hugging Face Hub page (usually under the \"Deploy\" -> \"Inference API\" tab, or directly from the model page widget).\n",
        "    * Note: You might need to authenticate with your Hugging Face token for private models or if you hit rate limits on public models.\n",
        "\n",
        "2.  **Develop a Python API Test Script:**\n",
        "    * Write a Python script (or directly in the notebook) that:\n",
        "        * Defines your Hugging Face `repo_id` and your `HF_TOKEN` (from `os.getenv` or similar).\n",
        "        * Constructs the Inference API URL.\n",
        "        * Uses the `requests` library to send a `POST` request to the API endpoint.\n",
        "        * The request body should be a JSON object containing your input text (e.g., `{\"inputs\": \"Your input text here\"}`). Adjust the structure based on your model type (e.g., for generative models, it might be `{\"inputs\": \"Your prompt here\", \"parameters\": {\"max_new_tokens\": 50}}`).\n",
        "        * Sets the `Authorization` header with your `Bearer HF_TOKEN`.\n",
        "        * Prints the JSON response received from the API.\n",
        "\n",
        "3.  **Test with Multiple Inputs:**\n",
        "    * Send at least **3 distinct test inputs** to your deployed model via the API script.\n",
        "    * For each input, show:\n",
        "        * The input text sent.\n",
        "        * The full JSON response received from the API.\n",
        "        * A brief analysis of the API's output. Does it match your expectations? Are there any errors or unexpected behaviors?\n",
        "\n",
        "4.  **Error Handling (Bonus - 5 Marks):**\n",
        "    * Implement basic error handling in your API test script (e.g., `try-except` for network issues, check HTTP status codes from the API response).\n",
        "    * Demonstrate triggering an error (e.g., sending an invalid input format) and show how your script handles it."
      ],
      "metadata": {
        "id": "R5JOriQYC5tS"
      },
      "id": "R5JOriQYC5tS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROICVhlSC5tS"
      },
      "outputs": [],
      "source": [
        "# Your Python code for accessing and testing the Inference API.\n",
        "# Show inputs, API responses, and analysis for each test case.\n",
        "# (For bonus, add error handling and demonstration.)"
      ],
      "id": "ROICVhlSC5tS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Reflection and Future Work (Bonus - 5 Marks)\n",
        "\n",
        "1.  **Benefits of Hugging Face Hub:**\n",
        "    * Discuss the advantages of uploading models to the Hugging Face Hub for sharing, collaboration, and deployment.\n",
        "    * How does the Inference API simplify model usage for others?\n",
        "\n",
        "2.  **Production Readiness:**\n",
        "    * What are the considerations for using the Hugging Face Inference API for a production application (e.g., rate limits, latency, cost, security)?\n",
        "    * When might you choose to deploy your model on a dedicated cloud service (like Cloud Run, SageMaker, Azure ML) instead of relying solely on the Hugging Face Inference API?\n",
        "\n",
        "3.  **Learnings:**\n",
        "    * Summarize your key learnings from packaging and deploying your model to the Hugging Face Hub."
      ],
      "metadata": {
        "id": "Ss_1kHY6C5tT"
      },
      "id": "Ss_1kHY6C5tT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Submit this Jupyter Notebook (.ipynb file) with all cells executed and outputs visible.\n",
        "* Ensure your code is well-commented and easy to understand.\n",
        "* Provide a `requirements.txt` file listing all dependencies.\n",
        "* Include all requested screenshots directly in the notebook.\n",
        "* Clearly present all API requests and responses.\n",
        "* Make sure your notebook runs without errors, assuming your Hugging Face token is correctly configured."
      ],
      "metadata": {
        "id": "DduW_ZljC5tT"
      },
      "id": "DduW_ZljC5tT"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}