{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Compare Multiple Tuned Outputs using BLEU and ROUGE\n",
        "\n",
        "\n",
        "## Objective\n",
        "This assignment focuses on evaluating and comparing the outputs of different Large Language Model (LLM) configurations or fine-tuning experiments, specifically for text generation tasks, using standard NLP metrics: BLEU and ROUGE. You will learn to prepare reference texts, generate hypotheses from multiple models, and calculate these metrics to quantitatively assess output quality."
      ],
      "metadata": {
        "id": "akKbl29bEble"
      },
      "id": "akKbl29bEble"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Environment Setup and Dataset Preparation (25 Marks)\n",
        "\n",
        "1.  **Environment Setup:**\n",
        "    * Create a new Python virtual environment.\n",
        "    * Install necessary libraries: `transformers`, `torch` (or `tensorflow`), `nltk` (for BLEU), `rouge_score` (for ROUGE), `datasets`, `pandas`, `numpy`.\n",
        "        * If using `nltk`, remember to download `punkt` and `wordnet`:\n",
        "          ```python\n",
        "          import nltk\n",
        "          nltk.download('punkt')\n",
        "          nltk.download('wordnet')\n",
        "          ```\n",
        "    * Provide a `requirements.txt` file.\n",
        "\n",
        "2.  **Dataset Acquisition:**\n",
        "    * Select a small, publicly available dataset suitable for a text generation task. Examples:\n",
        "        * **Summarization:** A subset of XSum or CNN/DailyMail dataset (focus on short summaries).\n",
        "        * **Question Answering:** A subset of SQuAD (where answers are extractive or short generated passages).\n",
        "        * **Dialogue Generation:** A subset of a dialogue dataset where models generate responses.\n",
        "    * **Minimum Requirement:** The dataset should have at least **50-100 samples**, with each sample containing:\n",
        "        * An input text/context for the LLM.\n",
        "        * One or more high-quality **reference (ground truth) outputs** for comparison.\n",
        "    * Load your chosen dataset into a format easily iterable (e.g., Hugging Face `Dataset` object or a list of dictionaries).\n",
        "    * Describe your chosen dataset, its source, number of samples, and the nature of the input/output pairs."
      ],
      "metadata": {
        "id": "oQDSaVAfEblf"
      },
      "id": "oQDSaVAfEblf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks9R5r3LEblg"
      },
      "outputs": [],
      "source": [
        "# Your code for environment setup, dataset loading, and NLTK downloads.\n",
        "# Provide `requirements.txt`.\n",
        "        # Describe your dataset."
      ],
      "id": "Ks9R5r3LEblg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Model Configuration and Output Generation (35 Marks)\n",
        "\n",
        "1.  **Model Selection and Loading:**\n",
        "    * Choose a base GPT-like model (e.g., `distilgpt2`, `gpt2`) for text generation.\n",
        "    * **Define at least three distinct model configurations/\"tunes\"** for text generation. These could be achieved by:\n",
        "        * **Option A (Recommended):** Using the same base model but loading different LoRA adapters (if you completed the LoRA assignment). This simulates different fine-tuning experiments.\n",
        "        * **Option B:** Using the same base model but varying inference parameters significantly (e.g., `do_sample=True, top_k=50, temperature=0.7` vs. `num_beams=4` vs. `do_sample=True, top_p=0.9` etc.). This simulates different decoding strategies.\n",
        "        * **Option C:** Using slightly different base models (e.g., `gpt2` vs. `distilgpt2` vs. `gpt2-medium`).\n",
        "    * Load each model/configuration and its corresponding tokenizer.\n",
        "\n",
        "2.  **Generate Hypotheses:**\n",
        "    * For each of your **three configurations**, iterate through your test dataset.\n",
        "    * For each sample, pass the input text to the LLM and generate an output (hypothesis).\n",
        "    * Store the generated outputs in separate lists for each configuration (e.g., `hypotheses_config_A`, `hypotheses_config_B`, `hypotheses_config_C`).\n",
        "    * Store the reference outputs for the entire dataset in a single list (where each item is a list of reference strings if multiple references exist).\n",
        "    * Ensure consistency in decoding parameters (e.g., `max_new_tokens`) across configurations unless intentionally varied for comparison."
      ],
      "metadata": {
        "id": "6IK7_Kl4Eblh"
      },
      "id": "6IK7_Kl4Eblh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymo0xyLwEbli"
      },
      "outputs": [],
      "source": [
        "# Your code for loading models/configurations and generating hypotheses for each.\n",
        "        # Clearly state the three configurations you are comparing."
      ],
      "id": "Ymo0xyLwEbli"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Metric Calculation and Comparison (30 Marks)\n",
        "\n",
        "1.  **BLEU Score Calculation:**\n",
        "    * Implement a function to calculate the BLEU score using `nltk.translate.bleu_score.sentence_bleu` or `corpus_bleu`.\n",
        "    * Remember that BLEU requires tokenized inputs (lists of words).\n",
        "    * For each of your three hypothesis sets, calculate the overall BLEU score against your reference set.\n",
        "    * Print the BLEU score for each configuration.\n",
        "\n",
        "2.  **ROUGE Score Calculation:**\n",
        "    * Implement a function to calculate ROUGE scores using `rouge_score.rouge_scorer.RougeScorer`.\n",
        "    * Calculate ROUGE-1, ROUGE-2, and ROUGE-L F1-scores for each of your three hypothesis sets against your reference set.\n",
        "    * Print the ROUGE scores (F1-score for each type) for each configuration.\n",
        "\n",
        "3.  **Tabular Comparison:**\n",
        "    * Present all calculated BLEU and ROUGE scores in a clear tabular format (e.g., using a Pandas DataFrame).\n",
        "\n",
        "4.  **Qualitative Analysis and Discussion:**\n",
        "    * Select 2-3 sample inputs from your dataset.\n",
        "    * For each sample, print the input, the reference output, and the generated outputs from all three configurations.\n",
        "    * Based on the quantitative scores (BLEU, ROUGE) and qualitative inspection of the outputs:\n",
        "        * Which configuration performs best according to each metric?\n",
        "        * Do the metrics align with your human intuition? Why or why not?\n",
        "        * Discuss the strengths and weaknesses of BLEU and ROUGE for evaluating text generation. When is one more appropriate than the other?\n",
        "        * What insights did you gain about your different model configurations?"
      ],
      "metadata": {
        "id": "yr2mZtQnEbli"
      },
      "id": "yr2mZtQnEbli"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrffesSCEbli"
      },
      "outputs": [],
      "source": [
        "# Your code for BLEU and ROUGE score calculation.\n",
        "        # Tabular comparison of scores.\n",
        "        # Qualitative analysis with sample outputs and discussion."
      ],
      "id": "YrffesSCEbli"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Reflection and Future Work (Bonus - 10 Marks)\n",
        "\n",
        "1.  **Limitations of N-gram Overlap Metrics:**\n",
        "    * Discuss the inherent limitations of n-gram overlap metrics (like BLEU and ROUGE) for evaluating the semantic quality and fluency of generated text.\n",
        "    * What scenarios might these metrics fail to capture true quality?\n",
        "\n",
        "2.  **Alternative Evaluation Metrics:**\n",
        "    * Suggest at least two alternative or complementary evaluation approaches for text generation (e.g., semantic similarity metrics like BERTScore, human evaluation, LLM-as-a-judge evaluators, task-specific metrics).\n",
        "    * Briefly explain how they work and what advantages they offer.\n",
        "\n",
        "3.  **Hyperparameter Tuning Strategy:**\n",
        "    * How would you use these metrics in an automated hyperparameter tuning loop for a text generation model? Which metric would you prioritize and why?"
      ],
      "metadata": {
        "id": "oFnMx67aEbli"
      },
      "id": "oFnMx67aEbli"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Submit this Jupyter Notebook (.ipynb file) with all cells executed and outputs visible.\n",
        "    * Ensure your code is well-commented and easy to understand.\n",
        "* Provide a `requirements.txt` file listing all dependencies.\n",
        "* Clearly present all calculated scores, tables, and discussions.\n",
        "* Make sure your notebook runs without errors in the specified environment."
      ],
      "metadata": {
        "id": "3dDUzNsmEblj"
      },
      "id": "3dDUzNsmEblj"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}