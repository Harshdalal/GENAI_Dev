{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Fine-tune BERT on a Custom Classification Dataset\n",
        "\n",
        "## Objective\n",
        "This assignment focuses on the practical application of transfer learning in Natural Language Processing by fine-tuning a pre-trained BERT model for a custom text classification task. You will learn to prepare a dataset, configure a BERT model for classification, train it, and evaluate its performance."
      ],
      "metadata": {
        "id": "yySXaNON_NZH"
      },
      "id": "yySXaNON_NZH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Environment Setup and Dataset Preparation (30 Marks)\n",
        "\n",
        "1.  **Environment Setup:**\n",
        "    * Create a new Python virtual environment.\n",
        "    * Install the necessary libraries: `transformers`, `torch` (or `tensorflow` if preferred), `scikit-learn`, `pandas`, `numpy`, `matplotlib`, `seaborn`.\n",
        "        * Provide a `requirements.txt` file listing all dependencies.\n",
        "\n",
        "2.  **Custom Dataset Acquisition and Loading:**\n",
        "    * Select a custom text classification dataset suitable for this assignment. Recommended options:\n",
        "        * **Sentiment Analysis:** (e.g., IMDB movie reviews, Twitter sentiment, Amazon reviews - you might need to combine and label if using raw data).\n",
        "        * **News Classification:** (e.g., AG News, BBC News summary dataset).\n",
        "        * **Spam Detection:** (e.g., SMS Spam Collection Dataset).\n",
        "        * **Topic Classification:** (e.g., a subset of Reddit posts by subreddit, or general articles by topic).\n",
        "    * **Minimum Requirements:** The dataset should have at least **2 classes** and a minimum of **1000 samples** in total (aim for more if possible for better results).\n",
        "    * Load your chosen dataset into a Pandas DataFrame. The DataFrame should have at least two columns: one for the text content and one for the corresponding label.\n",
        "    * Describe your chosen dataset, its source, the number of classes, and the distribution of samples per class.\n",
        "\n",
        "3.  **Data Preprocessing and Splitting:**\n",
        "    * Perform any necessary basic text preprocessing (e.g., handling missing values, uniform case, basic cleaning). *Note: BERT models handle a lot of this internally, so heavy cleaning is often not required, but basic sanity checks are good.*\n",
        "    * Split your dataset into training, validation, and test sets (e.g., 70% train, 15% validation, 15% test). Ensure stratification if your classes are imbalanced.\n",
        "    * Print the shape of each split and the distribution of classes within each split."
      ],
      "metadata": {
        "id": "YyVdEikg_NZI"
      },
      "id": "YyVdEikg_NZI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MkGAg_P_NZK"
      },
      "outputs": [],
      "source": [
        "# Your code for environment setup, dataset loading, and preprocessing here.\n",
        "# Include explanations, dataset description, and split statistics."
      ],
      "id": "-MkGAg_P_NZK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: BERT Tokenization and Model Setup (30 Marks)\n",
        "\n",
        "1.  **Load BERT Tokenizer:**\n",
        "    * Load a pre-trained BERT tokenizer (e.g., `bert-base-uncased` or `distilbert-base-uncased`).\n",
        "    * Explain your choice of tokenizer and its corresponding model.\n",
        "\n",
        "2.  **Tokenization and Input Formatting:**\n",
        "    * Tokenize your training, validation, and test texts using the loaded tokenizer.\n",
        "    * Ensure the tokenization includes:\n",
        "        * `truncation=True`\n",
        "        * `padding='max_length'`\n",
        "        * `return_tensors='pt'` (for PyTorch) or `'tf'` (for TensorFlow).\n",
        "    * Specify a `max_length` that is appropriate for your dataset (e.g., 128 or 256).\n",
        "    * Convert your labels into a suitable numerical format (e.g., using `LabelEncoder` or directly mapping to integers).\n",
        "    * Print the shape of the tokenized outputs (input IDs, attention masks) for a sample of your dataset (e.g., the first 5 samples) and verify their format.\n",
        "\n",
        "3.  **Load Pre-trained BERT Model for Sequence Classification:**\n",
        "    * Load a pre-trained BERT model suitable for sequence classification (e.g., `BertForSequenceClassification` from `transformers`).\n",
        "    * Initialize the model with the correct number of labels for your classification task (`num_labels`).\n",
        "    * Print the model architecture or a summary to understand its layers."
      ],
      "metadata": {
        "id": "o1YTfmHz_NZK"
      },
      "id": "o1YTfmHz_NZK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP25B6HP_NZM"
      },
      "outputs": [],
      "source": [
        "# Your code for loading tokenizer, tokenization, and model setup here.\n",
        "# Include explanations and verifications."
      ],
      "id": "hP25B6HP_NZM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Training and Evaluation (40 Marks)\n",
        "\n",
        "1.  **Define Training Arguments:**\n",
        "    * Use the `TrainingArguments` class from `transformers` to define your training configuration.\n",
        "    * Set parameters such as:\n",
        "        * `output_dir`\n",
        "        * `num_train_epochs` (e.g., 2-4)\n",
        "        * `per_device_train_batch_size` (e.g., 8-32, depends on GPU memory)\n",
        "        * `per_device_eval_batch_size`\n",
        "        * `warmup_steps`, `weight_decay`\n",
        "        * `logging_dir`, `logging_steps`\n",
        "        * `evaluation_strategy='epoch'`\n",
        "        * `save_strategy='epoch'`\n",
        "        * `load_best_model_at_end=True`\n",
        "    * Justify your choices for key training arguments.\n",
        "\n",
        "2.  **Define Metrics:**\n",
        "    * Create a `compute_metrics` function that takes `EvalPrediction` as input and returns a dictionary of metrics.\n",
        "    * Include metrics such as `accuracy`, `precision`, `recall`, and `f1-score` (use `sklearn.metrics`).\n",
        "    * Ensure proper handling for multi-class or binary classification.\n",
        "\n",
        "3.  **Create Trainer and Train:**\n",
        "    * Instantiate the `Trainer` class, passing your model, training arguments, train dataset, eval dataset, and `compute_metrics` function.\n",
        "    * Start the training process using `trainer.train()`.\n",
        "    * Show the training loss and evaluation metrics during training.\n",
        "\n",
        "4.  **Evaluate on Test Set:**\n",
        "    * After training, evaluate your fine-tuned model on the held-out test set using `trainer.evaluate()`.\n",
        "    * Print the final test set metrics (accuracy, precision, recall, f1-score).\n",
        "    * Analyze the results: How well did the model perform? Are there signs of overfitting or underfitting? How does the performance compare to potential baselines?\n",
        "\n",
        "5.  **Qualitative Analysis (Bonus - 5 Marks):**\n",
        "    * Choose 3-5 challenging samples from your test set.\n",
        "    * Predict their labels using your fine-tuned model.\n",
        "    * Compare the predicted label with the true label and the model's confidence. Discuss why the model might have made correct or incorrect predictions for these specific examples."
      ],
      "metadata": {
        "id": "aCJHqFoi_NZM"
      },
      "id": "aCJHqFoi_NZM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QWKLp5c_NZN"
      },
      "outputs": [],
      "source": [
        "# Your code for defining training arguments, metrics, Trainer, training, and evaluation.\n",
        "# Include the final test set metrics and analysis.\n",
        "# (For bonus, add qualitative analysis code and discussion.)"
      ],
      "id": "_QWKLp5c_NZN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Reflection and Future Work (Bonus - 10 Marks)\n",
        "\n",
        "1.  **Challenges Faced:**\n",
        "    * Describe any challenges you encountered during this assignment (e.g., GPU memory issues, hyperparameter tuning, data imbalance) and how you addressed them.\n",
        "\n",
        "2.  **Potential Improvements:**\n",
        "    * Suggest ways to further improve the model's performance. Consider:\n",
        "        * Different BERT variants (e.g., RoBERTa, ELECTRA, larger models).\n",
        "        * Advanced text preprocessing (e.g., handling emojis, slang).\n",
        "        * Hyperparameter optimization techniques (e.g., grid search, random search).\n",
        "        * Data augmentation.\n",
        "        * Class imbalance handling techniques.\n",
        "        * Different classification heads.\n",
        "\n",
        "3.  **Learnings:**\n",
        "    * Summarize your key learnings from fine-tuning BERT for text classification.\n",
        "    * How does transfer learning benefit NLP tasks compared to training from scratch?"
      ],
      "metadata": {
        "id": "ILua2LAQ_NZN"
      },
      "id": "ILua2LAQ_NZN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Submit this Jupyter Notebook (.ipynb file) with all cells executed and outputs visible.\n",
        "* Ensure your code is well-commented and easy to understand.\n",
        "* Provide a `requirements.txt` file listing all dependencies.\n",
        "        * Include a brief `README.md` file (optional but recommended) explaining how to run your code and any specific instructions, especially if your dataset needs specific download steps.\n",
        "* Make sure your notebook runs without errors in the specified environment."
      ],
      "metadata": {
        "id": "WXiPRJQ0_NZN"
      },
      "id": "WXiPRJQ0_NZN"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}