{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Evaluate LLM Outputs using LangSmith Eval Chains\n",
        "\n",
        "## Objective\n",
        "This assignment focuses on using LangSmith's evaluation capabilities, specifically its built-in evaluation chains (like `qa_flesch_kincaid`, `qa_coherence`, `qa_faithfulness`), to automatically assess the quality of LLM-generated outputs. You will learn to set up a LangSmith project, run a custom dataset through an LLM chain, and apply various evaluators to gain insights into your model's performance."
      ],
      "metadata": {
        "id": "tyZowpDLAKMc"
      },
      "id": "tyZowpDLAKMc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Environment Setup and LangSmith Project (20 Marks)\n",
        "\n",
        "1.  **Environment Setup:**\n",
        "    * Create a new Python virtual environment.\n",
        "    * Install necessary libraries: `langchain`, `langsmith`, `openai` (or your preferred LLM client library), `python-dotenv`, `datasets`.\n",
        "        * Provide a `requirements.txt` file.\n",
        "\n",
        "2.  **LangSmith Project Setup:**\n",
        "    * If you don't have a LangSmith account, create one at [https://smith.langchain.com/](https://smith.langchain.com/).\n",
        "    * Create a new LangSmith project for this assignment (e.g., \"LLM_Eval_Assignment\").\n",
        "    * Obtain your LangSmith API Key.\n",
        "    * Create a `.env` file in your project root and add your LangSmith API key (`LANGCHAIN_API_KEY`) and optionally `LANGCHAIN_TRACING_V2=true`.\n",
        "    * Set your project name (`LANGCHAIN_PROJECT`) in the `.env` file.\n",
        "    * Initialize LangChain's tracing by importing `os` and setting environment variables (or by using `load_dotenv()` and letting LangChain pick them up).\n",
        "    * Confirm that LangSmith tracing is enabled by running a trivial LangChain expression and checking your LangSmith project for a new trace."
      ],
      "metadata": {
        "id": "swOHetAxAKMe"
      },
      "id": "swOHetAxAKMe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh0czLO7AKMf"
      },
      "outputs": [],
      "source": [
        "# Your code for environment setup, requirements.txt, and LangSmith project initialization here.\n",
        "# Demonstrate a small LangChain expression to confirm tracing is active."
      ],
      "id": "gh0czLO7AKMf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Custom Dataset and LLM Chain (30 Marks)\n",
        "\n",
        "1.  **Custom Dataset Creation:**\n",
        "    * Create a small dataset for your evaluation. This dataset should consist of input-output pairs suitable for an LLM to generate responses.\n",
        "    * **Recommended Dataset Type:** Question-Answering (QA) pairs or simple text generation prompts.\n",
        "    * **Minimum Requirement:** At least **10-15 examples** for evaluation. Each example should include:\n",
        "        * `question` (str): The input query for the LLM.\n",
        "        * `answer` (str): The expected or ground-truth answer (for comparison).\n",
        "        * (Optional but Recommended) `context` (str): Relevant context that the LLM *should* use to answer the question, simulating a RAG setup.\n",
        "    * Store this dataset as a list of dictionaries.\n",
        "    * Upload this dataset to LangSmith using `client.upload_dataset`.\n",
        "    * Take a screenshot of your uploaded dataset in the LangSmith UI.\n",
        "\n",
        "2.  **LLM Chain Implementation:**\n",
        "    * Choose an LLM provider (e.g., OpenAI, Anthropic, Google Generative AI) and set up your API key in the `.env` file.\n",
        "    * Create a simple LangChain chain that takes a `question` (and optionally `context`) and generates an `answer`.\n",
        "        * A basic `ChatPromptTemplate` with a `ChatModel` is sufficient.\n",
        "        * If you included `context` in your dataset, ensure your prompt incorporates it for a RAG-like behavior.\n",
        "    * Demonstrate your LLM chain by passing a sample input and printing the generated response."
      ],
      "metadata": {
        "id": "Km9_ChyfAKMg"
      },
      "id": "Km9_ChyfAKMg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnwKAQhmAKMh"
      },
      "outputs": [],
      "source": [
        "# Your code for creating the custom dataset and uploading it to LangSmith.\n",
        "# Screenshot of the uploaded dataset in LangSmith UI.\n",
        "# Your code for implementing the LLM chain and demonstrating it with a sample input."
      ],
      "id": "OnwKAQhmAKMh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Running Evaluations with Eval Chains (40 Marks)\n",
        "\n",
        "1.  **Select Evaluators:**\n",
        "    * Import the necessary evaluators from `langchain.evaluation`.\n",
        "    * Select at least **three** distinct evaluators from the following list (or similar if new ones are available):\n",
        "        * `qa_flesch_kincaid` (Readability)\n",
        "        * `qa_coherence` (Coherence)\n",
        "        * `qa_faithfulness` (Faithfulness/Groundedness - requires `context`)\n",
        "        * `qa_answer_relevance` (Answer Relevance - requires `question` and `answer`)\n",
        "        * `qa_conciseness` (Conciseness)\n",
        "        * `LabeledScoreString` (for custom scoring, if you want to define one).\n",
        "    * Briefly explain what each chosen evaluator measures.\n",
        "\n",
        "2.  **Configure and Run Evaluation:**\n",
        "    * Use `run_evaluator` from `langsmith.evaluation`.\n",
        "    * Pass your LLM chain, the name of your uploaded dataset, and a list of the chosen evaluators.\n",
        "    * Set a `concurrency` (e.g., 5) to speed up evaluation if using an API-based LLM for evaluation.\n",
        "    * Run the evaluation.\n",
        "    * Capture the `run_id` returned by `run_evaluator`.\n",
        "\n",
        "3.  **Analyze Results in LangSmith UI:**\n",
        "    * Navigate to your LangSmith project and find the evaluation run associated with the `run_id`.\n",
        "    * Explore the various tabs:\n",
        "        * **Dataset & Runs:** View the overall scores.\n",
        "        * **Examples:** Drill down into individual examples to see the LLM's response, the ground truth, and the scores from each evaluator.\n",
        "        * **Traces:** For each example, inspect the full trace including the LLM call and how evaluators were invoked.\n",
        "    * Take screenshots of:\n",
        "        * The overall evaluation summary table showing scores for all evaluators.\n",
        "        * The detailed view of at least two individual examples, showing the generated output and the scores from the evaluators.\n",
        "\n",
        "4.  **Discussion of Results:**\n",
        "    * Based on the evaluation scores and your qualitative observations from the individual examples, discuss:\n",
        "        * Which aspects of your LLM's output did each evaluator highlight (strengths and weaknesses)?\n",
        "        * How useful are these automated evaluators for understanding LLM performance? What are their limitations?\n",
        "        * Suggest ways to improve the LLM's performance based on the evaluation results."
      ],
      "metadata": {
        "id": "mvlBwjJIAKMh"
      },
      "id": "mvlBwjJIAKMh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yv58-pxAKMi"
      },
      "outputs": [],
      "source": [
        "# Your code for selecting evaluators, configuring, and running the evaluation.\n",
        "# Include explanations of each evaluator.\n",
        "# Screenshots of LangSmith UI with evaluation summary and detailed example views.\n",
        "# Your discussion of the evaluation results."
      ],
      "id": "-yv58-pxAKMi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Reflection and Future Work (Bonus - 10 Marks)\n",
        "\n",
        "1.  **Custom Evaluators:**\n",
        "    * Beyond built-in evaluators, how would you design a custom evaluator for a specific aspect of your LLM's performance not covered by existing ones (e.g., style, tone, specific factual check)? Describe the logic and potential implementation.\n",
        "\n",
        "2.  **Experiment Tracking:**\n",
        "    * How does LangSmith facilitate experiment tracking and comparison of different LLM chains or prompts?\n",
        "\n",
        "3.  **Real-world Application:**\n",
        "    * In a production LLM application, how would you integrate continuous evaluation using tools like LangSmith to monitor performance drift or regression over time?"
      ],
      "metadata": {
        "id": "albSkjquAKMj"
      },
      "id": "albSkjquAKMj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Submit this Jupyter Notebook (.ipynb file) with all cells executed and outputs visible.\n",
        "* Ensure your code is well-commented and easy to understand.\n",
        "* Provide a `requirements.txt` file listing all dependencies.\n",
        "* Include all requested screenshots directly in the notebook or as clearly referenced image files.\n",
        "* Make sure your notebook runs without errors, assuming the LangSmith API key and LLM API key are correctly set up in the `.env` file."
      ],
      "metadata": {
        "id": "1M2jAYS8AKMj"
      },
      "id": "1M2jAYS8AKMj"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}