{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: LoRA-based Adapter Training for a Small GPT Model\n",
        "\n",
        "## Objective\n",
        "This assignment focuses on implementing and running Low-Rank Adaptation (LoRA) for fine-tuning a small Generative Pre-trained Transformer (GPT) model on a custom dataset. You will learn how LoRA enables efficient adaptation of large language models by training only a small number of new parameters, significantly reducing computational costs and memory requirements compared to full fine-tuning."
      ],
      "metadata": {
        "id": "IhETzNxQ_oMn"
      },
      "id": "IhETzNxQ_oMn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Environment Setup and Dataset Preparation (30 Marks)\n",
        "\n",
        "1.  **Environment Setup:**\n",
        "    * Create a new Python virtual environment.\n",
        "    * Install the necessary libraries: `transformers`, `torch`, `peft` (Parameter-Efficient Fine-Tuning library), `datasets`, `accelerate` (for distributed training setup, even if running on a single GPU), `numpy`, `scikit-learn`.\n",
        "        * Provide a `requirements.txt` file listing all dependencies.\n",
        "\n",
        "2.  **Dataset Acquisition and Preprocessing:**\n",
        "    * Select a text dataset suitable for a generative task (e.g., text summarization, dialogue generation, question answering, creative writing).\n",
        "        * **Recommended:** A small, focused dataset to see the effects of LoRA quickly. Examples:\n",
        "            * A subset of a dialogue dataset (e.g., DailyDialog).\n",
        "            * A small collection of simple story prompts and completions.\n",
        "            * Custom QA pairs where the answer is short and direct.\n",
        "    * **Minimum Requirement:** The dataset should contain at least **500 pairs/samples** for training, where each sample is a text input that you want the model to generate a continuation for. Ensure your data is in a format like `{'text': 'input text for generation'}`.\n",
        "    * Load your chosen dataset using the `datasets` library (e.g., `load_dataset` if public, or `load_dataset('text', data_files={'train': 'train.txt', 'test': 'test.txt'})`).\n",
        "    * Describe your chosen dataset, its source, and its characteristics (e.g., average length of input/output, number of samples).\n",
        "\n",
        "3.  **Tokenization:**\n",
        "    * Load a tokenizer for a small GPT-like model (e.g., `gpt2`, `distilgpt2`, `openai-community/gpt2-small`).\n",
        "    * Tokenize your dataset. For generative tasks, you typically concatenate input and target for causal language modeling.\n",
        "        * Handle padding and truncation appropriately (`max_length` should be reasonable, e.g., 128 or 256).\n",
        "        * Set `return_tensors=\"pt\"`.\n",
        "        * Ensure `labels` are set to `input_ids` for causal language modeling, masking special tokens if necessary (though `DataCollatorForLanguageModeling` usually handles this).\n",
        "    * Print a sample of the tokenized input IDs and their corresponding decoded text to verify correctness."
      ],
      "metadata": {
        "id": "bW5k6TTW_oMp"
      },
      "id": "bW5k6TTW_oMp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkVl08-C_oMq"
      },
      "outputs": [],
      "source": [
        "# Your code for environment setup, dataset loading, preprocessing, and tokenization here.\n",
        "# Include dataset description and verification of tokenized data."
      ],
      "id": "LkVl08-C_oMq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: LoRA Configuration and Model Preparation (30 Marks)\n",
        "\n",
        "1.  **Load Base GPT Model:**\n",
        "    * Load a small pre-trained GPT model for causal language modeling (e.g., `AutoModelForCausalLM.from_pretrained(\"gpt2\")` or `\"distilgpt2\"`).\n",
        "    * Ensure the model is moved to your available device (CPU/GPU).\n",
        "    * Print the model's structure to understand its layers.\n",
        "\n",
        "2.  **LoRA Configuration with PEFT:**\n",
        "    * Define a `LoraConfig` object from the `peft` library.\n",
        "    * Key parameters to set and justify:\n",
        "        * `r` (LoRA rank, e.g., 8, 16, 32): The rank of the update matrices. Explain its impact.\n",
        "        * `lora_alpha` (LoRA scaling factor, e.g., 16, 32): Scales the LoRA weights. Explain its purpose.\n",
        "        * `target_modules` (e.g., `[\"q_proj\", \"v_proj\"]`): Which layers in the GPT model to apply LoRA to. Justify your choice based on common practices.\n",
        "        * `lora_dropout` (e.g., 0.1): Dropout probability for LoRA layers.\n",
        "        * `bias` (e.g., \"none\"): Whether to train bias parameters.\n",
        "    * Explain what each chosen parameter signifies in the context of LoRA.\n",
        "\n",
        "3.  **Prepare Model for PEFT Training:**\n",
        "    * Use `peft_model = get_peft_model(model, peft_config)` to wrap your base GPT model with the LoRA adapters.\n",
        "    * Print the trainable parameters of the `peft_model` using `peft_model.print_trainable_parameters()`.\n",
        "    * Compare the number of trainable parameters in the LoRA-wrapped model to the total parameters of the base model. Discuss the significant reduction in trainable parameters and its implications."
      ],
      "metadata": {
        "id": "kMvKgCkX_oMr"
      },
      "id": "kMvKgCkX_oMr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji2BfXqH_oMs"
      },
      "outputs": [],
      "source": [
        "# Your code for loading the base GPT model, defining LoraConfig, and preparing the model with PEFT.\n",
        "# Include justifications for LoRA parameters and a comparison of trainable parameters."
      ],
      "id": "ji2BfXqH_oMs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Training and Evaluation (40 Marks)\n",
        "\n",
        "1.  **Define Training Arguments:**\n",
        "    * Use `TrainingArguments` from `transformers`.\n",
        "    * Set parameters such as:\n",
        "        * `output_dir`\n",
        "        * `num_train_epochs` (e.g., 3-5, depending on dataset size and desired convergence)\n",
        "        * `per_device_train_batch_size` (e.g., 4-16, depending on memory)\n",
        "        * `gradient_accumulation_steps` (if needed for larger effective batch size)\n",
        "        * `learning_rate` (e.g., 2e-4, 5e-5)\n",
        "        * `logging_steps`, `save_steps`\n",
        "        * `evaluation_strategy=\"epoch\"` (or `\"steps\"`)\n",
        "        * `load_best_model_at_end=True`\n",
        "    * Justify your choices for key training arguments.\n",
        "\n",
        "2.  **Create Data Collator:**\n",
        "    * Use `DataCollatorForLanguageModeling` (with `mlm=False` for causal language modeling).\n",
        "\n",
        "3.  **Create Trainer and Train:**\n",
        "    * Instantiate the `Trainer` class with your `peft_model`, `TrainingArguments`, tokenized datasets, and data collator.\n",
        "    * Start the training process using `trainer.train()`.\n",
        "    * Show the training loss and evaluation loss (if applicable) during training.\n",
        "\n",
        "4.  **Qualitative Evaluation:**\n",
        "    * After training, use the fine-tuned `peft_model` to generate text.\n",
        "    * Provide at least **3 distinct prompts** related to your dataset's domain.\n",
        "    * For each prompt, generate text using the `peft_model.generate()` method (remember to set `max_new_tokens`, `do_sample`, `top_k`, `num_beams` if desired).\n",
        "    * Compare the generated text with what you would expect given your fine-tuning dataset.\n",
        "    * Discuss the quality of the generated text and how it reflects the fine-tuning. Does it show signs of adapting to your custom data?\n",
        "\n",
        "5.  **Save and Load LoRA Adapters (Bonus - 5 Marks):**\n",
        "    * Save only the LoRA adapters using `peft_model.save_pretrained(\"my_lora_adapters\")`.\n",
        "    * Load the base GPT model again (without LoRA).\n",
        "    * Load the saved LoRA adapters onto the base model using `PeftModel.from_pretrained(base_model, \"my_lora_adapters\")`.\n",
        "    * Demonstrate text generation with the re-loaded LoRA model to confirm it works correctly.\n",
        "    * Discuss the advantage of saving only the adapters."
      ],
      "metadata": {
        "id": "nhwtNvpa_oMs"
      },
      "id": "nhwtNvpa_oMs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yO5DJtuw_oMt"
      },
      "outputs": [],
      "source": [
        "# Your code for defining training arguments, data collator, Trainer, training, and qualitative evaluation.\n",
        "# Include generated text samples and analysis.\n",
        "# (For bonus, add code for saving/loading adapters and discussion.)"
      ],
      "id": "yO5DJtuw_oMt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Reflection and Future Work (Bonus - 5 Marks)\n",
        "\n",
        "1.  **LoRA Benefits and Limitations:**\n",
        "    * Summarize the key benefits of using LoRA for fine-tuning LLMs, especially for smaller models or limited resources.\n",
        "    * What are potential limitations or scenarios where LoRA might not be the optimal choice?\n",
        "\n",
        "2.  **Hyperparameter Impact:**\n",
        "    * Discuss how changing `r` (LoRA rank) and `lora_alpha` might affect the model's performance and computational cost.\n",
        "\n",
        "3.  **Future Enhancements:**\n",
        "    * Suggest ways to further improve the LoRA fine-tuning process or the quality of generations.\n",
        "    * Consider combining LoRA with other PEFT methods or advanced training techniques."
      ],
      "metadata": {
        "id": "C51R6uF2_oMt"
      },
      "id": "C51R6uF2_oMt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Submit this Jupyter Notebook (.ipynb file) with all cells executed and outputs visible.\n",
        "* Ensure your code is well-commented and easy to understand.\n",
        "* Provide a `requirements.txt` file listing all dependencies.\n",
        "* Clearly present all requested code, outputs, and discussions.\n",
        "* Make sure your notebook runs without errors in the specified environment (ideally with GPU for faster training, but CPU should also work for small models/epochs)."
      ],
      "metadata": {
        "id": "T04Ob4WM_oMu"
      },
      "id": "T04Ob4WM_oMu"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}