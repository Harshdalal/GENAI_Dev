{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Stream Input/Output Logs to BigQuery from Application\n",
        "## Objective\n",
        "This assignment focuses on implementing a robust logging solution for a web application, specifically streaming application input/output (I/O) and request logs directly to Google BigQuery. This allows for powerful analytics, auditing, and debugging of application behavior. You will set up BigQuery, modify a simple application to emit structured logs, and configure Google Cloud's logging mechanisms to route these logs to BigQuery."
      ],
      "metadata": {
        "id": "fjxO2nX7JH7O"
      },
      "id": "fjxO2nX7JH7O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: GCP Setup and Basic Web Application (25 Marks)\n",
        "\n",
        "1.  **GCP Project Setup:**\n",
        "    * Ensure you have an active Google Cloud Platform (GCP) project with billing enabled.\n",
        "    * Enable the following APIs: Cloud Run API, Cloud Logging API, BigQuery API, Artifact Registry API.\n",
        "    * Provide `gcloud services enable` commands for each required API.\n",
        "\n",
        "2.  **BigQuery Dataset and Table:**\n",
        "    * Create a new BigQuery dataset (e.g., `app_logs_dataset`).\n",
        "    * Create a BigQuery table (e.g., `request_logs`) within this dataset with a schema suitable for storing application I/O and request details. Consider fields like:\n",
        "        * `timestamp` (TIMESTAMP)\n",
        "        * `request_id` (STRING)\n",
        "        * `user_id` (STRING, if applicable)\n",
        "        * `endpoint` (STRING)\n",
        "        * `http_method` (STRING)\n",
        "        * `request_payload` (JSON or STRING)\n",
        "        * `response_payload` (JSON or STRING)\n",
        "        * `status_code` (INTEGER)\n",
        "        * `latency_ms` (INTEGER)\n",
        "        * `log_level` (STRING)\n",
        "        * `message` (STRING)\n",
        "    * Provide `bq mk` command for the dataset and `bq mk --table` command with the schema definition for the table.\n",
        "\n",
        "3.  **Simple Web Application:**\n",
        "    * Create a simple web application (e.g., Python Flask/FastAPI, Node.js Express).\n",
        "    * It should have at least one API endpoint (e.g., `/process` or `/calculate`).\n",
        "    * This endpoint should:\n",
        "        * Accept a JSON `POST` request with some input data.\n",
        "        * Perform a very simple operation (e.g., sum two numbers, concatenate strings, or a mock calculation).\n",
        "        * Return a JSON response.\n",
        "    * Include a `Dockerfile` for your application.\n",
        "    * Provide the source code for your application and its `Dockerfile`.\n",
        "    * Implement basic `print()` or `logging.info()` statements for requests and responses for now; you'll enhance these in Part 2."
      ],
      "metadata": {
        "id": "2-gMSWaKJH7R"
      },
      "id": "2-gMSWaKJH7R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_oJaCFHJH7S"
      },
      "outputs": [],
      "source": [
        "# Your GCP CLI commands for API enablement.\n",
        "        # `bq mk` and `bq mk --table` commands with your chosen schema.\n",
        "        # Source code for your simple web application and Dockerfile."
      ],
      "id": "R_oJaCFHJH7S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Structured Logging and Deployment to Cloud Run (35 Marks)\n",
        "\n",
        "1.  **Structured Logging in Application:**\n",
        "    * Modify your web application to emit **structured logs** (JSON format) to standard output (stdout/stderr).\n",
        "    * For each incoming request to your API endpoint, log:\n",
        "        * The full request payload.\n",
        "        * The generated response payload.\n",
        "        * `request_id` (generate a unique ID for each request).\n",
        "        * `timestamp`.\n",
        "        * `endpoint`, `http_method`.\n",
        "        * `status_code`, `latency_ms`.\n",
        "        * A descriptive `message` (e.g., \"Request processed successfully\").\n",
        "        * (Optional but recommended) `user_id` if you can mock one.\n",
        "    * Use standard logging libraries (e.g., Python's `logging` with `json_formatter`, or Node.js's `winston` with JSON output).\n",
        "    * Provide the updated source code for your application, highlighting the structured logging implementation.\n",
        "\n",
        "2.  **Containerization and Deployment:**\n",
        "    * Build your Docker image using Cloud Build (or locally and push to Artifact Registry).\n",
        "    * Push the image to Artifact Registry (e.g., `gcr.io/[PROJECT-ID]/my-app-logger:latest`).\n",
        "    * Deploy your application to Cloud Run, allowing unauthenticated invocations.\n",
        "    * Provide `gcloud builds submit` and `gcloud run deploy` commands.\n",
        "\n",
        "3.  **Test Deployed Application and Cloud Logging:**\n",
        "    * Access your Cloud Run service URL and send several requests to your API endpoint (e.g., using `curl`, Postman, or a simple Python script).\n",
        "    * Navigate to Cloud Logging in the GCP Console.\n",
        "    * Filter logs by your Cloud Run service and verify that:\n",
        "        * Your structured JSON logs are correctly parsed and displayed in the Log Explorer.\n",
        "        * Fields like `request_payload`, `response_payload`, `status_code`, etc., are appearing as individual fields in the structured log entries.\n",
        "    * Take a screenshot of the Cloud Logging interface showing your parsed structured logs.\n",
        "    * Discuss how structured logging improves log readability and queryability."
      ],
      "metadata": {
        "id": "jRPo8XTJJH7T"
      },
      "id": "jRPo8XTJJH7T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXKgomuAJH7U"
      },
      "outputs": [],
      "source": [
        "# Updated source code of your web application with structured logging.\n",
        "        # `gcloud builds submit` and `gcloud run deploy` commands.\n",
        "        # Screenshot of structured logs in Cloud Logging.\n",
        "        # Discussion on structured logging benefits."
      ],
      "id": "jXKgomuAJH7U"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Log Sink to BigQuery (30 Marks)\n",
        "\n",
        "1.  **Create a Cloud Logging Sink:**\n",
        "    * Create a Cloud Logging sink that routes logs from your Cloud Run service to your BigQuery table.\n",
        "    * The sink should:\n",
        "        * Have a destination of `bigquery.table`.\n",
        "        * Specify the BigQuery table created in Part 1 (e.g., `bigquery.googleapis.com/projects/[PROJECT-ID]/datasets/app_logs_dataset/tables/request_logs`).\n",
        "        * Include a filter to capture only logs from your specific Cloud Run service.\n",
        "        * Crucially, ensure the sink is configured to use the `destination.use_partitioned_tables` property if you want daily partitioned tables, or `destination.schema_options` for auto-detecting schema (though defining it manually is safer).\n",
        "    * Provide the `gcloud logging sinks create` command.\n",
        "\n",
        "2.  **Grant Sink Permissions:**\n",
        "    * After creating the sink, you will receive a service account email for the sink.\n",
        "    * Grant this service account `BigQuery Data Editor` role on your BigQuery dataset (or table).\n",
        "    * Provide the `gcloud projects add-iam-policy-binding` command.\n",
        "\n",
        "3.  **Generate Traffic and Verify in BigQuery:**\n",
        "    * Send a significant number of requests to your deployed Cloud Run application (e.g., 20-50 requests) to generate enough logs.\n",
        "    * Navigate to BigQuery in the GCP Console.\n",
        "    * Query your `request_logs` table.\n",
        "    * Verify that the structured log entries from your application are appearing as rows in your BigQuery table, with fields correctly mapped to columns.\n",
        "    * Run a few analytical queries (e.g., count requests by endpoint, calculate average latency, find requests with errors).\n",
        "    * Provide a screenshot of your BigQuery table showing the ingested logs and the results of at least one analytical query.\n",
        "    * Discuss how the BigQuery integration enables powerful log analytics."
      ],
      "metadata": {
        "id": "REa-hVB3JH7U"
      },
      "id": "REa-hVB3JH7U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HFRgzRUJH7V"
      },
      "outputs": [],
      "source": [
        "# `gcloud logging sinks create` command with the filter.\n",
        "        # `gcloud projects add-iam-policy-binding` command for the sink service account.\n",
        "        # Screenshot of logs in BigQuery table and results of analytical query.\n",
        "        # Discussion on BigQuery analytics benefits."
      ],
      "id": "3HFRgzRUJH7V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Reflection and Clean-up (10 Marks)\n",
        "\n",
        "1.  **Benefits of Centralized Logging:**\n",
        "    * Summarize the benefits of streaming application I/O logs to a centralized data warehouse like BigQuery.\n",
        "    * How does this approach enhance debugging, auditing, and business intelligence compared to just viewing logs in Cloud Logging?\n",
        "\n",
        "2.  **Considerations for Production:**\n",
        "    * What additional considerations would be important for implementing this logging solution in a production environment (e.g., cost optimization, data retention, PII handling, log sampling, error handling in application)?\n",
        "\n",
        "3.  **Clean Up Resources:**\n",
        "    * After completing the assignment, delete all created GCP resources to avoid incurring unnecessary costs:\n",
        "        * Cloud Run service.\n",
        "        * BigQuery dataset (which deletes the table).\n",
        "        * Cloud Logging sink.\n",
        "        * Artifact Registry images.\n",
        "    * Provide the relevant `gcloud` and `bq` commands for thorough cleanup."
      ],
      "metadata": {
        "id": "PqeBRt2SJH7V"
      },
      "id": "PqeBRt2SJH7V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Submit this Jupyter Notebook (.ipynb file) with all cells executed and outputs visible.\n",
        "* Provide the full source code for your web application and its `Dockerfile` in a compressed archive (e.g., `.zip` file) or a link to a Git repository.\n",
        "* Clearly present all requested commands, schemas, URLs, and screenshots.\n",
        "* Ensure your application can be deployed and verified, and logs can be streamed to BigQuery as demonstrated."
      ],
      "metadata": {
        "id": "VQxsJeXWJH7W"
      },
      "id": "VQxsJeXWJH7W"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}