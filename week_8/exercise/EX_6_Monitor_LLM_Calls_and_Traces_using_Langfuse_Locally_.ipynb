{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Monitor LLM Calls and Traces using Langfuse Locally\n",
        "\n",
        "## Objective\n",
        "This assignment focuses on integrating Langfuse into a Python application to monitor LLM calls, traces, and evaluations locally. You will learn to set up Langfuse, instrument your code to capture LLM interactions, and visualize the traces in the Langfuse UI. This is crucial for debugging, optimizing, and understanding the behavior of complex LLM applications."
      ],
      "metadata": {
        "id": "ug-0Q-2p5jW0"
      },
      "id": "ug-0Q-2p5jW0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Environment Setup and Langfuse Initialization (30 Marks)\n",
        "\n",
        "1.  **Environment Setup:**\n",
        "    * Create a new Python virtual environment.\n",
        "    * Install necessary libraries: `langfuse`, `openai` (or `anthropic`, `google-generativeai`, etc., depending on your LLM choice), `python-dotenv`.\n",
        "        * Provide a `requirements.txt` file.\n",
        "\n",
        "2.  **Langfuse Local Setup:**\n",
        "    * Install and run the Langfuse server locally using Docker (recommended).\n",
        "        * Provide the Docker commands needed to get the Langfuse server running.\n",
        "        * Confirm that you can access the Langfuse UI in your browser (usually `http://localhost:3000`). Take a screenshot of the empty Langfuse UI dashboard.\n",
        "\n",
        "3.  **Langfuse SDK Initialization:**\n",
        "    * Create a `.env` file in your project root and add the necessary Langfuse environment variables (`LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY`, `LANGFUSE_HOST`). You will obtain these from your local Langfuse UI.\n",
        "    * In your Python script (e.g., `main.py` or within this notebook), initialize the Langfuse client using these environment variables."
      ],
      "metadata": {
        "id": "Hu8Otqwf5jW1"
      },
      "id": "Hu8Otqwf5jW1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCIqFfnc5jW2"
      },
      "outputs": [],
      "source": [
        "# Your commands for Langfuse local setup (Docker).\n",
        "# Screenshot of the empty Langfuse UI dashboard.\n",
        "# Your Python code for .env file setup and Langfuse SDK initialization."
      ],
      "id": "FCIqFfnc5jW2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Instrumenting LLM Calls (40 Marks)\n",
        "\n",
        "1.  **LLM Integration:**\n",
        "    * Choose an LLM provider (e.g., OpenAI, Anthropic, Google Generative AI).\n",
        "    * Obtain an API key for your chosen LLM and add it to your `.env` file.\n",
        "    * Write a simple Python function that makes a call to your chosen LLM. This function should take a `prompt` as input and return the LLM's response.\n",
        "\n",
        "2.  **Tracing with Langfuse:**\n",
        "    * Instrument your LLM call function using Langfuse decorators or context managers to create a trace.\n",
        "        * Use `@langfuse_client.trace()` for a simple trace.\n",
        "        * Inside the trace, use `@langfuse_client.span()` to wrap the actual LLM API call. This creates a 'span' within the trace, allowing you to see the LLM call details.\n",
        "        * Alternatively, use `langfuse_client.chat()` or `langfuse_client.generation()` directly if your LLM integration is compatible.\n",
        "\n",
        "3.  **Running and Observing Traces:**\n",
        "    * Call your instrumented LLM function at least 3-5 times with different prompts.\n",
        "    * After running your script, navigate to your local Langfuse UI.\n",
        "    * Take screenshots of:\n",
        "        * The \"Traces\" overview page showing your recent traces.\n",
        "        * A detailed view of one of your traces, clearly showing the spans (especially the LLM call).\n",
        "        * The inputs and outputs of the LLM call within a span."
      ],
      "metadata": {
        "id": "ZvY8bIGh5jW2"
      },
      "id": "ZvY8bIGh5jW2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9bV6Il55jW3"
      },
      "outputs": [],
      "source": [
        "# Your Python code for LLM integration and Langfuse instrumentation (trace and span).\n",
        "# Code calls to the instrumented function with various prompts.\n",
        "# Screenshots of Langfuse UI showing traces, detailed trace view, and LLM call details."
      ],
      "id": "w9bV6Il55jW3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Adding Spans and Evaluations (30 Marks)\n",
        "\n",
        "1.  **Adding Custom Spans:**\n",
        "    * Modify your existing code or create a new function that represents a multi-step process (e.g., a simple RAG-like process: retrieve documents, then generate response).\n",
        "    * Instrument each logical step within a trace as a separate `span` (e.g., `retrieval_span`, `generation_span`).\n",
        "    * Ensure the `generation_span` encapsulates the actual LLM call.\n",
        "    * Run this multi-step process a few times.\n",
        "    * Take a screenshot of a detailed trace in the Langfuse UI that clearly shows multiple custom spans and the LLM call within one of them.\n",
        "\n",
        "2.  **Adding Evaluations (Optional, for extra credit):**\n",
        "    * Programmatically add a `score` to one of your traces or spans based on some criteria (e.g., a simple length check on the response, or a hardcoded 'good'/'bad' score).\n",
        "    * Go to the Langfuse UI and observe how the score is displayed with the trace.\n",
        "    * Take a screenshot showing a trace with a score attached.\n",
        "\n",
        "3.  **Reflection:**\n",
        "    * Discuss the benefits of using tracing tools like Langfuse for LLM application development and debugging.\n",
        "    * How does Langfuse help in understanding the flow and performance of your LLM-powered applications?\n",
        "    * What kind of insights can you gain from the data captured by Langfuse?"
      ],
      "metadata": {
        "id": "F3SJnRsZ5jW3"
      },
      "id": "F3SJnRsZ5jW3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0UX26QI5jW3"
      },
      "outputs": [],
      "source": [
        "# Your Python code for adding custom spans and (optional) evaluations.\n",
        "# Screenshots of Langfuse UI showing traces with multiple spans and (optional) scores.\n",
        "# Your written reflection."
      ],
      "id": "X0UX26QI5jW3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Submit this Jupyter Notebook (.ipynb file) with all cells executed and outputs visible.\n",
        "* Ensure your code is well-commented and easy to understand.\n",
        "* Provide a `requirements.txt` file listing all dependencies.\n",
        "* Include all requested screenshots directly in the notebook or as clearly referenced image files.\n",
        "* Make sure your notebook can be run and your Langfuse server can be started locally to reproduce the results."
      ],
      "metadata": {
        "id": "KfzHwxnE5jW4"
      },
      "id": "KfzHwxnE5jW4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}