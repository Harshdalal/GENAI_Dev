{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Implement Grounding Score Checker and Display with Logs\n",
        "## Objective\n",
        "This assignment focuses on implementing a **grounding score checker** for an LLM-powered application. You will learn to evaluate how well an LLM's generated response is supported by the provided context (grounding). This is a crucial step in ensuring the factual accuracy and trustworthiness of RAG systems. You'll also integrate logging to visualize these scores and associated information."
      ],
      "metadata": {
        "id": "kgP-4T2-6esR"
      },
      "id": "kgP-4T2-6esR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Basic RAG Pipeline (30 Marks)\n",
        "\n",
        "1.  **Environment Setup:**\n",
        "    * Create a new Python virtual environment.\n",
        "    * Install necessary libraries: `transformers`, `torch`/`tensorflow`, `sentence-transformers`, `scikit-learn`, `numpy`, `openai` (or your preferred LLM client library), `logging` (built-in).\n",
        "    * Provide a `requirements.txt` file.\n",
        "\n",
        "2.  **Document Corpus:**\n",
        "    * Create a small, focused document corpus (e.g., 5-10 paragraphs about a specific topic, like a company's history, a product's features, or a specific scientific concept).\n",
        "    * Ensure some sentences within the corpus are distinct and can be used to test grounding.\n",
        "\n",
        "3.  **Basic RAG Pipeline Implementation:**\n",
        "    * Implement a simplified RAG pipeline as functions:\n",
        "        * `generate_embeddings(text_list)`: Takes a list of strings and returns their embeddings using a `sentence-transformers` model (e.g., `all-MiniLM-L6-v2`).\n",
        "        * `retrieve_documents(query, document_embeddings, documents, k=3)`: Takes a query, document embeddings, the original documents, and returns the top `k` most similar document chunks (or full documents if chunks are not used).\n",
        "        * `generate_response(query, context)`: Takes a user query and the retrieved context, and generates a response using a small LLM (e.g., `distilgpt2` from Hugging Face, or make a call to a larger model like OpenAI's `gpt-3.5-turbo` if you have an API key).\n",
        "    * Demonstrate that your RAG pipeline can generate a response for a sample query, using the retrieved context."
      ],
      "metadata": {
        "id": "bYAnxJJt6esS"
      },
      "id": "bYAnxJJt6esS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RShhA54z6esS"
      },
      "outputs": [],
      "source": [
        "# Your code for environment setup, document corpus, and basic RAG pipeline here.\n",
        "# Include a demonstration of the RAG pipeline with a sample query."
      ],
      "id": "RShhA54z6esS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Grounding Score Implementation (40 Marks)\n",
        "\n",
        "1.  **Sentence Segmentation:**\n",
        "    * Implement a function `split_into_sentences(text)` that takes a string and splits it into individual sentences. You can use `nltk.sent_tokenize` (remember to download `punkt` tokenizer) or a rule-based approach.\n",
        "\n",
        "2.  **Semantic Similarity for Grounding:**\n",
        "    * Implement a function `calculate_grounding_score(response_sentence, context_sentences, embedding_model, threshold=0.7)`:\n",
        "        * This function should take an individual sentence from the LLM's response, a list of sentences from the retrieved context, the embedding model, and a similarity threshold.\n",
        "        * It should generate embeddings for the `response_sentence` and all `context_sentences`.\n",
        "        * Calculate the cosine similarity between the `response_sentence` embedding and *each* `context_sentence` embedding.\n",
        "        * If the maximum similarity between the `response_sentence` and any `context_sentence` exceeds the `threshold`, consider that response sentence \"grounded\" and return `True` (or the max similarity score).\n",
        "        * Otherwise, return `False` (or 0).\n",
        "\n",
        "3.  **Overall Grounding Score:**\n",
        "    * Create a function `get_overall_grounding(llm_response, retrieved_context, embedding_model, threshold=0.7)`:\n",
        "        * This function will orchestrate the grounding check.\n",
        "        * It should split the `llm_response` into individual sentences.\n",
        "        * For each sentence in the `llm_response`, call `calculate_grounding_score` against the `retrieved_context`.\n",
        "        * Calculate an overall grounding score. This could be:\n",
        "            * **Option A (Recommended):** The percentage of sentences in the `llm_response` that are grounded.\n",
        "            * **Option B:** The average of the maximum similarity scores for each response sentence.\n",
        "        * Return the overall grounding score and, optionally, a list of (response_sentence, is_grounded) tuples for detailed logging.\n",
        "\n",
        "4.  **Test Grounding Score:**\n",
        "    * Call your `get_overall_grounding` function with your RAG pipeline's output (LLM response and retrieved context) for at least **3 different queries**.\n",
        "    * Choose one query where you expect good grounding and one where you might expect poor grounding (e.g., by asking something outside the provided context, making the LLM hallucinate).\n",
        "    * Print the overall grounding score for each test case.\n",
        "    * Justify your chosen `threshold` value and how it impacts the score."
      ],
      "metadata": {
        "id": "AKLDRBL_6esT"
      },
      "id": "AKLDRBL_6esT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2RoY2B56esT"
      },
      "outputs": [],
      "source": [
        "# Your code for sentence segmentation, grounding score functions, and overall grounding calculation.\n",
        "# Include tests with different queries and discussions of the results and threshold choice."
      ],
      "id": "d2RoY2B56esT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Logging and Display (30 Marks)\n",
        "\n",
        "1.  **Configure Logging:**\n",
        "    * Configure Python's `logging` module to output messages to the console.\n",
        "    * Set up different logging levels (e.g., `INFO`, `DEBUG`).\n",
        "    * Include timestamps in your log format.\n",
        "\n",
        "2.  **Integrate Logging into RAG and Grounding:**\n",
        "    * Modify your RAG pipeline and grounding functions to emit log messages at key stages:\n",
        "        * When a query is received.\n",
        "        * The retrieved documents/context (e.g., `logging.debug` for the full text).\n",
        "        * The LLM-generated response (`logging.info`).\n",
        "        * For each response sentence, log whether it was grounded and its max similarity score (`logging.debug`).\n",
        "        * The overall grounding score (`logging.info`).\n",
        "        * Any warnings or errors (e.g., if LLM fails, or no context is found).\n",
        "\n",
        "3.  **Demonstrate Logging:**\n",
        "    * Run your complete RAG pipeline with grounding score calculation for at least **3 sample queries**.\n",
        "    * Ensure the log output clearly shows:\n",
        "        * The query.\n",
        "        * The LLM response.\n",
        "        * The overall grounding score.\n",
        "        * (If DEBUG level is enabled) details for each response sentence grounding and the retrieved context.\n",
        "    * Copy and paste the log output for one detailed example into your notebook.\n",
        "\n",
        "4.  **Analysis:**\n",
        "    * Based on your log outputs and scores, discuss instances where the LLM might have \"hallucinated\" (generated ungrounded information) or where the grounding was strong.\n",
        "    * Explain the importance of logging for observability in LLM applications, especially for debugging and validating grounding."
      ],
      "metadata": {
        "id": "IgnOrEta6esT"
      },
      "id": "IgnOrEta6esT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcR1M6rg6esU"
      },
      "outputs": [],
      "source": [
        "# Your code for logging configuration and integration.\n",
        "# Run the pipeline with logging.\n",
        "# Copy and paste a sample log output.\n",
        "# Your analysis of grounding and logging importance."
      ],
      "id": "FcR1M6rg6esU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Reflection and Future Work (Bonus - 5 Marks)\n",
        "\n",
        "1.  **Limitations of Semantic Similarity for Grounding:**\n",
        "    * What are the limitations of using only semantic similarity to check grounding? (e.g., does high similarity always mean factual correctness?)\n",
        "    * Suggest other methods or metrics that could enhance grounding evaluation (e.g., fact-checking, entailment models).\n",
        "\n",
        "2.  **Integrating with Tracing Tools:**\n",
        "    * If you've used tools like Langfuse or MLflow in previous assignments, how would you integrate this grounding score checker with such tracing/observability platforms?\n",
        "\n",
        "3.  **Practical Applications:**\n",
        "    * Where else can this grounding score checker be useful in real-world scenarios beyond RAG (e.g., content moderation, data validation)?"
      ],
      "metadata": {
        "id": "yC8cLVIz6esU"
      },
      "id": "yC8cLVIz6esU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Submit this Jupyter Notebook (.ipynb file) with all cells executed and outputs visible.\n",
        "* Ensure your code is well-commented and easy to understand.\n",
        "* Provide a `requirements.txt` file listing all dependencies.\n",
        "* Clearly present all requested code, outputs, and discussions.\n",
        "* Make sure your notebook runs without errors in the specified environment."
      ],
      "metadata": {
        "id": "_dBSlyKC6esU"
      },
      "id": "_dBSlyKC6esU"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}