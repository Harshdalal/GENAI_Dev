{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG-powered Chatbot Assignment: Document Indexing and Retrieval\n",
        "\n",
        "\n",
        "## Objective\n",
        "This assignment aims to provide hands-on experience in building a Retrieval Augmented Generation (RAG) powered chatbot. You will focus on the core components of RAG: document indexing and efficient retrieval to answer user queries based on a given corpus of documents."
      ],
      "metadata": {
        "id": "yB_TgeOK2IT7"
      },
      "id": "yB_TgeOK2IT7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Environment (10 Marks)\n",
        "\n",
        "1.  **Environment Setup:**\n",
        "    * Create a new Python virtual environment for this project.\n",
        "    * Install all necessary libraries. (e.g., `transformers`, `torch`/`tensorflow`, `faiss-cpu`/`annoy`/`chromadb`, `nltk`/`spacy`, `langchain`/`llamaindex` - choose one, `scikit-learn`, `unstructured` or `pypdf` for document parsing).\n",
        "    * Provide a `requirements.txt` file listing all dependencies.\n",
        "\n",
        "2.  **Dataset Acquisition:**\n",
        "    * Choose a suitable document corpus. This could be:\n",
        "        * A collection of Wikipedia articles on a specific topic (e.g., \"Artificial Intelligence\", \"Climate Change\").\n",
        "        * A set of research papers (e.g., from arXiv).\n",
        "        * A collection of documentation files for a software project.\n",
        "        * **Minimum Requirement:** The corpus should contain at least **50 distinct documents** and a total of at least **10,000 words**.\n",
        "    * Describe your chosen dataset, its source, and its characteristics."
      ],
      "metadata": {
        "id": "JCah2_mB2IT9"
      },
      "id": "JCah2_mB2IT9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jDUajdb2IT-"
      },
      "outputs": [],
      "source": [
        "# Your code for environment setup description and dataset acquisition details here."
      ],
      "id": "4jDUajdb2IT-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Document Preprocessing and Chunking (20 Marks)\n",
        "\n",
        "1.  **Document Loading and Parsing:**\n",
        "    * Implement a robust method to load documents from your chosen corpus. Handle various file formats if applicable (e.g., PDF, TXT, Markdown).\n",
        "    * Extract plain text content from each document.\n",
        "\n",
        "2.  **Text Preprocessing:**\n",
        "    * Apply necessary text preprocessing steps (e.g., lowercasing, punctuation removal, stop word removal, stemming/lemmatization - justify your choices).\n",
        "\n",
        "3.  **Document Chunking:**\n",
        "    * Divide the processed documents into smaller, meaningful chunks. Experiment with different chunking strategies:\n",
        "        * **Fixed-size chunks:** e.g., 200 words with an overlap of 50 words.\n",
        "        * **Sentence-based chunks:** Divide based on sentence boundaries.\n",
        "        * **Semantic chunks:** (Optional, for extra credit) Use more advanced methods to ensure chunks retain semantic coherence.\n",
        "    * Justify your chosen chunking strategy and the parameters used. Explain why this strategy is suitable for RAG."
      ],
      "metadata": {
        "id": "bbHdSscI2IT_"
      },
      "id": "bbHdSscI2IT_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtGt9-2P2IUA"
      },
      "outputs": [],
      "source": [
        "# Your code for document loading, parsing, preprocessing, and chunking here.\n",
        "# Include explanations and justifications for your choices."
      ],
      "id": "OtGt9-2P2IUA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Document Indexing and Vector Store (30 Marks)\n",
        "\n",
        "1.  **Text Embedding:**\n",
        "    * Choose and implement a pre-trained language model for generating embeddings for your text chunks. (e.g., Sentence-BERT models like `all-MiniLM-L6-v2`, `mpnet-base-v2`, `RoBERTa`, or `BERT` based models).\n",
        "    * Describe the chosen model and justify why it's suitable for your task.\n",
        "\n",
        "2.  **Vector Store Implementation:**\n",
        "    * Build a vector store (or knowledge base) to efficiently store and retrieve the embeddings.\n",
        "    * You can use libraries like:\n",
        "        * `FAISS` (Facebook AI Similarity Search) for efficient similarity search.\n",
        "        * `Annoy` (Approximate Nearest Neighbors Oh Yeah).\n",
        "        * `ChromaDB` or `Pinecone` (if you are comfortable with cloud-based solutions, but local is preferred for this assignment).\n",
        "    * Index all your document chunks and their corresponding embeddings into the chosen vector store.\n",
        "    * Explain your choice of vector store and how it works.\n",
        "\n",
        "3.  **Persistence (Optional, for extra credit):**\n",
        "    * Implement a way to save and load your pre-built index to avoid re-indexing every time."
      ],
      "metadata": {
        "id": "tu4tF9OJ2IUA"
      },
      "id": "tu4tF9OJ2IUA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRkFY5TG2IUB"
      },
      "outputs": [],
      "source": [
        "# Your code for text embedding and vector store implementation here.\n",
        "# Include explanations and justifications for your choices."
      ],
      "id": "nRkFY5TG2IUB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Retrieval Module (25 Marks)\n",
        "\n",
        "1.  **Query Embedding:**\n",
        "    * Implement a function to convert a user query into an embedding using the *same* embedding model used for document chunks.\n",
        "\n",
        "2.  **Similarity Search:**\n",
        "    * Implement a function to perform a similarity search (e.g., cosine similarity) in your vector store using the query embedding.\n",
        "    * Retrieve the top `k` most relevant document chunks based on their similarity scores. Experiment with different values of `k` (e.g., 3, 5, 10) and justify your chosen `k`.\n",
        "    * Display the retrieved chunks and their similarity scores for a few sample queries.\n",
        "\n",
        "3.  **Retrieval Evaluation (Qualitative):**\n",
        "    * Formulate at least 5 diverse sample queries related to your document corpus.\n",
        "    * For each query, analyze the retrieved chunks. Do they seem relevant? Are there any irrelevant chunks? What are the potential issues?\n",
        "    * Discuss how you might improve the retrieval quality."
      ],
      "metadata": {
        "id": "ytAmAAVz2IUB"
      },
      "id": "ytAmAAVz2IUB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlFQlMy22IUB"
      },
      "outputs": [],
      "source": [
        "# Your code for query embedding, similarity search, and qualitative retrieval evaluation here.\n",
        "# Include your sample queries and analysis."
      ],
      "id": "GlFQlMy22IUB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Chatbot Integration (Basic RAG) (15 Marks)\n",
        "\n",
        "1.  **LLM Integration:**\n",
        "    * Choose a suitable pre-trained Language Model (LLM) for text generation. (e.g., `distilbert-base-uncased`, `gpt2`, or a smaller open-source model you can run locally). For demonstration, you can use a Hugging Face pipeline for text generation.\n",
        "    * **Note:** You are *not* required to fine-tune an LLM. The focus is on RAG.\n",
        "\n",
        "2.  **RAG Pipeline:**\n",
        "    * Build a simple RAG pipeline:\n",
        "        * User provides a query.\n",
        "        * Retrieve relevant document chunks using your retrieval module.\n",
        "        * Construct a prompt for the LLM that includes the user query and the retrieved context.\n",
        "        * Generate a response using the LLM based on the prompt.\n",
        "\n",
        "3.  **Demonstration:**\n",
        "    * Provide at least 3 examples of user queries and the chatbot's generated responses. Demonstrate how the retrieved information is used to answer the questions.\n",
        "    * Discuss the strengths and limitations of your basic RAG chatbot."
      ],
      "metadata": {
        "id": "PuIZib432IUC"
      },
      "id": "PuIZib432IUC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3aIEZpW2IUC"
      },
      "outputs": [],
      "source": [
        "# Your code for basic RAG pipeline implementation and demonstration here.\n",
        "# Include your sample queries and chatbot responses."
      ],
      "id": "y3aIEZpW2IUC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Reflection and Future Work (Bonus - 5 Marks)\n",
        "\n",
        "1.  **Challenges Faced:**\n",
        "    * Describe any challenges you encountered during this assignment and how you overcame them.\n",
        "\n",
        "2.  **Potential Improvements:**\n",
        "    * Suggest ways to improve the RAG pipeline. Consider aspects like:\n",
        "        * Advanced chunking strategies.\n",
        "        * Re-ranking retrieved documents.\n",
        "        * Handling multi-turn conversations.\n",
        "        * Integrating more sophisticated LLMs.\n",
        "        * Quantitative evaluation metrics for retrieval and generation.\n",
        "\n",
        "3.  **Learnings:**\n",
        "    * Summarize your key learnings from building this RAG-powered chatbot."
      ],
      "metadata": {
        "id": "ky_yYbkH2IUD"
      },
      "id": "ky_yYbkH2IUD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Submit this Jupyter Notebook (.ipynb file) with all cells executed and outputs visible.\n",
        "* Ensure your code is well-commented and easy to understand.\n",
        "* Provide a `requirements.txt` file.\n",
        "* Include a brief `README.md` file (optional but recommended) explaining how to run your code and any specific instructions.\n",
        "* Make sure your notebook runs without errors in the specified environment."
      ],
      "metadata": {
        "id": "lIFQhCGt2IUD"
      },
      "id": "lIFQhCGt2IUD"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}