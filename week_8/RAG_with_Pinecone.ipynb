{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step-by-Step RAG with Pinecone (New SDK) + Gemini"
      ],
      "metadata": {
        "id": "sT60Exqw8FqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 0: Install and Import Dependencies"
      ],
      "metadata": {
        "id": "fJNn6tuz8JZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY_EkD-82Lsd",
        "outputId": "ea9b2688-5021-48ac-c76e-493a9594c008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/516.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.7/516.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/240.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pinecone google-generativeai langchain tiktoken\n",
        "!pip install -U langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step 1: Setup API Keys and Import Libraries"
      ],
      "metadata": {
        "id": "IS6CMAYD8M2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "#from pinecone import Pinecone ServerlessSpec  CloudProvider AwsRegion VectorType\n",
        "\n",
        "\n",
        "# Set your keys\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_3ZTayB_5mX6F1R8bG7wLKCghRP42JXDd5mRS1GqNcdLLk1u6FDeNoBAi5qWBtVacwBwiFw\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDR7ItGwxOcbodnqRZXJQzFN_MVrRWxGaw\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "\n",
        "# Create Pinecone index (if it doesn't exist)\n",
        "index_name = \"rag-gemini-demo\"\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,  # Gemini uses 768-dim embeddings\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "\n",
        "index = pc.Index(index_name)\n"
      ],
      "metadata": {
        "id": "1Oqzi7wE2yfo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step 2: Prepare and Chunk Documents"
      ],
      "metadata": {
        "id": "5h_LekGD8V9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "docs = [\n",
        "    \"Gemini is a large language model developed by Google DeepMind.\",\n",
        "    \"It is designed for multimodal reasoning and can process text, images, and more.\",\n",
        "    \"Gemini competes with OpenAI's GPT models in generative AI capabilities.\"\n",
        "]\n",
        "\n",
        "# Chunk documents\n",
        "splitter = CharacterTextSplitter(separator=\". \", chunk_size=100, chunk_overlap=10)\n",
        "doc_chunks = splitter.create_documents(docs)\n",
        "\n",
        "for i, doc in enumerate(doc_chunks):\n",
        "    print(f\"Chunk {i+1}: {doc.page_content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFMSOOkj4VB5",
        "outputId": "d829c17d-96ea-4949-fe37-2f197d4aec68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1: Gemini is a large language model developed by Google DeepMind.\n",
            "Chunk 2: It is designed for multimodal reasoning and can process text, images, and more.\n",
            "Chunk 3: Gemini competes with OpenAI's GPT models in generative AI capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step 3: Embed and Index in Pinecone\n",
        "Use Gemini embeddings via LangChain."
      ],
      "metadata": {
        "id": "pkBVFfE38aHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.embeddings import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from uuid import uuid4\n",
        "\n",
        "# Load Gemini embedding model\n",
        "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "# Convert chunks into embeddings\n",
        "texts = [doc.page_content for doc in doc_chunks]\n",
        "embeddings = embedding_model.embed_documents(texts)\n",
        "\n",
        "# Format for Pinecone\n",
        "pinecone_data = [\n",
        "    {\n",
        "        \"id\": str(uuid4()),\n",
        "        \"values\": emb,\n",
        "        \"metadata\": {\"text\": text}\n",
        "    }\n",
        "    for text, emb in zip(texts, embeddings)\n",
        "]\n",
        "\n",
        "# Upsert to Pinecone\n",
        "index.upsert(vectors=pinecone_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4MRInfq47Vh",
        "outputId": "3b92913e-e832-4480-ecd7-a0b21a1460de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'upserted_count': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step 4: Query and Retrieve Relevant Chunks"
      ],
      "metadata": {
        "id": "HKG8POwm80Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"What is Gemini and who developed it?\"\n",
        "\n",
        "# Convert query to vector\n",
        "query_vector = embedding_model.embed_query(query)\n",
        "\n",
        "# Query Pinecone for top relevant chunks\n",
        "results = index.query(vector=query_vector, top_k=3, include_metadata=True)\n",
        "\n",
        "# Extract relevant texts\n",
        "retrieved_docs = [match.metadata[\"text\"] for match in results.matches]\n",
        "\n",
        "print(\"Top Retrieved Chunks:\")\n",
        "for doc in retrieved_docs:\n",
        "    print(\"-\", doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt5l9YA-7DUW",
        "outputId": "a138a5f0-2714-4614-a06b-51d0480a2ec0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Retrieved Chunks:\n",
            "- Gemini is a large language model developed by Google DeepMind.\n",
            "- Gemini competes with OpenAI's GPT models in generative AI capabilities.\n",
            "- It is designed for multimodal reasoning and can process text, images, and more.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step 5: Generate Answer Using Gemini"
      ],
      "metadata": {
        "id": "VPOwvfzV9Nb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine retrieved chunks\n",
        "context = \"\\n\".join(retrieved_docs)\n",
        "\n",
        "# Create prompt for Gemini\n",
        "prompt = f\"\"\"\n",
        "You are an AI assistant. Based on the following context, answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "# Initialize Gemini Pro model\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "\n",
        "# Generate answer\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "print(\"Gemini Answer:\\n\", response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "IwZksX7L7h6S",
        "outputId": "434a57e1-a062-404f-af9f-3355ebff72ce"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini Answer:\n",
            " Gemini is a large language model developed by Google DeepMind.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Summary\n",
        "\n",
        "| Component | Tool                      |\n",
        "| --------- | ------------------------- |\n",
        "| Chunking  | LangChain                 |\n",
        "| Embedding | Gemini (`embedding-001`)  |\n",
        "| Vector DB | Pinecone (new SDK)        |\n",
        "| Generator | Gemini Pro (`gemini-pro`) |\n"
      ],
      "metadata": {
        "id": "mBTk2Lqk9lYP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Bh52pJN7iMl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}