{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfY6b6FGjFbS"
      },
      "source": [
        "# Python Assignment: K-Means Clustering and Elbow Method Implementation\n",
        "\n",
        "This assignment will challenge your understanding of unsupervised learning by requiring you to implement the K-Means clustering algorithm and the Elbow Method from scratch (or primarily using `numpy` for numerical operations). You will generate synthetic datasets, apply your implementation, and visualize the results. This will solidify your grasp of the algorithm's mechanics and the heuristics for determining optimal cluster counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R05x68AvjFbT"
      },
      "source": [
        "## Part 1: Data Generation (15 points)\n",
        "\n",
        "We'll start by generating synthetic 2D datasets that will be used for clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuL0QrsJjFbU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.random.seed(42) # for reproducibility\n",
        "\n",
        "# 1.1 Generate Dataset 1: Distinct Blobs\n",
        "#    Generate a dataset `X1` with 3 well-separated clusters.\n",
        "#    - `n_samples`: 500\n",
        "#    - `n_features`: 2\n",
        "#    - `centers`: 3\n",
        "#    - `cluster_std`: 0.8\n",
        "#    Visualize `X1` using a scatter plot. Ensure axes are labeled.\n",
        "\n",
        "X1, y1 = make_blobs(\n",
        "    # Your parameters here\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Your visualization code here\n",
        "plt.title(\"Dataset 1: Distinct Blobs\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 1.2 Generate Dataset 2: Overlapping Blobs\n",
        "#    Generate a dataset `X2` with 4 clusters that are somewhat overlapping.\n",
        "#    - `n_samples`: 700\n",
        "#    - `n_features`: 2\n",
        "#    - `centers`: 4\n",
        "#    - `cluster_std`: 1.5 (increase this to create overlap)\n",
        "#    Visualize `X2` using a scatter plot.\n",
        "\n",
        "X2, y2 = make_blobs(\n",
        "    # Your parameters here\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Your visualization code here\n",
        "plt.title(\"Dataset 2: Overlapping Blobs\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 1.3 Preprocessing: Standardization\n",
        "#    Standardize both datasets `X1` and `X2` using `StandardScaler` from `sklearn.preprocessing`.\n",
        "#    Store the standardized data in `X1_scaled` and `X2_scaled`.\n",
        "#    Explain why standardization is important for K-Means clustering.\n",
        "\n",
        "# Your standardization code here\n",
        "\n",
        "print(\"X1_scaled shape:\", X1_scaled.shape)\n",
        "print(\"X2_scaled shape:\", X2_scaled.shape)\n",
        "\n",
        "### Explanation: Why Standardization for K-Means?\n",
        "*(Write your explanation here)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBhMInBAjFbV"
      },
      "source": [
        "## Part 2: K-Means Clustering Implementation (40 points)\n",
        "\n",
        "Implement the K-Means algorithm from scratch. You are allowed to use `numpy` for numerical operations (e.g., array manipulations, distance calculations, mean). Avoid using `sklearn.cluster.KMeans` for the core algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt5ObSHTjFbV"
      },
      "outputs": [],
      "source": [
        "class KMeans:\n",
        "    def __init__(self, n_clusters: int, max_iter: int = 300, tol: float = 1e-4):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.centroids = None\n",
        "        self.labels = None\n",
        "        self.inertia_ = None # Sum of squared distances of samples to their closest cluster center\n",
        "\n",
        "    def _initialize_centroids(self, X: np.ndarray) -> np.ndarray:\n",
        "        # TODO: Implement K-Means++ initialization.\n",
        "        # K-Means++ is a smarter way to initialize centroids to speed up convergence\n",
        "        # and avoid poor local optima.\n",
        "        # Steps:\n",
        "        # 1. Choose one center uniformly at random from the data points.\n",
        "        # 2. For each data point x, compute D(x), the distance between x and the nearest center that has already been chosen.\n",
        "        # 3. Choose one new data point as a new center, with probability proportional to D(x)^2.\n",
        "        # 4. Repeat steps 2 and 3 until k centers have been chosen.\n",
        "\n",
        "        # If K-Means++ is too challenging, fall back to random initialization,\n",
        "        # but clearly state that it's random initialization.\n",
        "\n",
        "        # Random initialization (fallback):\n",
        "        # indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)\n",
        "        # return X[indices]\n",
        "\n",
        "        # K-Means++ implementation (highly recommended):\n",
        "        n_samples, n_features = X.shape\n",
        "        centroids = np.zeros((self.n_clusters, n_features))\n",
        "\n",
        "        # 1. Choose first centroid uniformly at random\n",
        "        first_idx = np.random.randint(0, n_samples)\n",
        "        centroids[0] = X[first_idx]\n",
        "\n",
        "        # 2. Iteratively choose other centroids\n",
        "        for i in range(1, self.n_clusters):\n",
        "            # Compute distances from each point to the nearest existing centroid\n",
        "            distances = np.min(self._euclidean_distance(X, centroids[:i]), axis=1)\n",
        "            # Probabilities proportional to D(x)^2\n",
        "            probabilities = distances**2 / np.sum(distances**2)\n",
        "            # Choose new centroid based on probabilities\n",
        "            new_centroid_idx = np.random.choice(n_samples, p=probabilities)\n",
        "            centroids[i] = X[new_centroid_idx]\n",
        "        return centroids\n",
        "\n",
        "\n",
        "    def _euclidean_distance(self, X: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
        "        # TODO: Calculate Euclidean distance between each data point in X and each centroid.\n",
        "        # X: (n_samples, n_features)\n",
        "        # centroids: (n_clusters, n_features)\n",
        "        # Returns: (n_samples, n_clusters) matrix of distances\n",
        "\n",
        "        # Hint: Use broadcasting or np.linalg.norm\n",
        "        # Example: np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
        "        return np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
        "\n",
        "    def _assign_clusters(self, X: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
        "        # TODO: Assign each data point to the closest centroid.\n",
        "        # Returns: (n_samples,) array of cluster labels (integers from 0 to n_clusters-1)\n",
        "\n",
        "        distances = self._euclidean_distance(X, centroids)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def _update_centroids(self, X: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
        "        # TODO: Calculate new centroids as the mean of all points assigned to each cluster.\n",
        "        # Handle empty clusters (e.g., re-initialize them randomly or keep their old position).\n",
        "        # For this assignment, if a cluster becomes empty, you can either:\n",
        "        # 1. Keep its old centroid position.\n",
        "        # 2. Re-initialize it randomly from the data points.\n",
        "        # Option 1 is simpler for initial implementation.\n",
        "\n",
        "        new_centroids = np.zeros_like(self.centroids)\n",
        "        for i in range(self.n_clusters):\n",
        "            points_in_cluster = X[labels == i]\n",
        "            if len(points_in_cluster) > 0:\n",
        "                new_centroids[i] = np.mean(points_in_cluster, axis=0)\n",
        "            else:\n",
        "                # Handle empty cluster: Keep old centroid for now.\n",
        "                new_centroids[i] = self.centroids[i]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X: np.ndarray):\n",
        "        if X.ndim != 2:\n",
        "            raise ValueError(\"Input data X must be 2-dimensional (n_samples, n_features).\")\n",
        "\n",
        "        self.centroids = self._initialize_centroids(X)\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            old_centroids = self.centroids.copy()\n",
        "\n",
        "            self.labels = self._assign_clusters(X, self.centroids)\n",
        "            self.centroids = self._update_centroids(X, self.labels)\n",
        "\n",
        "            # Check for convergence\n",
        "            if np.linalg.norm(self.centroids - old_centroids) < self.tol:\n",
        "                print(f\"K-Means converged after {i+1} iterations.\")\n",
        "                break\n",
        "\n",
        "        # Calculate final inertia\n",
        "        distances_to_centroids = self._euclidean_distance(X, self.centroids)\n",
        "        self.inertia_ = np.sum(distances_to_centroids[np.arange(len(X)), self.labels]**2)\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        if self.centroids is None:\n",
        "            raise ValueError(\"Model not fitted yet. Call fit() first.\")\n",
        "        return self._assign_clusters(X, self.centroids)\n",
        "\n",
        "\n",
        "# Test your KMeans implementation with Dataset 1 (X1_scaled)\n",
        "print(\"\\n--- Testing K-Means on X1_scaled ---\")\n",
        "kmeans1 = KMeans(n_clusters=3, max_iter=300, tol=1e-4)\n",
        "kmeans1.fit(X1_scaled)\n",
        "\n",
        "print(\"Final Centroids for X1_scaled:\\n\", kmeans1.centroids)\n",
        "print(\"Final Inertia for X1_scaled:\", kmeans1.inertia_)\n",
        "\n",
        "# Visualize the results for X1_scaled\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X1_scaled[:, 0], X1_scaled[:, 1], c=kmeans1.labels, cmap='viridis', s=50, alpha=0.8)\n",
        "plt.scatter(kmeans1.centroids[:, 0], kmeans1.centroids[:, 1], marker='X', s=200, color='red', label='Centroids')\n",
        "plt.title(\"K-Means Clustering on X1_scaled (K=3)\")\n",
        "plt.xlabel(\"Feature 1 (Scaled)\")\n",
        "plt.ylabel(\"Feature 2 (Scaled)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Test your KMeans implementation with Dataset 2 (X2_scaled)\n",
        "print(\"\\n--- Testing K-Means on X2_scaled ---\")\n",
        "kmeans2 = KMeans(n_clusters=4, max_iter=300, tol=1e-4)\n",
        "kmeans2.fit(X2_scaled)\n",
        "\n",
        "print(\"Final Centroids for X2_scaled:\\n\", kmeans2.centroids)\n",
        "print(\"Final Inertia for X2_scaled:\", kmeans2.inertia_)\n",
        "\n",
        "# Visualize the results for X2_scaled\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X2_scaled[:, 0], X2_scaled[:, 1], c=kmeans2.labels, cmap='viridis', s=50, alpha=0.8)\n",
        "plt.scatter(kmeans2.centroids[:, 0], kmeans2.centroids[:, 1], marker='X', s=200, color='red', label='Centroids')\n",
        "plt.title(\"K-Means Clustering on X2_scaled (K=4)\")\n",
        "plt.xlabel(\"Feature 1 (Scaled)\")\n",
        "plt.ylabel(\"Feature 2 (Scaled)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Zm8kHIjFbW"
      },
      "source": [
        "## Part 3: Elbow Method Implementation (35 points)\n",
        "\n",
        "Implement the Elbow Method to find the optimal number of clusters for a dataset. This method relies on the within-cluster sum of squares (WCSS), which is often referred to as 'inertia' in `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md4-pZRcjFbX"
      },
      "outputs": [],
      "source": [
        "def run_elbow_method(X: np.ndarray, max_k: int) -> list:\n",
        "    # TODO: Implement the Elbow Method.\n",
        "    # Iterate K from 1 to `max_k`.\n",
        "    # For each K, run your `KMeans` implementation.\n",
        "    # Store the `inertia_` (WCSS) value.\n",
        "    # Return a list of inertia values for each K.\n",
        "\n",
        "    inertias = []\n",
        "    for k in range(1, max_k + 1):\n",
        "        print(f\"Running K-Means for K={k}...\")\n",
        "        kmeans = KMeans(n_clusters=k, max_iter=300, tol=1e-4)\n",
        "        kmeans.fit(X)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "    return inertias\n",
        "\n",
        "\n",
        "# 3.1 Apply Elbow Method to Dataset 1 (X1_scaled)\n",
        "#    Run the elbow method for `max_k` up to 10.\n",
        "#    Plot the inertia values against K. Clearly label the plot.\n",
        "#    Based on the plot, suggest the optimal number of clusters for X1_scaled.\n",
        "\n",
        "max_k1 = 10\n",
        "inertias1 = run_elbow_method(X1_scaled, max_k1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, max_k1 + 1), inertias1, marker='o', linestyle='-')\n",
        "plt.title(\"Elbow Method for X1_scaled\")\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Inertia (WCSS)\")\n",
        "plt.xticks(range(1, max_k1 + 1))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "### Optimal K for X1_scaled:\n",
        "*(State your optimal K here and justify your choice based on the plot.)*\n",
        "\n",
        "\n",
        "# 3.2 Apply Elbow Method to Dataset 2 (X2_scaled)\n",
        "#    Run the elbow method for `max_k` up to 10.\n",
        "#    Plot the inertia values against K.\n",
        "#    Based on the plot, suggest the optimal number of clusters for X2_scaled.\n",
        "#    Discuss the challenges of interpreting the elbow curve for overlapping clusters.\n",
        "\n",
        "max_k2 = 10\n",
        "inertias2 = run_elbow_method(X2_scaled, max_k2)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, max_k2 + 1), inertias2, marker='o', linestyle='-')\n",
        "plt.title(\"Elbow Method for X2_scaled\")\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Inertia (WCSS)\")\n",
        "plt.xticks(range(1, max_k2 + 1))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "### Optimal K for X2_scaled:\n",
        "*(State your optimal K here and justify your choice based on the plot.)*\n",
        "\n",
        "### Discussion: Challenges of Elbow Method for Overlapping Clusters:\n",
        "*(Write your discussion here)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_YH9bVqjFbX"
      },
      "source": [
        "## Part 4: Advanced Challenges & Reflection (10 points)\n",
        "\n",
        "Consider the limitations and potential improvements of your K-Means implementation and the Elbow Method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8rxAdvPjFbX"
      },
      "outputs": [],
      "source": [
        "# 4.1 Multiple Runs (Conceptual)\n",
        "#    K-Means is sensitive to initial centroid placement. How would you modify your `KMeans` class\n",
        "#    or the `fit` method to run the algorithm multiple times with different initializations\n",
        "#    and select the best result (e.g., the one with the lowest inertia)?\n",
        "\n",
        "### Your Answer:\n",
        "*(Write your conceptual answer here)*\n",
        "\n",
        "\n",
        "# 4.2 Limitations of K-Means\n",
        "#    List at least three inherent limitations of the K-Means clustering algorithm itself.\n",
        "\n",
        "### Your Answer:\n",
        "1.  *(Limitation 1)*\n",
        "2.  *(Limitation 2)*\n",
        "3.  *(Limitation 3)*\n",
        "\n",
        "\n",
        "# 4.3 Alternative Evaluation Metrics (Conceptual)\n",
        "#    Besides the Elbow Method's inertia, what other metrics or techniques can be used\n",
        "#    to evaluate clustering performance, especially when true labels are not available?\n",
        "#    Name at least two and briefly explain their purpose.\n",
        "\n",
        "### Your Answer:\n",
        "1.  *(Metric 1 and purpose)*\n",
        "2.  *(Metric 2 and purpose)*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}