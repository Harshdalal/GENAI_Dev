{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE5YJHlupxax"
      },
      "source": [
        "# Python Assignment: Model Pipeline Serialization and Deployment with Joblib\n",
        "\n",
        "This assignment focuses on a crucial aspect of MLOps: packaging your entire machine learning workflow, from preprocessing to prediction, into a single, deployable unit. You will build a Scikit-learn pipeline, train it, serialize it using `joblib`, and then demonstrate how to load and use it to make predictions on new, raw data. This ensures consistency and reproducibility in deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcPluzngpxa0"
      },
      "source": [
        "## Part 1: Data Generation and Feature Engineering Setup (30 points)\n",
        "\n",
        "We'll create a synthetic dataset that requires various preprocessing steps, laying the groundwork for building a robust pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mLw1xIvpxa0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import joblib # For serialization\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\") # Suppress warnings for cleaner output\n",
        "np.random.seed(42) # for reproducibility\n",
        "\n",
        "# 1.1 Generate Synthetic Dataset\n",
        "#    Create a DataFrame with:\n",
        "#    - `numerical_feature_1`: Continuous, some missing values (NaN)\n",
        "#    - `numerical_feature_2`: Continuous, no missing values\n",
        "#    - `categorical_feature`: Categorical, 3-4 unique values, some missing values (NaN)\n",
        "#    - `target`: A continuous target variable related to the features.\n",
        "#    Ensure `n_samples` is at least 500.\n",
        "\n",
        "n_samples = 500\n",
        "\n",
        "# Generate numerical features\n",
        "num_f1 = np.random.rand(n_samples) * 100\n",
        "num_f2 = np.random.normal(loc=50, scale=15, size=n_samples)\n",
        "\n",
        "# Introduce missing values in num_f1\n",
        "missing_indices_f1 = np.random.choice(n_samples, size=int(0.05 * n_samples), replace=False)\n",
        "num_f1[missing_indices_f1] = np.nan\n",
        "\n",
        "# Generate categorical feature\n",
        "categories = ['A', 'B', 'C', 'D']\n",
        "cat_f = np.random.choice(categories, size=n_samples, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Introduce missing values in cat_f\n",
        "missing_indices_cat = np.random.choice(n_samples, size=int(0.03 * n_samples), replace=False)\n",
        "cat_f[missing_indices_cat] = np.nan\n",
        "\n",
        "# Generate target variable with some noise\n",
        "target = 2 * num_f1 + 0.5 * num_f2 + (10 if cat_f[0] == 'A' else (5 if cat_f[0] == 'B' else 0)) + np.random.randn(n_samples) * 5\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'numerical_feature_1': num_f1,\n",
        "    'numerical_feature_2': num_f2,\n",
        "    'categorical_feature': cat_f,\n",
        "    'target': target\n",
        "})\n",
        "\n",
        "print(\"Original Data Head:\\n\", data.head())\n",
        "print(\"\\nOriginal Data Info:\")\n",
        "data.info()\n",
        "\n",
        "# Define features and target\n",
        "X = data[['numerical_feature_1', 'numerical_feature_2', 'categorical_feature']]\n",
        "y = data['target']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTrain set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfDsaIdTpxa2"
      },
      "source": [
        "## Part 2: Building and Training the Pipeline (40 points)\n",
        "\n",
        "You will now construct a comprehensive Scikit-learn pipeline that handles all preprocessing steps and integrates the final model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F561-qFopxa2"
      },
      "outputs": [],
      "source": [
        "# 2.1 Define Preprocessing Steps\n",
        "#    - For `numerical_feature_1`: Impute missing values with the mean, then apply `StandardScaler`.\n",
        "#    - For `numerical_feature_2`: Apply `StandardScaler` directly.\n",
        "#    - For `categorical_feature`: Impute missing values with the most frequent value, then apply `OneHotEncoder`.\n",
        "#    Use `SimpleImputer`, `StandardScaler`, and `OneHotEncoder` from `sklearn.preprocessing`.\n",
        "\n",
        "# Define column types\n",
        "numerical_features_with_missing = ['numerical_feature_1']\n",
        "numerical_features_no_missing = ['numerical_feature_2']\n",
        "categorical_features = ['categorical_feature']\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "\n",
        "# TODO: Numerical pipeline for features with missing values\n",
        "numerical_transformer_missing = Pipeline(steps=[\n",
        "    # ('imputer', SimpleImputer(strategy='mean')),\n",
        "    # ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# TODO: Numerical pipeline for features without missing values\n",
        "numerical_transformer_no_missing = Pipeline(steps=[\n",
        "    # ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# TODO: Categorical pipeline\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    # ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    # ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "\n",
        "# 2.2 Create a ColumnTransformer\n",
        "#    Combine the preprocessing steps for different column types using `ColumnTransformer`.\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        # TODO: Add your transformers here\n",
        "        # ('num_missing', numerical_transformer_missing, numerical_features_with_missing),\n",
        "        # ('num_no_missing', numerical_transformer_no_missing, numerical_features_no_missing),\n",
        "        # ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough' # Keep other columns if any (not strictly needed here)\n",
        ")\n",
        "\n",
        "print(\"Preprocessor created:\\n\", preprocessor)\n",
        "\n",
        "\n",
        "# 2.3 Build and Train the Full Pipeline\n",
        "#    Create a full `Pipeline` that first applies the `preprocessor` and then trains a `RandomForestRegressor`.\n",
        "#    Train this entire pipeline on your `X_train` and `y_train` data.\n",
        "#    Evaluate the pipeline's performance on the `X_test` and `y_test` data (RMSE and R2 Score).\n",
        "\n",
        "# TODO: Create the full pipeline\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    # ('preprocessor', preprocessor),\n",
        "    # ('regressor', RandomForestRegressor(n_estimators=100, random_state=42)) # or LinearRegression()\n",
        "])\n",
        "\n",
        "print(\"\\nFull Pipeline created:\\n\", full_pipeline)\n",
        "\n",
        "# Train the pipeline\n",
        "print(\"\\nTraining the full pipeline...\")\n",
        "# TODO: Fit the pipeline\n",
        "# full_pipeline.fit(X_train, y_train)\n",
        "print(\"Pipeline training complete.\")\n",
        "\n",
        "# Evaluate the pipeline\n",
        "print(\"\\nEvaluating the pipeline performance...\")\n",
        "# TODO: Make predictions and calculate metrics\n",
        "pipeline_predictions = # ... predictions\n",
        "pipeline_rmse = # ... RMSE\n",
        "pipeline_r2 = # ... R2 Score\n",
        "\n",
        "print(f\"Pipeline Test RMSE: {pipeline_rmse:.4f}\")\n",
        "print(f\"Pipeline Test R2 Score: {pipeline_r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdyzKVzppxa3"
      },
      "source": [
        "## Part 3: Serialization with `joblib` (15 points)\n",
        "\n",
        "You will now save the entire trained pipeline to disk using `joblib`. This is essential for deploying the exact trained state of your model and its preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs1uJDTPpxa3"
      },
      "outputs": [],
      "source": [
        "# 3.1 Save the Trained Pipeline\n",
        "#    Use `joblib.dump()` to save the `full_pipeline` object to a file named `model_pipeline.joblib`.\n",
        "#    Explain why `joblib` is often preferred over Python's built-in `pickle` for serializing Scikit-learn models and pipelines.\n",
        "\n",
        "pipeline_filename = 'model_pipeline.joblib'\n",
        "\n",
        "print(f\"\\nSaving the pipeline to {pipeline_filename}...\")\n",
        "# TODO: Save the pipeline\n",
        "# joblib.dump(full_pipeline, pipeline_filename)\n",
        "print(\"Pipeline saved successfully.\")\n",
        "\n",
        "### Explanation: Why Joblib over Pickle for Scikit-learn?\n",
        "*(Write your explanation here)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2J3bUxbpxa4"
      },
      "source": [
        "## Part 4: Loading and Deployment Simulation (15 points)\n",
        "\n",
        "Demonstrate how to load the saved pipeline and use it to make predictions on new, raw data, mimicking a production environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_SqtOKgpxa4"
      },
      "outputs": [],
      "source": [
        "# 4.1 Load the Pipeline\n",
        "#    Load the `model_pipeline.joblib` file back into memory using `joblib.load()`.\n",
        "\n",
        "print(f\"\\nLoading the pipeline from {pipeline_filename}...\")\n",
        "# TODO: Load the pipeline\n",
        "# loaded_pipeline = joblib.load(pipeline_filename)\n",
        "print(\"Pipeline loaded successfully.\")\n",
        "\n",
        "# 4.2 Simulate New Raw Data for Prediction\n",
        "#    Create a new `pd.DataFrame` that represents unseen, raw input data (i.e., not yet preprocessed).\n",
        "#    Ensure it has the same column names and data types as your original training data.\n",
        "\n",
        "new_raw_data = pd.DataFrame({\n",
        "    'numerical_feature_1': [85.0, np.nan, 23.5],\n",
        "    'numerical_feature_2': [45.1, 60.5, 38.0],\n",
        "    'categorical_feature': ['A', 'C', np.nan]\n",
        "})\n",
        "print(\"\\nNew Raw Data for Prediction:\\n\", new_raw_data)\n",
        "\n",
        "# 4.3 Make Predictions with the Loaded Pipeline\n",
        "#    Use the `loaded_pipeline` to make predictions on the `new_raw_data`.\n",
        "#    Observe that the pipeline automatically handles all preprocessing steps before prediction.\n",
        "\n",
        "print(\"\\nMaking predictions on new raw data with loaded pipeline...\")\n",
        "# TODO: Make predictions\n",
        "# new_predictions = loaded_pipeline.predict(new_raw_data)\n",
        "# print(f\"Predictions: {new_predictions}\")\n",
        "\n",
        "# 4.4 (Tougher) Create a simple prediction function/endpoint simulation\n",
        "#    Define a function that takes raw input data (e.g., a dictionary or list) and uses the loaded pipeline to return a prediction.\n",
        "#    This simulates what a web API endpoint might do.\n",
        "\n",
        "def predict_from_raw_input(input_dict: dict, pipeline) -> float:\n",
        "    # TODO: Convert input_dict to a DataFrame suitable for the pipeline\n",
        "    input_df = pd.DataFrame([input_dict])\n",
        "    prediction = pipeline.predict(input_df)[0]\n",
        "    return prediction\n",
        "\n",
        "sample_input = {\n",
        "    'numerical_feature_1': 75.2,\n",
        "    'numerical_feature_2': 55.0,\n",
        "    'categorical_feature': 'B'\n",
        "}\n",
        "\n",
        "print(f\"\\nSimulating API prediction for: {sample_input}\")\n",
        "# TODO: Call your prediction function\n",
        "# api_prediction = predict_from_raw_input(sample_input, loaded_pipeline)\n",
        "# print(f\"API Prediction: {api_prediction:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peei8pl4pxa4"
      },
      "source": [
        "## Part 5: Reflection and Best Practices (10 points)\n",
        "\n",
        "Answer the following questions in a markdown cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC4xMIgWpxa5"
      },
      "source": [
        "### Your Answers to Reflection Questions:\n",
        "\n",
        "1.  **What are the primary advantages of saving an entire machine learning pipeline (preprocessing + model) instead of just the trained model?** (List at least 3 advantages)\n",
        "\n",
        "    * **Advantage 1:** _(Your answer here)_\n",
        "    * **Advantage 2:** _(Your answer here)_\n",
        "    * **Advantage 3:** _(Your answer here)_\n",
        "\n",
        "2.  **When deploying a `joblib`-serialized pipeline to a production environment (e.g., a Docker container or cloud function), what key dependencies or considerations must you ensure are present in that environment?**\n",
        "\n",
        "    _(Your answer here)_\n",
        "\n",
        "3.  **Are there any potential issues or limitations with using `joblib` for model serialization, especially in large-scale or long-term production scenarios? (e.g., version compatibility, cross-language support)**\n",
        "\n",
        "    _(Your answer here)_\n",
        "\n",
        "4.  **Briefly compare and contrast `joblib` with other model serialization formats/tools you might know (e.g., `pickle`, ONNX, PMML). When might you choose one over the other?**\n",
        "\n",
        "    _(Your answer here)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9mV9HtOpxa5"
      },
      "source": [
        "## Deliverables:\n",
        "\n",
        "1.  This completed Jupyter Notebook (`joblib_pipeline_deployment_assignment.ipynb`) with all code cells executed and reflection questions answered.\n",
        "2.  The `model_pipeline.joblib` file generated by your code (you can optionally include it in your submission if submitting as a zipped folder)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}