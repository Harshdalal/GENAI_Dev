{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Construct a LangGraph Workflow for Review Classification with Gemini\n",
        "You'll learn how to:\n",
        "\n",
        "Load & preprocess product reviews.\n",
        "\n",
        "Classify reviews via Gemini using LLM nodes.\n",
        "\n",
        "Route reviews into \"Positive\" or \"Negative\" buckets.\n",
        "\n",
        "Automate the workflow with LangGraph."
      ],
      "metadata": {
        "id": "oi0bhg_T7ZwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ Step 1: Install Gemini SDK"
      ],
      "metadata": {
        "id": "eCDg__ph7Ztd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "j-g5nrY557kB"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install if not already done\n",
        "!pip install -q langchain langchain-google-genai google-generativeai langgraph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ Step 2:  Set Up Gemini LLM in LangChain"
      ],
      "metadata": {
        "id": "pD0Tl8eM8G3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Gemini LLM Setup\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "ydpFQ-m07aOv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Gemini API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCOQXtLBKUXIlw4p-jarVeENvtvmnBPLiw\"  # ðŸ” replace with your key\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# Define LLM instance\n",
        "classification_llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.0\n",
        ")\n"
      ],
      "metadata": {
        "id": "qPuaf3KrNDLZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ Step 3: classification node"
      ],
      "metadata": {
        "id": "OhnBJQt08I2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(state: State) -> State:\n",
        "    review = state[\"current_review\"]\n",
        "    prompt = f'Classify this review as \"Positive\" or \"Negative\": \"{review}\"'\n",
        "    response = classification_llm.invoke(prompt)\n",
        "    # response.content holds the text output\n",
        "    label = response.content.strip().split()[0]\n",
        "    state[\"classification\"] = label if label in [\"Positive\", \"Negative\"] else \"Negative\"\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "itlKMUde7g8Q"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 4  Full Workflow"
      ],
      "metadata": {
        "id": "Hxd4KetxRp3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (imports and initial setup) ...\n",
        "\n",
        "# Load reviews\n",
        "def load_reviews(state: State) -> State:\n",
        "    state[\"reviews\"] = [\n",
        "        \"Love this product! Works perfectly.\",\n",
        "        \"Terrible quality. Broke after one day.\",\n",
        "        \"Okay for the price. Not amazing.\",\n",
        "        \"Excellent customer service and fast shipping.\"\n",
        "    ]\n",
        "    state[\"buckets\"] = {\"Positive\": [], \"Negative\": []}\n",
        "    return state\n",
        "\n",
        "# Pick next review\n",
        "def pick_next(state: State) -> State:\n",
        "    if state[\"reviews\"]:\n",
        "        state[\"current_review\"] = state[\"reviews\"].pop(0)\n",
        "    else:\n",
        "        state[\"current_review\"] = \"\"\n",
        "    return state\n",
        "\n",
        "# Classify using Gemini invoke\n",
        "def classify(state: State) -> State:\n",
        "    response = classification_llm.invoke(\n",
        "        f'Classify this review as \"Positive\" or \"Negative\": \"{state[\"current_review\"]}\"'\n",
        "    )\n",
        "    label = response.content.strip().split()[0]\n",
        "    state[\"classification\"] = label if label in [\"Positive\", \"Negative\"] else \"Negative\"\n",
        "    return state\n",
        "\n",
        "# Bucket the review\n",
        "def bucket_review(state: State) -> State:\n",
        "    state[\"buckets\"][state[\"classification\"]].append(state[\"current_review\"])\n",
        "    return state\n",
        "\n",
        "# Build the graph with conditional branching\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "    reviews: list[str]\n",
        "    current_review: str\n",
        "    classification: str\n",
        "    buckets: dict[str, list[str]]\n",
        "\n",
        "graph = StateGraph(State)\n",
        "graph.add_node(\"load\", load_reviews)\n",
        "graph.add_node(\"pick\", pick_next)\n",
        "graph.add_node(\"classify\", classify)\n",
        "graph.add_node(\"bucket\", bucket_review)\n",
        "\n",
        "graph.add_edge(START, \"load\")\n",
        "graph.add_edge(\"load\", \"pick\")\n",
        "\n",
        "def route_after_pick(state: State) -> str:\n",
        "    return \"classify\" if state[\"current_review\"] else END\n",
        "\n",
        "graph.add_conditional_edges(\"pick\", route_after_pick)\n",
        "graph.add_edge(\"classify\", \"bucket\")\n",
        "graph.add_edge(\"bucket\", \"pick\")\n",
        "graph.add_edge(\"pick\", END)\n",
        "\n",
        "# Run the workflow\n",
        "compiled = graph.compile()\n",
        "final_state = compiled.invoke({})\n",
        "\n",
        "print(\"âœ… Results\")\n",
        "print(\"Positive:\", final_state[\"buckets\"][\"Positive\"])\n",
        "print(\"Negative:\", final_state[\"buckets\"][\"Negative\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv3xEnBFPQHW",
        "outputId": "86242064-e524-4c39-9e89-c891e8d581ee"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Results\n",
            "Positive: ['Love this product! Works perfectly.', 'Excellent customer service and fast shipping.']\n",
            "Negative: ['Terrible quality. Broke after one day.', 'Okay for the price. Not amazing.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKR6PCMIPPxx"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}