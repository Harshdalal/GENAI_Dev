{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Practical: Multi-Node LangGraph Workflow with Retry, Fallback & Tracing"
      ],
      "metadata": {
        "id": "QyvdaPf9Z6K1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "9Wy3XuVyZ9L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-google-genai google-generativeai langgraph langchain\n"
      ],
      "metadata": {
        "id": "9WIHwZlIZHSH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step 2: Configure Gemini and Enable Tracing"
      ],
      "metadata": {
        "id": "TCqCnANlZ_kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Set your Gemini API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCOQXtLBKUXIlw4p-jarVeENvtvmnBPLiw\"\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# Support LangSmith tracing; set up your LangSmith API key/environment\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "# Initialize Gemini Flash model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n"
      ],
      "metadata": {
        "id": "0VVb28dkZHme"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step 3: Define State and Workflow Nodes"
      ],
      "metadata": {
        "id": "r8mXysjiaDOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# Define the shared state structure for the workflow\n",
        "class State(TypedDict):\n",
        "    task: str\n",
        "    answer: str\n",
        "    fallback_answer: str\n",
        "    attempt: int\n",
        "\n",
        "# Node 1: Initialize task context\n",
        "def init_node(state: State) -> State:\n",
        "    state[\"attempt\"] = 0\n",
        "    state[\"answer\"] = \"\"\n",
        "    state[\"fallback_answer\"] = \"\"\n",
        "    return state\n",
        "\n",
        "# Node 2: Invoke LLM, with incrementing attempts\n",
        "def llm_node(state: State) -> State:\n",
        "    state[\"attempt\"] += 1\n",
        "    prompt = f\"Answer clearly: {state['task']}\"\n",
        "    try:\n",
        "        resp = llm.invoke(prompt)\n",
        "        state[\"answer\"] = resp.content.strip()\n",
        "    except Exception as e:\n",
        "        # Leave answer blank on failure\n",
        "        state[\"answer\"] = \"\"\n",
        "    return state\n",
        "\n",
        "# Node 3: Provide fallback after retries are exhausted\n",
        "def fallback_node(state: State) -> State:\n",
        "    if not state[\"answer\"] and state[\"attempt\"] >= 2:\n",
        "        state[\"fallback_answer\"] = f\"😔 Sorry, I couldn't answer: {state['task']}\"\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "KMBAGU3BZQYO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step 4: Construct the Graph with Retry and Fallback"
      ],
      "metadata": {
        "id": "qOJCUl-ZaGxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = StateGraph(State)\n",
        "graph.add_node(\"init\", init_node)\n",
        "graph.add_node(\"call_llm\", llm_node)\n",
        "graph.add_node(\"fallback\", fallback_node)\n",
        "\n",
        "graph.add_edge(START, \"init\")\n",
        "graph.add_edge(\"init\", \"call_llm\")\n",
        "\n",
        "# Conditional routing based on answer status and attempt count\n",
        "def route_after_llm(state: State) -> str:\n",
        "    if state[\"answer\"]:\n",
        "        return END\n",
        "    elif state[\"attempt\"] < 2:\n",
        "        return \"call_llm\"\n",
        "    else:\n",
        "        return \"fallback\"\n",
        "\n",
        "graph.add_conditional_edges(\"call_llm\", route_after_llm)\n",
        "graph.add_edge(\"fallback\", END)\n",
        "\n",
        "# Compile the graph for execution\n",
        "compiled_graph = graph.compile()\n"
      ],
      "metadata": {
        "id": "ra1DNIQlZSM9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#▶️ Step 5: Run the Workflow"
      ],
      "metadata": {
        "id": "XsIzpOHAaJlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_state = {\"task\": \"Explain quantum entanglement in simple terms.\"}\n",
        "final_state = compiled_graph.invoke(input_state)\n",
        "\n",
        "print(\"✅ Final Output\")\n",
        "print(\"Answer:\", final_state[\"answer\"])\n",
        "print(\"Fallback:\", final_state[\"fallback_answer\"])\n",
        "print(\"Attempts:\", final_state[\"attempt\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR2wJneCZTzY",
        "outputId": "37e0de7a-d503-41df-b5b7-9914387914db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Final Output\n",
            "Answer: Imagine two coins flipped at the same time, but in a special way where they're magically linked.  If one lands on heads, the other *instantly* lands on tails, no matter how far apart they are.  That's kind of like quantum entanglement.\n",
            "\n",
            "Two entangled particles are linked together in a way that their fates are intertwined.  Measuring a property of one particle (like its spin) instantly tells you the corresponding property of the other particle, even if they're light-years apart.  It's not that the information is traveling faster than light; it's that the particles are fundamentally connected.  Their properties aren't defined until measured, and the measurement on one instantly defines the property of the other.\n",
            "Fallback: \n",
            "Attempts: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Fl5uJ9EZV1e"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}