{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Add Streaming Responses to LangGraph Nodes\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TJ_8jAq1TiJw"
      },
      "id": "TJ_8jAq1TiJw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective:\n",
        "This assignment challenges you to **implement streaming responses within a LangGraph workflow**. Traditionally, LLM responses are received as a single, complete block. However, for a better user experience, especially with longer generations, streaming allows you to display text as it's generated. You will build a simple content generation and summarization workflow, demonstrating how to stream output from the LLM nodes and integrate it into the overall LangGraph execution."
      ],
      "metadata": {
        "id": "I6NaE_zBTiJy"
      },
      "id": "I6NaE_zBTiJy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ilCWy8bETiJy"
      },
      "id": "ilCWy8bETiJy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions:\n",
        "1.  **LLM Access**: You'll need access to an LLM API that supports streaming. For this assignment, we'll primarily use **Google's Gemini Pro model** via the `langchain-google-genai` integration.\n",
        "2.  **Environment Setup**: Install the necessary Python libraries: `pip install langchain-google-genai langgraph langchain_core`.\n",
        "3.  **API Key**: Securely handle your API key. It's best practice to load it from an environment variable.\n",
        "4.  **Jupyter Notebook**: All your code, outputs, observations, and analysis must be documented in this Jupyter Notebook.\n",
        "5.  **Workflow Scenario**: You will create a two-node workflow:\n",
        "    * **Content Generation Node**: Generates a short article.\n",
        "    * **Summarization Node**: Summarizes the generated article.\n",
        "6.  **Streaming Requirement**: Both the `Content Generation Node` and `Summarization Node` must demonstrate streaming behavior when invoking the LLM.\n",
        "7.  **Analysis**: Discuss the implementation of streaming and its benefits."
      ],
      "metadata": {
        "id": "kl7fD9JvTiJz"
      },
      "id": "kl7fD9JvTiJz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "mUUMUeBrTiJz"
      },
      "id": "mUUMUeBrTiJz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Workflow State Definition\n",
        "Configure your LLM and define the state your LangGraph workflow will manage."
      ],
      "metadata": {
        "id": "tJUpRlB8TiJ0"
      },
      "id": "tJUpRlB8TiJ0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1: API Configuration and LLM Initialization\n",
        "Set up your Google Generative AI API key and initialize the `ChatGoogleGenerativeAI` model. Ensure it's configured for streaming."
      ],
      "metadata": {
        "id": "mg7sSC2uTiJ0"
      },
      "id": "mg7sSC2uTiJ0"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import TypedDict, Iterator\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# --- YOUR API KEY HERE ---\n",
        "# It's highly recommended to load your API key from an environment variable for security.\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY_HERE\" # Replace with your actual API key\n",
        "\n",
        "# Initialize the LLM (Gemini Pro) - Streaming is usually enabled by default for supported models\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7, streaming=True)\n",
        "\n",
        "print(\"LLM initialized with Gemini Pro and streaming enabled!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "jNN5d9n5TiJ0"
      },
      "id": "jNN5d9n5TiJ0",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Define Workflow State\n",
        "Define a `TypedDict` to represent the state of your LangGraph workflow. This state will hold the generated content and its summary."
      ],
      "metadata": {
        "id": "9gsvbcuOTiJ1"
      },
      "id": "9gsvbcuOTiJ1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `topic`: The subject for content generation.\n",
        "* `generated_article`: The full generated article text.\n",
        "* `summary`: The summary of the article."
      ],
      "metadata": {
        "id": "zj-xJ483TiJ2"
      },
      "id": "zj-xJ483TiJ2"
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentWorkflowState(TypedDict):\n",
        "    topic: str\n",
        "    generated_article: str\n",
        "    summary: str\n",
        "\n",
        "print(\"ContentWorkflowState defined!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "x0oyukjHTiJ2"
      },
      "id": "x0oyukjHTiJ2",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "w6JaIpslTiJ2"
      },
      "id": "w6JaIpslTiJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Define Nodes with Streaming Implementation\n",
        "Create the nodes, ensuring they utilize the LLM's streaming capabilities and print the streamed chunks as they arrive."
      ],
      "metadata": {
        "id": "1LlQVXDNTiJ3"
      },
      "id": "1LlQVXDNTiJ3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1: `generate_article_node` with Streaming\n",
        "This node will generate a short article based on the `topic`. It **must stream** the content and print each chunk to the console as it's received."
      ],
      "metadata": {
        "id": "IsXgKRgBTiJ3"
      },
      "id": "IsXgKRgBTiJ3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Prompt Design**: Create a prompt for article generation.\n",
        "* **Streaming Logic**: Use `llm.stream()` or chain's `stream()` method. Iterate over the streamed chunks and print them. Accumulate the full content for the state.\n",
        "* **Output**: Update `generated_article` in the state."
      ],
      "metadata": {
        "id": "GT19CpWlTiJ3"
      },
      "id": "GT19CpWlTiJ3"
    },
    {
      "cell_type": "code",
      "source": [
        "generation_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Generate a short, informative article (around 200-300 words) on the given topic.\"),\n",
        "    (\"user\", \"Topic: {topic}\")\n",
        "])\n",
        "\n",
        "generation_chain = generation_prompt | llm\n",
        "\n",
        "def generate_article_node(state: ContentWorkflowState) -> ContentWorkflowState:\n",
        "    print(\"\\n--- Node: Generating Article (Streaming) ---\")\n",
        "    topic = state[\"topic\"]\n",
        "    full_article = \"\"\n",
        "\n",
        "    print(f\"Generating article on '{topic}':\")\n",
        "    for chunk in generation_chain.stream({\"topic\": topic}):\n",
        "        if isinstance(chunk, BaseMessage) and chunk.content:\n",
        "            print(chunk.content, end=\"\", flush=True) # Print each chunk\n",
        "            full_article += chunk.content\n",
        "        elif isinstance(chunk, str):\n",
        "            print(chunk, end=\"\", flush=True)\n",
        "            full_article += chunk\n",
        "\n",
        "    print(\"\\n--- Article Generation Complete ---\")\n",
        "    return {\"generated_article\": full_article}\n",
        "\n",
        "print(\"generate_article_node defined!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "fonn7R2oTiJ3"
      },
      "id": "fonn7R2oTiJ3",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2: `summarize_article_node` with Streaming\n",
        "This node will take the `generated_article` and produce a concise summary. It also **must stream** its output."
      ],
      "metadata": {
        "id": "IXVT8UE_TiJ4"
      },
      "id": "IXVT8UE_TiJ4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Prompt Design**: Create a prompt for summarization.\n",
        "* **Streaming Logic**: Similar to the previous node, iterate and print chunks.\n",
        "* **Output**: Update `summary` in the state."
      ],
      "metadata": {
        "id": "xBAbLBZPTiJ4"
      },
      "id": "xBAbLBZPTiJ4"
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Summarize the following article concisely, focusing on key points and main ideas.\"),\n",
        "    (\"user\", \"Article: {article_text}\")\n",
        "])\n",
        "\n",
        "summarization_chain = summarization_prompt | llm\n",
        "\n",
        "def summarize_article_node(state: ContentWorkflowState) -> ContentWorkflowState:\n",
        "    print(\"\\n--- Node: Summarizing Article (Streaming) ---\")\n",
        "    article_text = state[\"generated_article\"]\n",
        "    full_summary = \"\"\n",
        "\n",
        "    print(\"Generating summary:\")\n",
        "    for chunk in summarization_chain.stream({\"article_text\": article_text}):\n",
        "        if isinstance(chunk, BaseMessage) and chunk.content:\n",
        "            print(chunk.content, end=\"\", flush=True) # Print each chunk\n",
        "            full_summary += chunk.content\n",
        "        elif isinstance(chunk, str):\n",
        "            print(chunk, end=\"\", flush=True)\n",
        "            full_summary += chunk\n",
        "\n",
        "    print(\"\\n--- Summary Generation Complete ---\")\n",
        "    return {\"summary\": full_summary}\n",
        "\n",
        "print(\"summarize_article_node defined!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "ihffU2b3TiJ4"
      },
      "id": "ihffU2b3TiJ4",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "OWKWbVFsTiJ5"
      },
      "id": "OWKWbVFsTiJ5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Construct and Execute the LangGraph Workflow\n",
        "Combine your streaming nodes into a `StateGraph` and execute it to observe the streaming behavior."
      ],
      "metadata": {
        "id": "wJkbpn9MTiJ5"
      },
      "id": "wJkbpn9MTiJ5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.1: Define Edges and Compile the Graph\n",
        "Set up the sequential flow for your content generation and summarization workflow."
      ],
      "metadata": {
        "id": "63AH_mXYTiJ5"
      },
      "id": "63AH_mXYTiJ5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Graph Flow**:\n",
        "    1.  Start at `generate_article_node`.\n",
        "    2.  From `generate_article_node`, go to `summarize_article_node`.\n",
        "    3.  From `summarize_article_node`, `END` the workflow."
      ],
      "metadata": {
        "id": "tbT2FhckTiJ5"
      },
      "id": "tbT2FhckTiJ5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the graph\n",
        "workflow = StateGraph(ContentWorkflowState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"generate_article\", generate_article_node)\n",
        "workflow.add_node(\"summarize_article\", summarize_article_node)\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"generate_article\")\n",
        "\n",
        "# Define edges\n",
        "workflow.add_edge(\"generate_article\", \"summarize_article\")\n",
        "workflow.add_edge(\"summarize_article\", END)\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow compiled!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "u48bMnW8TiJ6"
      },
      "id": "u48bMnW8TiJ6",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.2: Execute the Workflow with Streaming Observation\n",
        "Run the compiled `app` with a sample topic. Pay close attention to the console output to observe the streaming chunks being printed in real-time."
      ],
      "metadata": {
        "id": "QNYElxiHTiJ6"
      },
      "id": "QNYElxiHTiJ6"
    },
    {
      "cell_type": "code",
      "source": [
        "sample_topic = \"The Benefits of Renewable Energy Sources\"\n",
        "\n",
        "print(f\"\\n\\n=========== Starting Workflow for Topic: '{sample_topic}' ===========\")\n",
        "initial_state = {\n",
        "    \"topic\": sample_topic,\n",
        "    \"generated_article\": \"\",\n",
        "    \"summary\": \"\"\n",
        "}\n",
        "\n",
        "final_state = None\n",
        "try:\n",
        "    # Using .stream() on the compiled graph itself to get state updates\n",
        "    # The streaming output from the LLM is handled within the nodes.\n",
        "    for state_update in app.stream(initial_state):\n",
        "        print(f\"\\n[LangGraph State Update]: {state_update}\")\n",
        "        final_state = state_update\n",
        "\n",
        "    print(\"\\n--- Workflow Execution Complete --- \")\n",
        "    print(\"\\nFinal Article:\\n\", final_state.get(\"generated_article\", \"N/A\"))\n",
        "    print(\"\\nFinal Summary:\\n\", final_state.get(\"summary\", \"N/A\"))\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during workflow execution: {e}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "FCLgutMwTiJ6"
      },
      "id": "FCLgutMwTiJ6",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "0MLEyOcKTiJ7"
      },
      "id": "0MLEyOcKTiJ7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Analysis and Reflection\n",
        "Provide a comprehensive summary of your findings and reflections based on this assignment."
      ],
      "metadata": {
        "id": "0H6W-mlETiJ7"
      },
      "id": "0H6W-mlETiJ7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Observation of Streaming**: Did you observe the content and summary being printed character by character or word by word? Describe your observation.\n",
        "* **Implementation Details**: Explain how you implemented streaming within each node. What specific method or approach did you use (`.stream()`, `yield`, etc.) and why?\n",
        "* **Benefits of Streaming**: Discuss the advantages of using streaming responses in LLM applications, especially in the context of user experience and perceived performance.\n",
        "* **Challenges/Considerations**: What are some potential challenges or considerations when implementing streaming in a multi-node workflow (e.g., handling partial outputs, error handling, integrating with UI)?\n",
        "* **Real-world Applications**: Provide examples of real-world applications where streaming LLM responses would be crucial or highly beneficial."
      ],
      "metadata": {
        "id": "JXKFSTAQTiJ7"
      },
      "id": "JXKFSTAQTiJ7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ZQQSgxMKTiJ7"
      },
      "id": "ZQQSgxMKTiJ7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission:\n",
        "* Ensure all code cells have been executed and their outputs are visible.\n",
        "* All analysis and reflections are clearly written in markdown cells.\n",
        "* Save your Jupyter Notebook as `[YourName]_LangGraph_Streaming_Assignment.ipynb`."
      ],
      "metadata": {
        "id": "dsOQcs-uTiJ7"
      },
      "id": "dsOQcs-uTiJ7"
    }
  ]
}