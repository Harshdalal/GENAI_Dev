{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Automate Review Classification with LangGraph\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "QGWvbnELQL4-"
      },
      "id": "QGWvbnELQL4-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective:\n",
        "This assignment challenges you to **design and implement a robust review classification workflow using LangGraph**. You'll leverage LangGraph's state management and conditional routing capabilities to create an automated agent that can classify customer reviews (e.g., positive, negative, neutral) and handle ambiguous cases by requesting more information or escalating. This project will solidify your understanding of building sophisticated, multi-step LLM applications."
      ],
      "metadata": {
        "id": "rNrHC5YbQL4_"
      },
      "id": "rNrHC5YbQL4_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "tOB_HQGuQL4_"
      },
      "id": "tOB_HQGuQL4_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions:\n",
        "1.  **LLM Access**: You'll need access to an LLM API (e.g., Google's Gemini, OpenAI's GPT-4). For this assignment, we'll primarily use **Google's Gemini Pro model** via the `langchain-google-genai` integration.\n",
        "2.  **Environment Setup**: Install the necessary Python libraries: `pip install langchain-google-genai langchain_community langgraph pydantic`.\n",
        "3.  **API Key**: Securely handle your API key. It's best practice to load it from an environment variable.\n",
        "4.  **Jupyter Notebook**: All your code, outputs, workflow diagrams (if you generate them), observations, and analysis must be documented in this Jupyter Notebook.\n",
        "5.  **Review Samples**: Prepare a set of diverse customer reviews for testing (positive, negative, neutral, and ambiguous ones).\n",
        "6.  **Analysis**: Critically evaluate your LangGraph workflow's ability to classify reviews and handle different scenarios."
      ],
      "metadata": {
        "id": "UCEaZfRSQL5A"
      },
      "id": "UCEaZfRSQL5A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "baWorfsoQL5A"
      },
      "id": "baWorfsoQL5A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Workflow State Definition\n",
        "Begin by configuring your LLM and defining the state that your LangGraph workflow will manage."
      ],
      "metadata": {
        "id": "HaPvBHB6QL5A"
      },
      "id": "HaPvBHB6QL5A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1: API Configuration and LLM Initialization\n",
        "Set up your Google Generative AI API key and initialize the `ChatGoogleGenerativeAI` model."
      ],
      "metadata": {
        "id": "7vgbIoikQL5A"
      },
      "id": "7vgbIoikQL5A"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Literal\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# --- YOUR API KEY HERE ---\n",
        "# It's highly recommended to load your API key from an environment variable for security.\n",
        "# For example:\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
        "# For this assignment, you can temporarily paste it directly, but be careful not to share your notebook with the key.\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY_HERE\" # Replace with your actual API key\n",
        "\n",
        "# Initialize the LLM (Gemini Pro)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.0) # Low temperature for classification tasks\n",
        "\n",
        "print(\"LLM initialized with Gemini Pro!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "LQjIDNWsQL5B"
      },
      "id": "LQjIDNWsQL5B",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Define Workflow State\n",
        "Define a `TypedDict` to represent the state of your LangGraph workflow. This state will hold information as the review progresses through the classification process."
      ],
      "metadata": {
        "id": "tp4Lp6XDQL5C"
      },
      "id": "tp4Lp6XDQL5C"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Requirements for State**:\n",
        "    * `review_text`: The original customer review string.\n",
        "    * `classification`: The predicted sentiment (e.g., \"Positive\", \"Negative\", \"Neutral\", \"Ambiguous\"). This should be optional as it's determined later.\n",
        "    * `clarification_needed`: A boolean indicating if more information is required for ambiguous reviews.\n",
        "    * `escalation_reason`: An optional string describing why a review was escalated (e.g., \"Ambiguous sentiment\", \"Requires human intervention\")."
      ],
      "metadata": {
        "id": "3e6eFlsrQL5C"
      },
      "id": "3e6eFlsrQL5C"
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewClassificationState(TypedDict):\n",
        "    review_text: str\n",
        "    classification: Literal[\"Positive\", \"Negative\", \"Neutral\", \"Ambiguous\", \"Escalated\", \"N/A\"] # N/A for initial state\n",
        "    clarification_needed: bool\n",
        "    escalation_reason: str # Default to empty string\n",
        "\n",
        "print(\"ReviewClassificationState defined!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "Mplze2omQL5C"
      },
      "id": "Mplze2omQL5C",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "_BusSWwXQL5C"
      },
      "id": "_BusSWwXQL5C"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Define Nodes for the Workflow\n",
        "Create the individual nodes (functions) that represent steps in your classification process."
      ],
      "metadata": {
        "id": "6bz8eOOuQL5C"
      },
      "id": "6bz8eOOuQL5C"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1: `classify_review` Node\n",
        "This node will use the LLM to classify the sentiment of a review. It should output one of \"Positive\", \"Negative\", \"Neutral\", or \"Ambiguous\"."
      ],
      "metadata": {
        "id": "cwdoZpDdQL5D"
      },
      "id": "cwdoZpDdQL5D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Prompt Design**: Create a prompt that clearly instructs the LLM to classify the review and specify the allowed output categories. Consider using Pydantic for structured output.\n",
        "* **Output**: The node should update the `classification` field in the state."
      ],
      "metadata": {
        "id": "KY865eLVQL5D"
      },
      "id": "KY865eLVQL5D"
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationOutput(BaseModel):\n",
        "    sentiment: Literal[\"Positive\", \"Negative\", \"Neutral\", \"Ambiguous\"]\n",
        "    reason: str = Field(description=\"Brief reason for the sentiment classification.\")\n",
        "\n",
        "classification_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert review sentiment classifier. Classify the user's review into one of 'Positive', 'Negative', 'Neutral', or 'Ambiguous'. Provide a brief reason.\"),\n",
        "    (\"user\", \"Review: {review_text}\")\n",
        "]).partial(\n",
        "    format_instructions=ClassificationOutput.schema_json()\n",
        ")\n",
        "\n",
        "classification_chain = classification_prompt | llm.with_structured_output(ClassificationOutput)\n",
        "\n",
        "def classify_review_node(state: ReviewClassificationState) -> ReviewClassificationState:\n",
        "    print(\"\\n--- Node: Classifying Review ---\")\n",
        "    review_text = state[\"review_text\"]\n",
        "\n",
        "    try:\n",
        "        classification_result = classification_chain.invoke({\"review_text\": review_text})\n",
        "        print(f\"Raw classification result: {classification_result}\")\n",
        "\n",
        "        # Update the state based on the classification result\n",
        "        new_classification = classification_result.sentiment\n",
        "\n",
        "        return {\n",
        "            \"classification\": new_classification,\n",
        "            \"clarification_needed\": (new_classification == \"Ambiguous\"),\n",
        "            \"escalation_reason\": \"\" # Clear any previous escalation reason\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error classifying review: {e}\")\n",
        "        # Fallback to an error state or escalation if classification fails\n",
        "        return {\n",
        "            \"classification\": \"Escalated\",\n",
        "            \"clarification_needed\": False,\n",
        "            \"escalation_reason\": f\"Classification failed: {e}\"\n",
        "        }\n",
        "\n",
        "print(\"classify_review_node defined!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "iZxHQ9JNQL5D"
      },
      "id": "iZxHQ9JNQL5D",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2: `request_clarification` Node\n",
        "This node is called if the review is classified as \"Ambiguous\". It should generate a follow-up question to ask the customer or an internal note for a human agent."
      ],
      "metadata": {
        "id": "_2AmAmWeQL5D"
      },
      "id": "_2AmAmWeQL5D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Prompt Design**: Craft a prompt that takes the ambiguous review and generates a concise question to clarify its sentiment.\n",
        "* **Output**: The node should update the `escalation_reason` field with the clarification question, and set `clarification_needed` to `True` (though it should already be true). This is where a real system might send an email or trigger an external action."
      ],
      "metadata": {
        "id": "luBp2ihfQL5D"
      },
      "id": "luBp2ihfQL5D"
    },
    {
      "cell_type": "code",
      "source": [
        "clarification_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. The following review is ambiguous. Generate a concise question to clarify its sentiment or intent. The question should be directed to the user, or an internal note for a human agent.\"),\n",
        "    (\"user\", \"Ambiguous Review: {review_text}\")\n",
        "])\n",
        "\n",
        "clarification_chain = clarification_prompt | llm\n",
        "\n",
        "def request_clarification_node(state: ReviewClassificationState) -> ReviewClassificationState:\n",
        "    print(\"\\n--- Node: Requesting Clarification ---\")\n",
        "    review_text = state[\"review_text\"]\n",
        "\n",
        "    try:\n",
        "        clarification_question = clarification_chain.invoke({\"review_text\": review_text}).content\n",
        "        print(f\"Clarification requested: {clarification_question}\")\n",
        "        return {\n",
        "            \"classification\": \"Ambiguous\",\n",
        "            \"clarification_needed\": True,\n",
        "            \"escalation_reason\": f\"Clarification needed: {clarification_question}\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error requesting clarification: {e}\")\n",
        "        return {\n",
        "            \"classification\": \"Escalated\",\n",
        "            \"clarification_needed\": False,\n",
        "            \"escalation_reason\": f\"Failed to request clarification: {e}\"\n",
        "        }\n",
        "\n",
        "print(\"request_clarification_node defined!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "4OabrcpAQL5D"
      },
      "id": "4OabrcpAQL5D",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.3: `escalate_review` Node\n",
        "This node handles reviews that cannot be automatically classified or require human attention. It should set the `escalation_reason`."
      ],
      "metadata": {
        "id": "GK9w_SL4QL5E"
      },
      "id": "GK9w_SL4QL5E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Purpose**: This node simply marks the review for human review.\n",
        "* **Output**: It should update the `classification` to \"Escalated\" and provide a suitable `escalation_reason`."
      ],
      "metadata": {
        "id": "JeaGJUhJQL5E"
      },
      "id": "JeaGJUhJQL5E"
    },
    {
      "cell_type": "code",
      "source": [
        "def escalate_review_node(state: ReviewClassificationState) -> ReviewClassificationState:\n",
        "    print(\"\\n--- Node: Escalating Review ---\")\n",
        "    # This node is triggered when classification is needed but ambiguous, or direct escalation\n",
        "    current_classification = state.get(\"classification\", \"N/A\")\n",
        "    current_reason = state.get(\"escalation_reason\", \"\")\n",
        "\n",
        "    if current_classification == \"Ambiguous\":\n",
        "        reason = current_reason if current_reason else \"Sentiment is ambiguous and requires human review.\"\n",
        "    else:\n",
        "        reason = current_reason if current_reason else \"Review requires manual review due to unexpected scenario or LLM failure.\"\n",
        "\n",
        "    print(f\"Review escalated. Reason: {reason}\")\n",
        "    return {\n",
        "        \"classification\": \"Escalated\",\n",
        "        \"clarification_needed\": False, # Once escalated, clarification isn't needed by the system, but by human\n",
        "        \"escalation_reason\": reason\n",
        "    }\n",
        "\n",
        "print(\"escalate_review_node defined!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "8Du_m8YNQL5E"
      },
      "id": "8Du_m8YNQL5E",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "5eEJBYS1QL5E"
      },
      "id": "5eEJBYS1QL5E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Constructing the LangGraph Workflow\n",
        "Combine your nodes into a directed graph using LangGraph's `StateGraph`."
      ],
      "metadata": {
        "id": "EavZ_iVvQL5E"
      },
      "id": "EavZ_iVvQL5E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.1: Define Edges and Conditional Logic\n",
        "Define the graph's nodes, entry point, and the conditional edges that determine the flow based on the review classification."
      ],
      "metadata": {
        "id": "PxAs1rLEQL5E"
      },
      "id": "PxAs1rLEQL5E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Graph Flow**:\n",
        "    1.  Start at `classify_review_node`.\n",
        "    2.  From `classify_review_node`:\n",
        "        * If classification is \"Positive\", \"Negative\", or \"Neutral\", end the workflow.\n",
        "        * If classification is \"Ambiguous\", go to `request_clarification_node`.\n",
        "        * If classification is \"Escalated\" (e.g., due to an error in `classify_review_node`), go to `escalate_review_node`.\n",
        "    3.  From `request_clarification_node`: Go to `escalate_review_node` (as automated clarification isn't handled in this basic example, it always escalates for human review after generating the clarification question).\n",
        "    4.  From `escalate_review_node`: End the workflow."
      ],
      "metadata": {
        "id": "sHddQRvpQL5F"
      },
      "id": "sHddQRvpQL5F"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the graph\n",
        "workflow = StateGraph(ReviewClassificationState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"classify\", classify_review_node)\n",
        "workflow.add_node(\"clarify\", request_clarification_node)\n",
        "workflow.add_node(\"escalate\", escalate_review_node)\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"classify\")\n",
        "\n",
        "# Define conditional edge from classify\n",
        "def route_classification(state: ReviewClassificationState) -> Literal[\"clarify\", \"escalate\", END]:\n",
        "    classification = state[\"classification\"]\n",
        "    if classification == \"Ambiguous\":\n",
        "        return \"clarify\"\n",
        "    elif classification == \"Escalated\":\n",
        "        return \"escalate\"\n",
        "    else:\n",
        "        return END # Positive, Negative, Neutral reviews end here\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"classify\",\n",
        "    route_classification,\n",
        "    {\n",
        "        \"clarify\": \"clarify\",\n",
        "        \"escalate\": \"escalate\",\n",
        "        END: END,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Define edges from clarify and escalate\n",
        "workflow.add_edge(\"clarify\", \"escalate\") # After requesting clarification, it's considered escalated for human\n",
        "workflow.add_edge(\"escalate\", END)\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow compiled!\")\n",
        "\n",
        "# Optional: Visualize the graph (requires 'graphviz' and 'pydotplus')\n",
        "# try:\n",
        "#     from IPython.display import Image, display\n",
        "#     display(Image(app.get_graph().draw_png()))\n",
        "# except ImportError:\n",
        "#     print(\"Install graphviz and pydotplus for visualization: pip install graphviz pydotplus\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "mGAhbUH0QL5F"
      },
      "id": "mGAhbUH0QL5F",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "k8mwATGKQL5F"
      },
      "id": "k8mwATGKQL5F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Testing the LangGraph Workflow\n",
        "Test your workflow with various review samples to ensure it behaves as expected."
      ],
      "metadata": {
        "id": "Zga1cnXVQL5F"
      },
      "id": "Zga1cnXVQL5F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4.1: Test Cases\n",
        "Run the workflow with the following types of reviews. For each, print the final state of the workflow and any intermediate steps you observe."
      ],
      "metadata": {
        "id": "NheWVwlPQL5F"
      },
      "id": "NheWVwlPQL5F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Test Reviews**:\n",
        "    1.  **Positive**: \"This product is absolutely amazing! Exceeded all my expectations. Five stars!\"\n",
        "    2.  **Negative**: \"Worst customer service ever. My order was late and damaged. Highly disappointed.\"\n",
        "    3.  **Neutral**: \"The delivery was on time, and the packaging was adequate. The item itself is functional.\"\n",
        "    4.  **Ambiguous**: \"It's okay, I guess. Not great, but not terrible. I'm just very confused about how to use it.\"\n",
        "    5.  **Very Short/Unclear**: \"Meh.\"\n",
        "    6.  **Edge Case/Complex**: \"I love the design, but it broke after one week and the return process is a nightmare. I want a refund but also think it's a great concept.\""
      ],
      "metadata": {
        "id": "j8oM6ckHQL5F"
      },
      "id": "j8oM6ckHQL5F"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_workflow_and_print_state(review_text: str):\n",
        "    print(f\"\\n\\n=========== Processing Review: '{review_text}' ===========\")\n",
        "    initial_state = {\n",
        "        \"review_text\": review_text,\n",
        "        \"classification\": \"N/A\",\n",
        "        \"clarification_needed\": False,\n",
        "        \"escalation_reason\": \"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        for state in app.stream(initial_state):\n",
        "            print(f\"Current state: {state}\")\n",
        "        print(f\"Final state for '{review_text}': {state}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during workflow execution: {e}\")\n",
        "\n",
        "# --- Run your test cases ---\n",
        "test_reviews = [\n",
        "    \"This product is absolutely amazing! Exceeded all my expectations. Five stars!\", # Positive\n",
        "    \"Worst customer service ever. My order was late and damaged. Highly disappointed.\", # Negative\n",
        "    \"The delivery was on time, and the packaging was adequate. The item itself is functional.\", # Neutral\n",
        "    \"It's okay, I guess. Not great, but not terrible. I'm just very confused about how to use it.\", # Ambiguous\n",
        "    \"Meh.\", # Very Short/Unclear\n",
        "    \"I love the design, but it broke after one week and the return process is a nightmare. I want a refund but also think it's a great concept.\" # Edge Case/Complex\n",
        "]\n",
        "\n",
        "for review in test_reviews:\n",
        "    run_workflow_and_print_state(review)"
      ],
      "outputs": [],
      "metadata": {
        "id": "hH9G7mfZQL5F"
      },
      "id": "hH9G7mfZQL5F",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis for Task 4.1:\n",
        "* For each test review, did the workflow produce the expected classification (Positive, Negative, Neutral, Ambiguous, Escalated)?\n",
        "* Did the ambiguous and edge cases correctly trigger the `clarify` and `escalate` nodes? What clarification questions or escalation reasons were generated?\n",
        "* Were there any unexpected behaviors or misclassifications? If so, why do you think they occurred?\n",
        "* Discuss the advantages of using LangGraph (or similar state-machine libraries) for complex LLM workflows compared to simple sequential chains."
      ],
      "metadata": {
        "id": "9rwK18wDQL5G"
      },
      "id": "9rwK18wDQL5G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "T--Rb-WwQL5G"
      },
      "id": "T--Rb-WwQL5G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Conclusion and Reflection\n",
        "In this markdown cell, provide a comprehensive summary of your findings and reflections based on this assignment."
      ],
      "metadata": {
        "id": "xdO9fDOeQL5G"
      },
      "id": "xdO9fDOeQL5G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Key Learnings about LangGraph**: What were your main takeaways regarding LangGraph's utility for building multi-step LLM applications?\n",
        "* **Challenges Encountered**: What difficulties did you face during the assignment (e.g., defining state, routing logic, debugging)?\n",
        "* **Workflow Improvements**: If you were to enhance this review classification workflow, what specific new nodes, states, or conditional logic would you add (e.g., human-in-the-loop for ambiguous reviews, specific product category classification, integrating external tools)?\n",
        "* **Practical Applications**: How can automated review classification be applied in real-world business scenarios (e.g., customer support routing, product analytics, marketing insights)?\n",
        "* **Ethical Considerations**: Discuss any ethical implications related to automating sentiment analysis or classification, such as potential for bias in the LLM, misinterpretation of nuanced language, or impact on customer relations."
      ],
      "metadata": {
        "id": "_hdIua4_QL5G"
      },
      "id": "_hdIua4_QL5G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "jCpbCbLwQL5G"
      },
      "id": "jCpbCbLwQL5G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission:\n",
        "* Ensure all code cells have been executed and their outputs are visible.\n",
        "* All analysis and reflections are clearly written in markdown cells.\n",
        "* Save your Jupyter Notebook as `[YourName]_LangGraph_Review_Classification_Assignment.ipynb`."
      ],
      "metadata": {
        "id": "tQwLZeM9QL5G"
      },
      "id": "tQwLZeM9QL5G"
    }
  ]
}