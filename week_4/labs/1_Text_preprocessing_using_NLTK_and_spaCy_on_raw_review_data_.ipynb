{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#‚úÖ Practical: Text Preprocessing Using NLTK and spaCy\n",
        "\n",
        "\n",
        "#üéØ Objective:\n",
        "\n",
        "Clean and preprocess raw customer reviews using:\n",
        "\n",
        "NLTK for tokenization, stopword removal, and stemming\n",
        "\n",
        "spaCy for advanced lemmatization and named entity recognition (NER)\n",
        "\n",
        "\n",
        "#üõçÔ∏è Scenario: Synthetic Review Dataset (e.g., product reviews)\n",
        "\n",
        "üß© Step 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "GjoMyVxae7md"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rgkGfS5ek2I",
        "outputId": "b51668ff-1056-4b0e-ae52-9815f1c0bedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Run this if not already installed\n",
        "# Installs the Natural Language Toolkit (NLTK) and spaCy libraries.\n",
        "# NLTK is used for various text processing tasks, and spaCy is an industrial-strength\n",
        "# natural language processing (NLP) library.\n",
        "!pip install nltk spacy\n",
        "\n",
        "# Download required NLTK data\n",
        "import nltk\n",
        "# Downloads the 'punkt' tokenizer model from NLTK. This model is used for\n",
        "# tokenizing text into sentences or words.\n",
        "nltk.download('punkt')\n",
        "# Downloads the 'stopwords' corpus from NLTK. Stopwords are common words (e.g., \"the\", \"is\", \"and\")\n",
        "# that are often removed from text before processing to focus on more meaningful terms.\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download spaCy model\n",
        "# Downloads the small English language model for spaCy ('en_core_web_sm').\n",
        "# This model includes pre-trained pipelines for tasks like tokenization, part-of-speech tagging,\n",
        "# named entity recognition, and more, providing a good balance of speed and accuracy for many NLP tasks.\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üßæ Step 2: Create Synthetic Review Data"
      ],
      "metadata": {
        "id": "VWsbMI7QfIiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample review data\n",
        "reviews = [\n",
        "    \"I absolutely loved the product! It works like a charm.\",\n",
        "    \"Terrible customer service. I waited 30 minutes to talk to an agent.\",\n",
        "    \"The packaging was okay, but the item arrived damaged.\",\n",
        "    \"Excellent value for the price. Will buy again.\",\n",
        "    \"Worst purchase ever. Completely useless and broke in a day!\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "Q1Xq5DrdfCUl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üî§ Step 3: Preprocessing with NLTK"
      ],
      "metadata": {
        "id": "hIcm8w5nfMbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "# Initialize components\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_nltk(text):\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "    # 2. Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # 3. Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # 4. Remove stopwords and stem\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"üîπ Preprocessing with NLTK:\\n\")\n",
        "for i, review in enumerate(reviews):\n",
        "    print(f\"Review {i+1}: {preprocess_nltk(review)}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsykGOfbfKuk",
        "outputId": "3f39e5c0-a437-4449-93c3-689fa75fa9f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Preprocessing with NLTK:\n",
            "\n",
            "Review 1: ['absolut', 'love', 'product', 'work', 'like', 'charm']\n",
            "\n",
            "Review 2: ['terribl', 'custom', 'servic', 'wait', '30', 'minut', 'talk', 'agent']\n",
            "\n",
            "Review 3: ['packag', 'okay', 'item', 'arriv', 'damag']\n",
            "\n",
            "Review 4: ['excel', 'valu', 'price', 'buy']\n",
            "\n",
            "Review 5: ['worst', 'purchas', 'ever', 'complet', 'useless', 'broke', 'day']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üß† Step 4: Preprocessing with spaCy"
      ],
      "metadata": {
        "id": "8JlPA-nifRel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "# Loads the small English language model pre-trained for spaCy.\n",
        "# This model provides various linguistic features like tokenization, part-of-speech tagging,\n",
        "# lemmatization, and named entity recognition.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_spacy(text):\n",
        "    # Process the input text with the loaded spaCy NLP model.\n",
        "    # This creates a 'Doc' object containing linguistic annotations for the text.\n",
        "    doc = nlp(text)\n",
        "    # Lemmatize and remove stopwords and punctuation\n",
        "    # Iterates through each token in the processed document.\n",
        "    # - `token.lemma_`: Returns the base form of the word (lemmatization).\n",
        "    # - `token.lower()`: Converts the lemma to lowercase.\n",
        "    # - `not token.is_stop`: Checks if the token is not a stop word (e.g., \"the\", \"is\").\n",
        "    # - `not token.is_punct`: Checks if the token is not punctuation.\n",
        "    # The list comprehension creates a list of filtered and lemmatized tokens.\n",
        "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return tokens\n",
        "\n",
        "print(\"üîπ Preprocessing with spaCy:\\n\")\n",
        "# Assuming 'reviews' is an iterable (e.g., a list of strings) containing text reviews.\n",
        "# This loop iterates through each review, preprocesses it using the `preprocess_spacy` function,\n",
        "# and prints the preprocessed tokens for each review.\n",
        "for i, review in enumerate(reviews):\n",
        "    print(f\"Review {i+1}: {preprocess_spacy(review)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DD1r7zKfOfC",
        "outputId": "78163ea3-5562-43a1-8c25-560c3ed2d6a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Preprocessing with spaCy:\n",
            "\n",
            "Review 1: ['absolutely', 'love', 'product', 'work', 'like', 'charm']\n",
            "\n",
            "Review 2: ['terrible', 'customer', 'service', 'wait', '30', 'minute', 'talk', 'agent']\n",
            "\n",
            "Review 3: ['packaging', 'okay', 'item', 'arrive', 'damage']\n",
            "\n",
            "Review 4: ['excellent', 'value', 'price', 'buy']\n",
            "\n",
            "Review 5: ['bad', 'purchase', 'completely', 'useless', 'break', 'day']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üéØ Step 5: Named Entity Recognition (NER) with spaCy"
      ],
      "metadata": {
        "id": "_PZ2jrR_fzh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üîç Named Entities:\\n\")\n",
        "# Iterate through each review in the 'reviews' list along with its index.\n",
        "for i, review in enumerate(reviews):\n",
        "    # Process the current review text using the pre-loaded spaCy NLP model (nlp).\n",
        "    # This creates a 'Doc' object, which is a container for accessing linguistic annotations.\n",
        "    doc = nlp(review)\n",
        "    print(f\"Review {i+1} Entities:\")\n",
        "    # Iterate through each detected named entity in the 'doc' object.\n",
        "    # Named entities are real-world objects such as persons, organizations, locations, etc.\n",
        "    for ent in doc.ents:\n",
        "        # Print the text of the named entity and its corresponding label (type of entity).\n",
        "        # `ent.text` gives the exact text span of the entity.\n",
        "        # `ent.label_` gives the string representation of the entity type (e.g., PERSON, GPE for geopolitical entity).\n",
        "        print(f\" - {ent.text} ({ent.label_})\")\n",
        "    print() # Print a blank line for better readability between reviews."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RH2sP_yf0eL",
        "outputId": "01a476e7-1cc9-4476-fdd9-a03fa2e3bb88"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Named Entities:\n",
            "\n",
            "Review 1 Entities:\n",
            "\n",
            "Review 2 Entities:\n",
            " - 30 minutes (TIME)\n",
            "\n",
            "Review 3 Entities:\n",
            "\n",
            "Review 4 Entities:\n",
            "\n",
            "Review 5 Entities:\n",
            " - a day (DATE)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#‚úÖ Summary Comparison"
      ],
      "metadata": {
        "id": "4BzVp0V_f5lP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature           | NLTK                            | spaCy                          |\n",
        "| ----------------- | ------------------------------- | ------------------------------ |\n",
        "| Tokenization      | `word_tokenize()`               | Built-in via `nlp()`           |\n",
        "| Stopword Removal  | Manual with `stopwords.words()` | `token.is_stop`                |\n",
        "| Stemming          | `PorterStemmer` (rule-based)    | ‚ùå                              |\n",
        "| Lemmatization     | ‚ùå                               | `token.lemma_` (context-aware) |\n",
        "| Named Entity Rec. | ‚ùå                               | ‚úÖ `doc.ents`                   |\n"
      ],
      "metadata": {
        "id": "yccPPd17f7sz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bN559Cdkf3dL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}