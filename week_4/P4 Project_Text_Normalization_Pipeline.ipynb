{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoHsxvUiaNOM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project: Text Normalization Pipeline"
      ],
      "metadata": {
        "id": "Wunz076laNnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective\n",
        "\n",
        "Build a reusable text normalization pipeline that can process raw text data into a clean and structured format for downstream NLP tasks."
      ],
      "metadata": {
        "id": "fepiZoOaaP8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stwp 1. Import Necessary Libraries\n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources for stopwords, tokenization, and lemmatization\n",
        "nltk.download('stopwords')  # For stopword removal\n",
        "nltk.download('punkt')      # For tokenization\n",
        "nltk.download('wordnet')    # For lemmatization\n",
        "\n",
        "#Explanation:\n",
        "\n",
        "#re: Regular expression library to perform text cleaning tasks such as removing URLs, mentions, and special characters.\n",
        "#string: Provides a collection of string constants, like punctuation characters.\n",
        "#nltk: Natural Language Toolkit, a powerful library for working with text data. We use it here for stopwords, tokenization, and lemmatization.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6-mrhnVaQR1",
        "outputId": "28da7d77-619d-4dcf-dd92-a3b8936c727c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Define the Contraction Expansion Map\n",
        "\n",
        "# Define a mapping of common contractions to their expanded forms\n",
        "contraction_map = {\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mustn't\": \"must not\"\n",
        "}\n",
        "\n",
        "#Explanation:\n",
        "\n",
        "#This dictionary maps contractions (e.g., \"can't\", \"won't\") to their expanded forms (e.g., \"cannot\", \"will not\").\n",
        "#This is important for understanding the full meaning of text, as contractions can make automated processing harder.\n",
        "\n",
        "# Define the Text Normalization Functions\n",
        "# Expand Contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    \"\"\"\n",
        "    This function expands contractions in the input text.\n",
        "    It will look for any contractions in the `contraction_map` dictionary and replace them with their expanded forms.\n",
        "    \"\"\"\n",
        "    # Loop through all contractions and replace them with expanded forms\n",
        "    for contraction, expanded in contraction_map.items():\n",
        "        text = re.sub(r'\\b' + contraction + r'\\b', expanded, text)\n",
        "    return text\n",
        "\n",
        "\n",
        "#Explanation:\n",
        "\n",
        "#The expand_contractions function looks for words in the text that match entries in the contraction_map and replaces them with their expanded versions.\n",
        "#The \\b ensures that only whole words are matched (e.g., \"can't\" but not \"scanner\").\n",
        "\n",
        "#Normalize Text\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    This function normalizes the input text. It performs the following operations:\n",
        "    - Converts text to lowercase\n",
        "    - Expands contractions\n",
        "    - Removes URLs, mentions, hashtags, and punctuation\n",
        "    - Tokenizes the text (splits into individual words)\n",
        "    - Removes stopwords\n",
        "    - Lemmatizes the words (converts to root form)\n",
        "    \"\"\"\n",
        "    # Convert the text to lowercase\n",
        "    text = text.lower()  # Converts all text to lowercase to ensure uniformity\n",
        "\n",
        "    # Expand contractions (e.g., \"i'm\" -> \"i am\")\n",
        "    text = expand_contractions(text)\n",
        "\n",
        "    # Remove URLs (e.g., http://example.com)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Regular expression to remove URLs\n",
        "\n",
        "    # Remove mentions and hashtags (e.g., @username and #hashtag)\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)  # Remove Twitter mentions and hashtags\n",
        "\n",
        "    # Remove punctuation (e.g., commas, periods, exclamation marks)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Use str.maketrans to remove punctuation\n",
        "\n",
        "    # Tokenize the text (split it into individual words)\n",
        "    tokens = word_tokenize(text)  # Tokenizes text into words\n",
        "\n",
        "    # Remove stopwords (common words like \"the\", \"and\", \"is\", etc.)\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Filters out stopwords\n",
        "\n",
        "    # Lemmatize the words (reduce words to their root form, e.g., \"running\" -> \"run\")\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize each token\n",
        "\n",
        "    # Join the tokens back into a single string\n",
        "    normalized_text = ' '.join(tokens)\n",
        "\n",
        "    return normalized_text\n",
        "\n",
        "#Explanation:\n",
        "\n",
        "#Lowercasing: Converts all text to lowercase so that the text is uniform (e.g., \"Apple\" and \"apple\" are treated the same).\n",
        "#Contraction Expansion: Expands contractions (e.g., \"can't\" becomes \"cannot\") for clearer interpretation.\n",
        "#URL Removal: Removes any URLs in the text using a regular expression (http\\S+ matches anything starting with \"http\", and \\S+ matches any non-whitespace characters).\n",
        "#Mention and Hashtag Removal: Removes social media mentions (e.g., @user) and hashtags (e.g., #hashtag).\n",
        "#Punctuation Removal: Strips out punctuation marks like periods, commas, and exclamation marks.\n",
        "#Tokenization: Breaks the text into individual words (tokens) using word_tokenize.\n",
        "#Stopword Removal: Removes common words (e.g., \"the\", \"and\") that don't carry meaningful information.\n",
        "#Lemmatization: Reduces words to their base form, e.g., \"running\" becomes \"run\", using WordNetLemmatizer.\n",
        "#Join Tokens: Converts the list of tokens back into a single string for further processing or analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "Otqa8jvMaXfn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step3. Test the Pipeline\n",
        "# Example raw text data\n",
        "raw_texts = [\n",
        "    \"I can't believe this product! It's AMAZING üòç #awesome\",\n",
        "    \"Check out our website: https://example.com for great deals!\",\n",
        "    \"I'm SO disappointed... The service was terrible. üò° @Company\",\n",
        "    \"We're loving the new features of your app! Keep it up! üëç\",\n",
        "    \"Why isn't this app working? Fix it!!! #frustrated\"\n",
        "]\n",
        "\n",
        "# Apply normalization\n",
        "normalized_texts = [normalize_text(text) for text in raw_texts]\n",
        "\n",
        "# Print results\n",
        "for i, (raw, normalized) in enumerate(zip(raw_texts, normalized_texts)):\n",
        "    print(f\"Original Text {i+1}: {raw}\")\n",
        "    print(f\"Normalized Text {i+1}: {normalized}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "#Explanation:\n",
        "\n",
        "#Test the Pipeline: We apply the normalize_text function to several example text entries.\n",
        "#These raw texts contain URLs, mentions, hashtags, punctuation, and contractions.\n",
        "#The results are compared by printing both the original and normalized versions to demonstrate how the pipeline cleans and processes the text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yams5D5UahuU",
        "outputId": "a2e8f1af-48a2-4cca-beca-6c2bf8c9f9e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text 1: I can't believe this product! It's AMAZING üòç #awesome\n",
            "Normalized Text 1: believe product amazing üòç\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text 2: Check out our website: https://example.com for great deals!\n",
            "Normalized Text 2: check website great deal\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text 3: I'm SO disappointed... The service was terrible. üò° @Company\n",
            "Normalized Text 3: disappointed service terrible üò°\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text 4: We're loving the new features of your app! Keep it up! üëç\n",
            "Normalized Text 4: loving new feature app keep üëç\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text 5: Why isn't this app working? Fix it!!! #frustrated\n",
            "Normalized Text 5: app working fix\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJRU1gojar79"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Use the Pipeline in Real-World Applications\n",
        "\n",
        "You can integrate this text normalization pipeline into various NLP projects, such as:\n",
        "\n",
        "\n",
        "Sentiment Analysis: Use normalized text as input for a sentiment analysis model.\n",
        "\n",
        "Topic Modeling: Process a corpus of documents to extract common topics.\n",
        "\n",
        "Chatbots: Preprocess user queries for better understanding and response\n",
        "generation.\n",
        "\n",
        "Key Features of the Pipeline\n",
        "\n",
        "Contraction Expansion: Handles informal contractions often found in real-world text.\n",
        "\n",
        "Noise Removal: Strips out URLs, hashtags, mentions, and special characters.\n",
        "Lemmatization: Ensures words are converted to their base form (e.g., \"running\" -> \"run\").\n",
        "\n",
        "Stopword Removal: Eliminates common words (e.g., \"the\", \"and\") that do not add meaning."
      ],
      "metadata": {
        "id": "rfpQ2Acfa8AP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GsAddXKvaxYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}