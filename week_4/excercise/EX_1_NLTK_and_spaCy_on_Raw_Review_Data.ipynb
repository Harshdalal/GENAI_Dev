{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B4AXIFusfRH"
      },
      "source": [
        "# Text Preprocessing Assignment: NLTK and spaCy on Raw Review Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBSuBlulsfRI"
      },
      "source": [
        "## Introduction\n",
        "This assignment focuses on the crucial step of text preprocessing in Natural Language Processing (NLP). You will utilize two popular Python libraries, NLTK and spaCy, to clean, transform, and prepare raw review data for further analysis or model building. Understanding and effectively applying these preprocessing techniques are fundamental skills for any NLP practitioner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wblnhMaysfRJ"
      },
      "source": [
        "## Learning Objectives\n",
        "Upon completion of this assignment, you should be able to:\n",
        "- Load and inspect raw text data.\n",
        "- Perform basic text cleaning operations (e.g., lowercasing, removing punctuation, numbers).\n",
        "- Understand and apply different tokenization techniques using NLTK and spaCy.\n",
        "- Identify and remove stop words effectively.\n",
        "- Differentiate between and apply stemming and lemmatization.\n",
        "- Utilize spaCy for advanced linguistic processing like Part-of-Speech (POS) tagging and Named Entity Recognition (NER).\n",
        "- Create a custom preprocessing pipeline.\n",
        "- Compare and contrast the functionalities of NLTK and spaCy for various preprocessing tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA50XZcpsfRK"
      },
      "source": [
        "## Dataset\n",
        "For this assignment, you will use a raw review dataset. You can either use a dataset provided by your instructor, or if not provided, you can download a suitable dataset from platforms like Kaggle. A good example would be a sentiment analysis dataset containing product reviews (e.g., Amazon reviews, Yelp reviews, IMDB reviews).\n",
        "\n",
        "**Assumption:** For the purpose of this notebook, we will assume you have a CSV file named `reviews.csv` with a column named `text` containing the review content. If your file or column name is different, please adjust the loading code accordingly.\n",
        "\n",
        "**If you need to download a dataset, consider searching for one of these on Kaggle:**\n",
        "- \"Amazon Product Reviews\"\n",
        "- \"Yelp Dataset\"\n",
        "- \"IMDB Movie Reviews\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSOyfDSesfRL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Download necessary NLTK data (if not already downloaded)\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load spaCy model (if not already loaded/downloaded)\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except OSError:\n",
        "    print(\"Downloading 'en_core_web_sm' spaCy model...\")\n",
        "    spacy.cli.download('en_core_web_sm')\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "# Load your dataset\n",
        "try:\n",
        "    df = pd.read_csv('reviews.csv')\n",
        "    # Assuming the review text is in a column named 'text'\n",
        "    # If your column is different, change 'text' below\n",
        "    reviews = df['text'].astype(str) # Ensure it's string type\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "    print(f\"Number of reviews: {len(reviews)}\")\n",
        "    print(\"First 5 reviews:\")\n",
        "    print(reviews.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'reviews.csv' not found. Please make sure the dataset is in the same directory or provide the correct path.\")\n",
        "    print(\"Please create a dummy DataFrame for demonstration if you don't have the file yet.\")\n",
        "    reviews = pd.Series([\n",
        "        \"This product is absolutely amazing! I love it. Highly recommended! #awesome\",\n",
        "        \"The quality was terrible. I would never buy this again. It cost $50.\",\n",
        "        \"Service was good, but the food was a bit cold. Overall 3/5 stars. ðŸŒŸðŸŒŸðŸŒŸ\",\n",
        "        \"Great experience with NLTK and spaCy. Learned a lot today at 9 AM!\",\n",
        "        \"I bought 2 of these. They arrived late. Contact customer support at 1-800-REV-VIEW.\"\n",
        "    ], name='text')\n",
        "    print(\"Using dummy data for demonstration.\")\n",
        "    print(reviews)\n",
        "\n",
        "# Select a sample review for detailed demonstration if the dataset is large\n",
        "if len(reviews) > 0:\n",
        "    sample_review = reviews.iloc[0]\n",
        "    print(f\"\\nSample Review for detailed steps: \"\\n\"{sample_review}\")\n",
        "else:\n",
        "    sample_review = \"This is a sample review for demonstration purposes.\"\n",
        "    print(f\"\\nNo reviews loaded, using generic sample: \"\\n\"{sample_review}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhQFxSFBsfRN"
      },
      "source": [
        "## Assignment Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7iSA9LjsfRO"
      },
      "source": [
        "### Question 1: Basic Text Cleaning\n",
        "Write a function `clean_text_basic(text)` that performs the following basic cleaning steps on a given string:\n",
        "1.  **Convert to Lowercase:** Convert all characters to lowercase.\n",
        "2.  **Remove Punctuation:** Remove all punctuation marks (e.g., `!`, `.`, `,`, `?`, etc.).\n",
        "3.  **Remove Numbers:** Remove all numerical digits.\n",
        "4.  **Remove Extra Whitespace:** Replace multiple spaces with a single space and strip leading/trailing whitespace.\n",
        "\n",
        "Apply this function to the `sample_review` and print the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQfS-00csfRP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjKe0tbsfRP"
      },
      "source": [
        "### Question 2: Tokenization (NLTK vs. spaCy)\n",
        "Tokenization is the process of breaking down text into individual words or units (tokens).\n",
        "\n",
        "#### 2.1 NLTK Word Tokenization\n",
        "Using NLTK's `word_tokenize`, tokenize the `cleaned_sample_review` (output from Q1). Print the first 20 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE67A8l2sfRP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzwVRvqOsfRQ"
      },
      "source": [
        "#### 2.2 spaCy Tokenization\n",
        "Using spaCy, tokenize the *original* `sample_review`. Print the first 20 tokens and for each token, also print its `text` and `is_punct` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfwhyNICsfRQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USob68pBsfRQ"
      },
      "source": [
        "#### 2.3 Comparison\n",
        "Briefly discuss the differences you observe in tokenization between NLTK and spaCy based on the `sample_review`. Which one handles punctuation and special characters more intuitively for your use case (review data)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3J2XFidsfRR"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfHa9MkCsfRR"
      },
      "source": [
        "### Question 3: Stop Word Removal\n",
        "Stop words are common words (like 'the', 'is', 'a') that often carry little meaning in text analysis and are typically removed.\n",
        "\n",
        "#### 3.1 NLTK Stop Word Removal\n",
        "Using NLTK's `stopwords` corpus, remove stop words from the tokens obtained in Question 2.1 (NLTK tokenized, cleaned sample review). Print the tokens after stop word removal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox8wOw3SsfRR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fghjsyLsfRR"
      },
      "source": [
        "#### 3.2 spaCy Stop Word Removal\n",
        "Using spaCy's built-in stop word list, remove stop words from the tokens obtained in Question 2.2 (spaCy tokenized, original sample review). Print the tokens (their `text` attribute) after stop word removal. (Hint: use `token.is_stop` attribute).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx3uRSbmsfRR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoC3v_CMsfRR"
      },
      "source": [
        "#### 3.3 Discussion\n",
        "Compare the stop word lists and the results between NLTK and spaCy. Are there any notable differences? Which approach do you prefer and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEt-ktZtsfRS"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86mnVlz4sfRS"
      },
      "source": [
        "### Question 4: Stemming vs. Lemmatization\n",
        "Stemming reduces words to their root form (stem), often by simply chopping off suffixes. Lemmatization reduces words to their base or dictionary form (lemma), considering the word's meaning.\n",
        "\n",
        "#### 4.1 NLTK Stemming (Porter Stemmer)\n",
        "Apply NLTK's `PorterStemmer` to the tokens obtained after stop word removal in Question 3.1. Print the stemmed tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3y8rjNFsfRS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlPbqfQzsfRS"
      },
      "source": [
        "#### 4.2 NLTK Lemmatization (WordNetLemmatizer)\n",
        "Apply NLTK's `WordNetLemmatizer` to the tokens obtained after stop word removal in Question 3.1. Remember to provide the POS tag for better lemmatization (e.g., `'v'` for verbs, `'n'` for nouns). For simplicity, you can initially assume all words are nouns if not explicitly tagged, or try to use a simple POS tagger for more accurate results. Print the lemmatized tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R808hi5tsfRS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9rlqW9csfRS"
      },
      "source": [
        "#### 4.3 spaCy Lemmatization\n",
        "Apply spaCy's lemmatization to the tokens obtained after stop word removal in Question 3.2. Print the lemma of each token (using `token.lemma_`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBKsQH6DsfRT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dja7Fw0jsfRT"
      },
      "source": [
        "#### 4.4 Comparison\n",
        "Compare the results of stemming and lemmatization. Provide examples from your output to illustrate the differences. When would you prefer lemmatization over stemming?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVHdS6dtsfRT"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnpQFxlNsfRT"
      },
      "source": [
        "### Question 5: Advanced spaCy Features\n",
        "spaCy excels at providing richer linguistic annotations.\n",
        "\n",
        "#### 5.1 Part-of-Speech (POS) Tagging\n",
        "For the *original* `sample_review`, use spaCy to perform POS tagging. Print each token along with its `text`, `pos_` (coarse-grained POS), and `tag_` (fine-grained POS) attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeAp80A-sfRT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mngHAO8lsfRT"
      },
      "source": [
        "#### 5.2 Named Entity Recognition (NER)\n",
        "For the *original* `sample_review`, use spaCy to identify named entities. Print each entity's `text`, `label_` (entity type), and `start_char`, `end_char` attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoRqF7vJsfRT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hydGBdesfRU"
      },
      "source": [
        "#### 5.3 Discussion\n",
        "How can POS tagging and NER be beneficial in the context of analyzing customer reviews? Give specific examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVUkFzs0sfRU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joeD2FjwsfRU"
      },
      "source": [
        "### Question 6: Custom Preprocessing Pipeline\n",
        "Create a function `preprocess_review(text)` that combines the following steps:\n",
        "1.  **Basic Cleaning:** Apply the cleaning steps from Question 1.\n",
        "2.  **spaCy Processing:** Process the cleaned text with spaCy.\n",
        "3.  **Tokenization & Lowercasing (if not already):** Extract tokens and ensure they are lowercased (though spaCy's `lemma_` is usually lowercase).\n",
        "4.  **Remove Stop Words:** Remove stop words using spaCy's `is_stop` attribute.\n",
        "5.  **Lemmatization:** Apply lemmatization using spaCy's `lemma_` attribute.\n",
        "6.  **Remove Punctuation Tokens:** Remove tokens that are purely punctuation (use `token.is_punct`).\n",
        "7.  **Filter for Alphanumeric Tokens:** Keep only tokens that are alphanumeric and are not just spaces (e.g., `token.is_alpha` or a regex check).\n",
        "8.  **Join Tokens:** Join the processed tokens back into a single string, separated by spaces.\n",
        "\n",
        "Apply this function to the `sample_review` and print the final preprocessed string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76OJXxISsfRU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIm9c7BZsfRU"
      },
      "source": [
        "### Question 7: Apply to Full Dataset and Analysis\n",
        "Apply your `preprocess_review` function (from Question 6) to the `text` column of your entire `reviews` DataFrame. Store the results in a new column named `processed_text`.\n",
        "\n",
        "After processing, display the first 5 rows of the DataFrame showing both the original `text` and the `processed_text` columns. Briefly discuss any challenges or observations you encountered when applying the function to the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eauUSoiAsfRU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPgNsbLnsfRV"
      },
      "source": [
        "## Submission Guidelines\n",
        "- Ensure your notebook runs without errors from top to bottom.\n",
        "- Save your notebook as `your_name_text_preprocessing_assignment.ipynb`.\n",
        "- Clearly answer all questions and provide explanations where requested.\n",
        "- Feel free to add markdown cells for additional explanations or observations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}