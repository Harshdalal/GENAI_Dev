{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGwN7mhetQJX"
      },
      "source": [
        "# Sentiment Classifier Assignment: Logistic Regression + TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGQ7CS6XtQJY"
      },
      "source": [
        "## Introduction\n",
        "In this assignment, you will build a sentiment classification model using one of the fundamental machine learning techniques: Logistic Regression. To represent text data, you will employ the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization method. This assignment will guide you through the complete pipeline, from data loading and preprocessing to model training and evaluation.\n",
        "\n",
        "**Sentiment analysis** (also known as opinion mining) is a natural language processing (NLP) technique used to determine whether data is positive, negative, or neutral. It's often used to identify customer sentiment toward products, brands, or services in feedback and online conversations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GloZoiO_tQJZ"
      },
      "source": [
        "## Learning Objectives\n",
        "Upon successful completion of this assignment, you should be able to:\n",
        "- Load and prepare a text classification dataset.\n",
        "- Apply TF-IDF vectorization to convert text into numerical features.\n",
        "- Split data into training and testing sets.\n",
        "- Train a Logistic Regression model for binary classification.\n",
        "- Evaluate the performance of a classification model using various metrics (accuracy, precision, recall, F1-score, confusion matrix).\n",
        "- Interpret model results and identify areas for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MwaRuYmtQJa"
      },
      "source": [
        "## Dataset\n",
        "For this assignment, we will use a sentiment analysis dataset. A good choice is the **IMDb Movie Reviews Dataset**, which contains 50,000 movie reviews labeled as either positive or negative.\n",
        "\n",
        "**How to get the dataset:**\n",
        "You can download the dataset from Kaggle or directly from `sklearn.datasets` if available (though for larger datasets, manual download is often preferred).\n",
        "\n",
        "**If using a CSV file (recommended for this setup):**\n",
        "Assume you have a CSV file named `IMDB_Dataset.csv` with at least two columns:\n",
        "- `review`: The raw text of the movie review.\n",
        "- `sentiment`: The sentiment label (`positive` or `negative`).\n",
        "\n",
        "If you don't have it, you can find it on Kaggle: [IMDb Movie Reviews Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
        "\n",
        "**Placeholder Data (if you don't download the file):**\n",
        "We will provide a small dummy dataset if `IMDB_Dataset.csv` is not found, so the code can still run for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6q-8JHktQJb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('omw-1.4') # Required for WordNetLemmatizer\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Load dataset\n",
        "try:\n",
        "    df = pd.read_csv('IMDB_Dataset.csv')\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "    print(f\"Number of reviews: {len(df)}\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'IMDB_Dataset.csv' not found. Creating a dummy DataFrame for demonstration.\")\n",
        "    data = {\n",
        "        'review': [\n",
        "            \"A truly fantastic movie! The acting was superb and the plot was engaging. Highly recommend.\",\n",
        "            \"Absolutely terrible film. Boring, confusing, and a waste of time. I hated it.\",\n",
        "            \"It was an okay movie, nothing special. Some good parts, some bad.\",\n",
        "            \"Loved every minute of it! Great characters and a thrilling story. Five stars!\",\n",
        "            \"Worst movie I've seen in years. Poor direction and terrible script. Avoid!\",\n",
        "            \"The plot was a bit slow at times, but the ending made it worthwhile. Enjoyed it overall.\",\n",
        "            \"Mediocre at best. Didn't live up to the hype at all.\",\n",
        "            \"Brilliant! A must-watch for anyone who loves psychological thrillers.\",\n",
        "            \"Could not finish it. Too boring and the sound quality was bad.\",\n",
        "            \"Surprisingly good! I went in with low expectations and was pleasantly surprised.\"\n",
        "        ],\n",
        "        'sentiment': [\n",
        "            'positive', 'negative', 'neutral', 'positive', 'negative',\n",
        "            'positive', 'negative', 'positive', 'negative', 'positive'\n",
        "        ]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    # For simplicity in this binary classification, let's map 'neutral' to 'negative' if present\n",
        "    df['sentiment'] = df['sentiment'].replace('neutral', 'negative')\n",
        "    print(\"Using dummy data with sentiment mapped to positive/negative.\")\n",
        "    print(df.head())\n",
        "\n",
        "\n",
        "# Map sentiment to numerical values: positive -> 1, negative -> 0\n",
        "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "print(\"\\nSentiment mapping to numerical values completed.\")\n",
        "print(df['sentiment'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfpotMWQtQJd"
      },
      "source": [
        "## Assignment Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rs-19bqtQJe"
      },
      "source": [
        "### Question 1: Text Preprocessing\n",
        "Before vectorization, raw text needs to be cleaned and normalized. Create a function `preprocess_text(text)` that performs the following steps:\n",
        "1.  **Convert to Lowercase:** Convert the entire text to lowercase.\n",
        "2.  **Remove HTML Tags:** Remove any HTML tags (e.g., `<br />`, `<p>`). Use `re.sub(r'<.*?>', '', text)`.\n",
        "3.  **Remove Punctuation:** Remove all punctuation marks. Use `str.translate` with `str.maketrans` or `re.sub`.\n",
        "4.  **Remove Numbers:** Remove all numerical digits.\n",
        "5.  **Remove Extra Whitespace:** Replace multiple spaces with a single space and strip leading/trailing whitespace.\n",
        "6.  **Tokenization:** Tokenize the text into individual words.\n",
        "7.  **Remove Stop Words:** Remove common English stop words using NLTK's `stopwords` corpus.\n",
        "8.  **Lemmatization:** Apply lemmatization to each token using NLTK's `WordNetLemmatizer`. (Remember to provide POS tags for better lemmatization, if possible, but for simplicity, you can default to 'n' for nouns if you don't want to implement POS tagging).\n",
        "9.  **Join Tokens:** Join the processed tokens back into a single string, separated by spaces.\n",
        "\n",
        "Apply this function to the `review` column of your DataFrame and store the results in a new column named `cleaned_review`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NmlMOMitQJf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuoxHULtQJf"
      },
      "source": [
        "### Question 2: TF-IDF Vectorization\n",
        "TF-IDF stands for Term Frequency-Inverse Document Frequency. It's a numerical statistic that reflects how important a word is to a document in a collection or corpus. Rare words tend to have higher TF-IDF values.\n",
        "\n",
        "1.  Initialize a `TfidfVectorizer` from `sklearn.feature_extraction.text`.\n",
        "    * Consider setting `max_features` to limit the number of features (e.g., 5000 or 10000) to manage computational complexity, especially for large datasets. This will select the top `max_features` terms by TF-IDF score.\n",
        "    * You might also consider `ngram_range` (e.g., `(1, 2)` for unigrams and bigrams) if you want to capture word combinations, but for this assignment, start with `(1,1)` (unigrams).\n",
        "2.  Fit the `TfidfVectorizer` on your `cleaned_review` column and then transform the text data into TF-IDF features.\n",
        "3.  Print the shape of the resulting TF-IDF feature matrix (`X`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf5n6jSetQJg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73lWIDkBtQJg"
      },
      "source": [
        "### Question 3: Train-Test Split\n",
        "Split your data into training and testing sets. This is crucial to evaluate your model's performance on unseen data.\n",
        "\n",
        "1.  Split the TF-IDF features (`X`) and the `sentiment` labels (`y`) into training and testing sets using `train_test_split` from `sklearn.model_selection`.\n",
        "2.  Use a `test_size` of 0.2 (20% for testing) and a `random_state` for reproducibility (e.g., 42).\n",
        "3.  Print the shapes of `X_train`, `X_test`, `y_train`, and `y_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDuoEvTOtQJh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nfu-RnxtQJh"
      },
      "source": [
        "### Question 4: Logistic Regression Model Training\n",
        "Logistic Regression is a linear model used for binary classification. Despite its name, it's a classification algorithm.\n",
        "\n",
        "1.  Initialize a `LogisticRegression` model from `sklearn.linear_model`.\n",
        "    * Set `solver='liblinear'` (a good default for small datasets and L1/L2 regularization).\n",
        "    * Set `random_state=42` for reproducibility.\n",
        "    * Consider setting `max_iter` to a higher value (e.g., 1000) if you encounter convergence warnings.\n",
        "2.  Train the model using your training data (`X_train`, `y_train`).\n",
        "3.  Make predictions on both the training set (`y_train_pred`) and the test set (`y_test_pred`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSA5t9kqtQJh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igonG84TtQJi"
      },
      "source": [
        "### Question 5: Model Evaluation\n",
        "Evaluate the performance of your trained model using various classification metrics.\n",
        "\n",
        "1.  Calculate and print the **Accuracy Score** for both the training and test sets.\n",
        "2.  Calculate and print the **Precision Score** for the test set.\n",
        "3.  Calculate and print the **Recall Score** for the test set.\n",
        "4.  Calculate and print the **F1-Score** for the test set.\n",
        "5.  Generate and display the **Confusion Matrix** for the test set.\n",
        "6.  Print the **Classification Report** for the test set (which conveniently provides precision, recall, f1-score, and support for each class)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSTzgIsVtQJi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HPjtW6EtQJi"
      },
      "source": [
        "### Question 6: Interpretation and Analysis\n",
        "Based on the evaluation metrics and your understanding of the process, answer the following questions:\n",
        "\n",
        "1.  **Training vs. Test Accuracy:** Compare the accuracy on the training set with the accuracy on the test set. What does this comparison tell you about potential overfitting or underfitting?\n",
        "2.  **Importance of Metrics:** Why is accuracy alone often not sufficient for evaluating classification models, especially with imbalanced datasets? Which metrics (precision, recall, F1-score) are more informative in such cases, and why?\n",
        "3.  **Confusion Matrix Analysis:** Explain what the values in your confusion matrix represent. Which type of error (false positives or false negatives) do you think is more critical for a movie review sentiment classifier, and why?\n",
        "4.  **Model Limitations and Improvements:** What are some limitations of using Logistic Regression with TF-IDF for sentiment analysis? Suggest at least two ways you could potentially improve this model (e.g., trying different preprocessing steps, other feature engineering techniques, or different models).\n",
        "5.  **Most Important Features (Optional/Bonus):** If time permits, try to identify the top 10 most influential words (features) that contribute to a 'positive' sentiment and the top 10 words that contribute to a 'negative' sentiment. (Hint: Look at the `coef_` attribute of the `LogisticRegression` model and the `get_feature_names_out()` method of the `TfidfVectorizer`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJewo5EbtQJj"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrdkwKcitQJj"
      },
      "outputs": [],
      "source": [
        "# Bonus: Identify top features (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrhjFNjgtQJj"
      },
      "source": [
        "## Submission Guidelines\n",
        "- Ensure your notebook runs without errors from top to bottom.\n",
        "- Save your notebook as `your_name_sentiment_classifier_assignment.ipynb`.\n",
        "- Clearly answer all questions and provide explanations where requested in Markdown cells.\n",
        "- Feel free to add additional code cells or markdown cells for clarity or experimentation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}