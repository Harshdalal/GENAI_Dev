{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBye4SydzDxa"
      },
      "source": [
        "# Text Similarity Checker: Cosine Similarity and Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGQMCeiEzDxc"
      },
      "source": [
        "## Introduction\n",
        "**Text similarity** is a core concept in Natural Language Processing (NLP) that involves determining how semantically alike two pieces of text are. This is incredibly useful for tasks like semantic search, plagiarism detection, recommendation systems, and grouping similar documents.\n",
        "\n",
        "Instead of relying on simple word overlap, modern approaches leverage **text embeddings**. These are dense vector representations of words, phrases, or entire sentences that capture their meaning and context. Texts with similar meanings will have embeddings that are 'close' to each other in a multi-dimensional space.\n",
        "\n",
        "To measure this 'closeness', we often use **Cosine Similarity**, which calculates the cosine of the angle between two non-zero vectors. A cosine similarity of 1 indicates identical direction (most similar), 0 indicates orthogonality (no similarity), and -1 indicates opposite directions (most dissimilar).\n",
        "\n",
        "In this assignment, you'll use pre-trained models from the `sentence-transformers` library to generate these embeddings and then apply cosine similarity to build a text similarity checker.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtQAl4gPzDxd"
      },
      "source": [
        "## Learning Objectives\n",
        "Upon completion of this assignment, you should be able to:\n",
        "- Understand the concept of text embeddings and their role in semantic similarity.\n",
        "- Load a pre-trained sentence embedding model using `sentence-transformers`.\n",
        "- Generate embeddings for individual sentences and batches of sentences.\n",
        "- Calculate cosine similarity between text embeddings.\n",
        "- Implement a function to calculate text similarity between any two given texts.\n",
        "- Find top-K most similar sentences from a collection.\n",
        "- Discuss the advantages, applications, and limitations of embedding-based text similarity.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfyW-a3nzDxd"
      },
      "source": [
        "## Setup and Prerequisites\n",
        "Ensure you have the necessary libraries installed. If not, uncomment and run the following cells:\n",
        "\n",
        "```bash\n",
        "# pip install sentence-transformers torch\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M9V8n2LzDxe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"Sentence-Transformers Version: {sentence_transformers.__version__}\")\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Sample Sentences for Testing ---\n",
        "sample_sentences = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"A feline was resting on the rug.\",\n",
        "    \"The dog barked loudly at the mailman.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is a rapidly evolving field.\",\n",
        "    \"AI is transforming various industries.\",\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"What is the largest city in France?\",\n",
        "    \"I love to eat pizza.\",\n",
        "    \"I enjoy consuming delicious pepperoni slices.\",\n",
        "    \"Space exploration fascinates me.\",\n",
        "    \"Astronauts are preparing for a mission to Mars.\"\n",
        "]\n",
        "\n",
        "print(\"\\nSample sentences loaded. Total sentences:\", len(sample_sentences))\n",
        "print(\"\\nFirst sample sentence:\\n\", sample_sentences[0])\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU4BdTMszDxf"
      },
      "source": [
        "## Assignment Questions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-RiuQSAzDxf"
      },
      "source": [
        "### Question 1: Load Sentence Embedding Model\n",
        "The `SentenceTransformer` class makes it easy to load pre-trained models specifically designed for generating sentence-level embeddings. A good general-purpose model is `'all-MiniLM-L6-v2'`.\n",
        "\n",
        "1.  **Load Model:** Load the pre-trained `SentenceTransformer` model `'all-MiniLM-L6-v2'`. Move the model to your `device`.\n",
        "2.  **Inspect:** Print the type of the loaded model and confirm it's on the correct device. You can also print the model's structure (e.g., `model` itself) to see its components.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbEbY9o-zDxf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36C5mEPfzDxf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKzSA5-9zDxg"
      },
      "source": [
        "### Question 2: Generate Sentence Embeddings\n",
        "Once the model is loaded, you can generate embeddings for individual sentences or lists of sentences.\n",
        "\n",
        "1.  **Select Sentences:** Choose `sample_sentences[0]` and `sample_sentences[1]`.\n",
        "2.  **Encode:** Generate embeddings for each of these two sentences using `model.encode()`. Make sure to convert them to PyTorch tensors and move to device if not already.\n",
        "3.  **Print Shape:** Print the shape of each generated embedding. Explain what the dimensions represent (e.g., `(768,)` means a single sentence is embedded into a 768-dimensional vector).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1gF9l4FzDxg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5TB3QMnzDxg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwWdZIlCzDxh"
      },
      "source": [
        "### Question 3: Calculate Cosine Similarity\n",
        "The `util.cos_sim` function from `sentence_transformers` or `torch.nn.functional.cosine_similarity` can be used to calculate the similarity between two embedding vectors.\n",
        "\n",
        "1.  **Calculate Similarity:** Calculate the cosine similarity between the two embeddings you generated in Question 2.\n",
        "2.  **Print Score:** Print the similarity score.\n",
        "3.  **Interpretation:** Based on the sentences and the score, explain what a high score (close to 1) and a low score (close to 0 or negative) signifies in terms of text similarity.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVLxy0IuzDxh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfXzdYEdzDxh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMKZpMMozDxh"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUmpuVBHzDxi"
      },
      "source": [
        "### Question 4: Build a Text Similarity Function\n",
        "Encapsulate the embedding generation and cosine similarity calculation into a reusable function.\n",
        "\n",
        "1.  **Create Function:** Define a Python function `get_text_similarity(text1, text2, model)` that:\n",
        "    * Takes two strings (`text1`, `text2`) and the `SentenceTransformer` model as input.\n",
        "    * Encodes both texts into embeddings.\n",
        "    * Calculates their cosine similarity.\n",
        "    * Returns the similarity score.\n",
        "2.  **Test Cases:** Test your function with the following pairs from `sample_sentences` and print their similarities:\n",
        "    * `sample_sentences[4]` (AI) and `sample_sentences[5]` (AI transforming industries)\n",
        "    * `sample_sentences[2]` (dog barked) and `sample_sentences[3]` (fox jumps)\n",
        "    * `sample_sentences[8]` (love pizza) and `sample_sentences[9]` (enjoy pepperoni)\n",
        "    * `sample_sentences[6]` (Paris capital) and `sample_sentences[7]` (largest city in France)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TKc372fzDxi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij14wgEPzDxj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhgDzucIzDxj"
      },
      "source": [
        "### Question 5: Top-K Similar Sentences\n",
        "A common application is to find the most similar documents/sentences to a given query from a larger collection.\n",
        "\n",
        "1.  **Define Query:** Choose a query sentence, e.g., `\"I want to learn about space.\" `\n",
        "2.  **Encode All Sentences:** Encode the query sentence and *all* `sample_sentences` from the initial list into embeddings.\n",
        "3.  **Calculate All Similarities:** Calculate the cosine similarity between the query embedding and the embedding of *each* sentence in the `sample_sentences` list.\n",
        "4.  **Find Top-K:** Identify the top 3 most similar sentences to the query. Sort them by similarity score in descending order.\n",
        "5.  **Print Results:** Print the query sentence, and then the top 3 similar sentences along with their respective similarity scores.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKEbWy-VzDxk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxvYOZLZzDxk"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyzI_FbqzDxk"
      },
      "source": [
        "### Question 6: Discussion and Applications\n",
        "1.  **Embeddings vs. Traditional Methods:** What are the key advantages of using sentence embeddings and cosine similarity for text similarity compared to traditional methods like TF-IDF + Cosine Similarity or Bag-of-Words based approaches? (Think about semantic understanding).\n",
        "2.  **Real-world Applications:** Describe three distinct real-world applications (different from those mentioned in the introduction) where an embedding-based text similarity checker would be highly beneficial. Explain *how* it would be used in each scenario.\n",
        "3.  **Limitations/Challenges:** What are some potential limitations or challenges of this embedding-based text similarity approach? (Consider aspects like sarcasm, ambiguity, very specific domain knowledge, or the computational cost for extremely large datasets).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmBN8XTSzDxl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lofLrAAzDxl"
      },
      "source": [
        "## Submission Guidelines\n",
        "- Ensure your notebook runs without errors from top to bottom.\n",
        "- Save your notebook as `your_name_text_similarity_assignment.ipynb`.\n",
        "- Clearly answer all questions and provide explanations where requested in Markdown cells.\n",
        "- Feel free to add additional code cells or markdown cells for clarity or experimentation.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}