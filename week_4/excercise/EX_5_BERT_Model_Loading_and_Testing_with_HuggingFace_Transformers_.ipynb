{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gpvtzFRxlV1"
      },
      "source": [
        "# BERT Model Loading and Testing with HuggingFace Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeyHuJYZxlV3"
      },
      "source": [
        "## Introduction\n",
        "**BERT (Bidirectional Encoder Representations from Transformers)** is a groundbreaking pre-trained language model developed by Google. It has revolutionized many Natural Language Processing (NLP) tasks by providing highly effective contextualized word embeddings.\n",
        "\n",
        "The **HuggingFace Transformers library** has become the de-facto standard for working with state-of-the-art transformer models like BERT, GPT, T5, and many more. It provides easy-to-use interfaces to load pre-trained models and their corresponding tokenizers, facilitating rapid experimentation and deployment.\n",
        "\n",
        "In this assignment, you will learn how to load a BERT model and its tokenizer, understand its inputs and outputs, and perform basic inference to get contextualized embeddings.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4_aWb_lxlV4"
      },
      "source": [
        "## Learning Objectives\n",
        "Upon completion of this assignment, you should be able to:\n",
        "- Install and set up the HuggingFace Transformers library.\n",
        "- Load pre-trained BERT models and their tokenizers using `AutoModel` and `AutoTokenizer`.\n",
        "- Understand the tokenization process for BERT, including special tokens (`[CLS]`, `[SEP]`) and attention masks.\n",
        "- Prepare input tensors for BERT models.\n",
        "- Perform basic forward passes (inference) through a BERT model to obtain outputs like `last_hidden_state` and `pooler_output`.\n",
        "- Handle batching for multiple input sentences.\n",
        "- Discuss different BERT variants and their applications.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ogOn1JdxlV4"
      },
      "source": [
        "## Setup and Prerequisites\n",
        "Before you begin, ensure you have the necessary libraries installed. If not, uncomment and run the following cells:\n",
        "\n",
        "```bash\n",
        "# pip install transformers torch # or tensorflow\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwCMY1v5xlV5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"Transformers Version: {transformers.__version__}\")\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJGUX-qJxlV6"
      },
      "source": [
        "## Assignment Questions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndj1rh80xlV6"
      },
      "source": [
        "### Question 1: Loading BERT Model and Tokenizer\n",
        "The `AutoTokenizer` and `AutoModel` classes from HuggingFace Transformers are convenient ways to load the correct tokenizer and model for a given pre-trained checkpoint name (e.g., `'bert-base-uncased'`).\n",
        "\n",
        "1.  **Load Tokenizer:** Load the tokenizer for the `'bert-base-uncased'` model.\n",
        "2.  **Load Model:** Load the model for `'bert-base-uncased'`. Move the model to the appropriate `device` (GPU if available, otherwise CPU).\n",
        "3.  **Inspect:** Print the type of the loaded tokenizer and model. Print the first few layers/modules of the model's structure to get an idea of its architecture (e.g., `model.encoder.layer[0]`).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThG9hKkDxlV7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPQs4U26xlV8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNoJclh4xlV9"
      },
      "source": [
        "### Question 2: Tokenization Process\n",
        "BERT models process text after it has been converted into numerical token IDs. The tokenizer handles this conversion, along with adding special tokens and creating attention masks.\n",
        "\n",
        "1.  **Sample Sentence:** Define a sample sentence, e.g., `\"Hello, how are you today? I hope you are having a great time learning about BERT.\" `\n",
        "2.  **Tokenize:** Tokenize the sentence using your loaded tokenizer. Store the output.\n",
        "3.  **Examine Output:** Print the following components from the tokenizer's output:\n",
        "    * `input_ids` (the token IDs)\n",
        "    * `token_type_ids` (segment IDs)\n",
        "    * `attention_mask`\n",
        "4.  **Decode Tokens:** Convert the `input_ids` back into human-readable tokens using `tokenizer.convert_ids_to_tokens()`. Explain the purpose of the special tokens `[CLS]` and `[SEP]` based on your observation.\n",
        "5.  **Explain Masks:** Briefly explain the purpose of `attention_mask` and `token_type_ids` in BERT's input.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ms6k5HcxlV9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z3pUzZzxlV-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2Ca5olYxlV-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMfy7q0yxlV-"
      },
      "source": [
        "### Question 3: Basic Inference (Getting Embeddings)\n",
        "Once the input is tokenized and prepared, you can pass it through the BERT model to obtain contextualized embeddings.\n",
        "\n",
        "1.  **Prepare Input Tensors:** Take the `input_ids`, `attention_mask`, and `token_type_ids` from Question 2, convert them into PyTorch tensors, and move them to your `device`.\n",
        "2.  **Forward Pass:** Pass these tensors as arguments to your loaded BERT model. Ensure you disable gradient calculation with `torch.no_grad()` as we're only doing inference.\n",
        "3.  **Inspect Output:** Print the shapes of the `last_hidden_state` and `pooler_output` from the model's output.\n",
        "4.  **Explain Outputs:** Briefly explain what `last_hidden_state` represents (the contextualized embeddings for each token) and what `pooler_output` typically represents (the aggregate representation of the entire sequence, often corresponding to the `[CLS]` token's representation after a linear layer and tanh activation).\n",
        "5.  **CLS Token Embedding:** How would you specifically extract the embedding for the `[CLS]` token from `last_hidden_state`? (Hint: it's typically the first token).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7wausCOxlV_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ-ZmQJDxlWA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZaYuHS0xlWA"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q1E1gAXxlWA"
      },
      "source": [
        "### Question 4: Handling Multiple Sentences / Batching\n",
        "In real-world scenarios, you'll often process multiple sentences simultaneously (in batches) for efficiency. HuggingFace tokenizers handle this automatically.\n",
        "\n",
        "1.  **Sample Sentences:** Define a list of 2-3 short sentences.\n",
        "2.  **Batch Tokenization:** Tokenize this list of sentences. Remember to set `padding=True` and `truncation=True` to ensure all sequences in the batch have the same length.\n",
        "3.  **Prepare Batched Input:** Convert the tokenized output to PyTorch tensors and move them to your `device`.\n",
        "4.  **Batched Forward Pass:** Pass the batched input through the BERT model.\n",
        "5.  **Output Shapes:** Print the shapes of `last_hidden_state` and `pooler_output` for the batched input. Explain how the shapes differ from the single-sentence case and what each dimension represents.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL78zXP9xlWA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjlnQGiGxlWB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGaUju2kxlWB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yjpfVZDxlWC"
      },
      "source": [
        "### Question 5: Exploring Different BERT Variants (Discussion/Code)\n",
        "The HuggingFace Hub contains hundreds of pre-trained models. BERT has several variants, each with different properties (size, language, training objective).\n",
        "\n",
        "1.  **Choose a Variant:** Select one *different* BERT variant from the HuggingFace model hub (e.g., `bert-large-uncased`, `distilbert-base-uncased`, `bert-base-multilingual-cased`, `bert-base-uncased-whole-word-masking`).\n",
        "2.  **Load and Inspect (Optional):** Load the tokenizer and model for your chosen variant. You can optionally print its total number of parameters (e.g., `model.num_parameters()`).\n",
        "3.  **Discuss Differences:** Briefly discuss what makes your chosen variant different from `bert-base-uncased` (e.g., size, language, training technique, specific application).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI5zN5uFxlWD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yULsx3h_xlWD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW_LPScUxlWE"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZleyRG9axlWE"
      },
      "source": [
        "### Question 6: Applications of BERT (Discussion)\n",
        "BERT's ability to generate rich contextualized embeddings has made it suitable for a wide array of NLP tasks.\n",
        "\n",
        "1.  **List 3 NLP Tasks:** Name at least three distinct NLP tasks where BERT models are commonly used. For each task, briefly explain how BERT's capabilities (e.g., contextual embeddings, bidirectional understanding) contribute to its effectiveness.\n",
        "2.  **Fine-tuning Concept:** Briefly explain the concept of \"fine-tuning\" a pre-trained BERT model for a downstream task. Why is this approach so powerful?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khFR9SOoxlWE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc7-xpOVxlWF"
      },
      "source": [
        "## Submission Guidelines\n",
        "- Ensure your notebook runs without errors from top to bottom.\n",
        "- Save your notebook as `your_name_bert_huggingface_assignment.ipynb`.\n",
        "- Clearly answer all questions and provide explanations where requested in Markdown cells.\n",
        "- Feel free to add additional code cells or markdown cells for clarity or experimentation.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}